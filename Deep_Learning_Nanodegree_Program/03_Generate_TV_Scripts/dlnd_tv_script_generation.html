<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>dlnd_tv_script_generation</title><script src="https://unpkg.com/jupyter-js-widgets@2.0.*/dist/embed.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}

@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="TV-Script-Generation">TV Script Generation<a class="anchor-link" href="#TV-Script-Generation">&#182;</a></h1><p>In this project, you'll generate your own <a href="https://en.wikipedia.org/wiki/The_Simpsons">Simpsons</a> TV scripts using RNNs.  You'll be using part of the <a href="https://www.kaggle.com/wcukierski/the-simpsons-by-the-data">Simpsons dataset</a> of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at <a href="https://simpsonswiki.com/wiki/Moe&#39;s_Tavern">Moe's Tavern</a>.</p>
<h2 id="Get-the-Data">Get the Data<a class="anchor-link" href="#Get-the-Data">&#182;</a></h2><p>The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like "Moe's Cavern", "Flaming Moe's", "Uncle Moe's Family Feed-Bag", etc..</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">helper</span>

<span class="n">data_dir</span> <span class="o">=</span> <span class="s1">&#39;./data/simpsons/moes_tavern_lines.txt&#39;</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">)</span>
<span class="c1"># Ignore notice, since we don&#39;t use it for analysing the data</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">text</span><span class="p">[</span><span class="mi">81</span><span class="p">:]</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Explore-the-Data">Explore the Data<a class="anchor-link" href="#Explore-the-Data">&#182;</a></h2><p>Play around with <code>view_sentence_range</code> to view different parts of the data.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[2]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">view_sentence_range</span> <span class="o">=</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Dataset Stats&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Roughly the number of unique words: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">({</span><span class="n">word</span><span class="p">:</span> <span class="kc">None</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()})))</span>
<span class="n">scenes</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n\n</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of scenes: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">scenes</span><span class="p">)))</span>
<span class="n">sentence_count_scene</span> <span class="o">=</span> <span class="p">[</span><span class="n">scene</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">scene</span> <span class="ow">in</span> <span class="n">scenes</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average number of sentences in each scene: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">sentence_count_scene</span><span class="p">)))</span>

<span class="n">sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sentence</span> <span class="k">for</span> <span class="n">scene</span> <span class="ow">in</span> <span class="n">scenes</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">scene</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Number of lines: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sentences</span><span class="p">)))</span>
<span class="n">word_count_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sentences</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Average number of words in each line: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">(</span><span class="n">word_count_sentence</span><span class="p">)))</span>

<span class="nb">print</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;The sentences </span><span class="si">{}</span><span class="s1"> to </span><span class="si">{}</span><span class="s1">:&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">view_sentence_range</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)[</span><span class="n">view_sentence_range</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span><span class="n">view_sentence_range</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Dataset Stats
Roughly the number of unique words: 11492
Number of scenes: 262
Average number of sentences in each scene: 15.251908396946565
Number of lines: 4258
Average number of words in each line: 11.50164396430249

The sentences 0 to 10:

Moe_Szyslak: (INTO PHONE) Moe&#39;s Tavern. Where the elite meet to drink.
Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.
Moe_Szyslak: (INTO PHONE) Hold on, I&#39;ll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?
Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I&#39;m gonna catch you, and I&#39;m gonna carve my name on your back with an ice pick.
Moe_Szyslak: What&#39;s the matter Homer? You&#39;re not your normal effervescent self.
Homer_Simpson: I got my problems, Moe. Give me another one.
Moe_Szyslak: Homer, hey, you should not drink to forget your problems.
Barney_Gumble: Yeah, you should only drink to enhance your social skills.

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implement-Preprocessing-Functions">Implement Preprocessing Functions<a class="anchor-link" href="#Implement-Preprocessing-Functions">&#182;</a></h2><p>The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:</p>
<ul>
<li>Lookup Table</li>
<li>Tokenize Punctuation</li>
</ul>
<h3 id="Lookup-Table">Lookup Table<a class="anchor-link" href="#Lookup-Table">&#182;</a></h3><p>To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:</p>
<ul>
<li>Dictionary to go from the words to an id, we'll call <code>vocab_to_int</code></li>
<li>Dictionary to go from the id to word, we'll call <code>int_to_vocab</code></li>
</ul>
<p>Return these dictionaries in the following tuple <code>(vocab_to_int, int_to_vocab)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="This-was-my-original-method.-The-next-cell-used-dictionary-comprehension-and-it-is-more-concise.-I-kept-this-because-I-did-do-some-research-to-get-it-to-work-and-it-highlights-some-differences-between-python-2-and-3.-This-can-be-refered-to-later-in-the-future.">This was my original method. The next cell used dictionary comprehension and it is more concise. I kept this because I did do some research to get it to work and it highlights some differences between python 2 and 3. This can be refered to later in the future.<a class="anchor-link" href="#This-was-my-original-method.-The-next-cell-used-dictionary-comprehension-and-it-is-more-concise.-I-kept-this-because-I-did-do-some-research-to-get-it-to-work-and-it-highlights-some-differences-between-python-2-and-3.-This-can-be-refered-to-later-in-the-future.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[3]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># import numpy as np</span>
<span class="c1"># import problem_unittests as tests</span>
<span class="c1"># from collections import Counter</span>

<span class="c1"># # Inverting a Dictionary</span>
<span class="c1"># # https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch04s15.html</span>
<span class="c1"># def invert_dict(d):</span>
    
<span class="c1">#     # NOTE: .iteritems is deprecated in python 3, use .items() instead</span>
<span class="c1">#     # https://stackoverflow.com/questions/30418481/error-dict-object-has-no-attribute-iteritems-when-trying-to-use-networkx</span>
<span class="c1">#     # https://wiki.python.org/moin/Python3.0</span>
<span class="c1">#     #return dict([ (v, k) for k, v in d.iteritems( ) ])</span>
<span class="c1">#     return dict([ (v, k) for k, v in d.items( ) ])</span>

<span class="c1"># # # Inverting a Dictionary</span>
<span class="c1"># # # Faster for larger dictionaries</span>
<span class="c1"># # # https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch04s15.html</span>
<span class="c1"># # from itertools import izip</span>
<span class="c1"># # def invert_dict_fast(d):</span>
<span class="c1"># #     return dict(izip(d.itervalues( ), d.iterkeys( )))</span>

<span class="c1"># # Inverting a Dictionary</span>
<span class="c1"># # Faster for larger dictionaries</span>
<span class="c1"># # https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch04s15.html</span>
<span class="c1"># #</span>
<span class="c1"># # NOTE: In python 3, there are some changes.</span>
<span class="c1"># #       - You don&#39;t need itertools izip, you can just use zip </span>
<span class="c1"># #         http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html</span>
<span class="c1"># #       - itervalues and iterkeys are deprecated dictionary methods, use dict.keys() and dict.values()</span>
<span class="c1"># #         https://wiki.python.org/moin/Python3.0</span>
<span class="c1"># def invert_dict_fast(d):</span>
<span class="c1">#     return dict(zip(d.values( ), d.keys( )))</span>


<span class="c1"># def create_lookup_tables(text):</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     Create lookup tables for vocabulary</span>
<span class="c1">#     :param text: The text of tv scripts split into words</span>
<span class="c1">#     :return: A tuple of dicts (vocab_to_int, int_to_vocab)</span>
<span class="c1">#     &quot;&quot;&quot;</span>
<span class="c1">#     # TODO: Implement Function</span>
<span class="c1">#     counts = Counter(text)</span>
<span class="c1">#     vocab = sorted(counts, key=counts.get, reverse=True)</span>
    
<span class="c1">#     # Dictionary comprehension.</span>
<span class="c1">#     #</span>
<span class="c1">#     # https://stackoverflow.com/questions/22171558/what-does-enumerate-mean</span>
<span class="c1">#     # The enumerate() function adds a counter to an iterable.</span>
<span class="c1">#     # So for each element in cursor, a tuple is produced with (counter, element); </span>
<span class="c1">#     # the for loop binds that to row_number and row, respectively.</span>
<span class="c1">#     # By default, enumerate() starts counting at 0 but if you give it a second integer </span>
<span class="c1">#     # argument, it&#39;ll start from that number instead:</span>
<span class="c1">#     vocab_to_int = {text: ii for ii, text in enumerate(vocab, 1)}</span>
    
<span class="c1">#     # Create the inverted dictionary</span>
<span class="c1">#     #int_to_vocab = invert_dict(vocab_to_int)</span>
<span class="c1">#     int_to_vocab = invert_dict_fast(vocab_to_int)</span>

    
<span class="c1">#     return vocab_to_int, int_to_vocab</span>


<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># tests.test_create_lookup_tables(create_lookup_tables)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Using-dictionary-comprehension-is-more-concise.">Using dictionary comprehension is more concise.<a class="anchor-link" href="#Using-dictionary-comprehension-is-more-concise.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[4]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="k">import</span> <span class="n">Counter</span>

<span class="k">def</span> <span class="nf">create_lookup_tables</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create lookup tables for vocabulary</span>
<span class="sd">    :param text: The text of tv scripts split into words</span>
<span class="sd">    :return: A tuple of dicts (vocab_to_int, int_to_vocab)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">vocab_to_int</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="n">int_to_vocab</span> <span class="o">=</span> <span class="p">{</span><span class="n">i</span><span class="p">:</span><span class="n">word</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
    <span class="k">return</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_create_lookup_tables</span><span class="p">(</span><span class="n">create_lookup_tables</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Tokenize-Punctuation">Tokenize Punctuation<a class="anchor-link" href="#Tokenize-Punctuation">&#182;</a></h3><p>We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word "bye" and "bye!".</p>
<p>Implement the function <code>token_lookup</code> to return a dict that will be used to tokenize symbols like "!" into "||Exclamation_Mark||".  Create a dictionary for the following symbols where the symbol is the key and value is the token:</p>
<ul>
<li>Period ( . )</li>
<li>Comma ( , )</li>
<li>Quotation Mark ( " )</li>
<li>Semicolon ( ; )</li>
<li>Exclamation mark ( ! )</li>
<li>Question mark ( ? )</li>
<li>Left Parentheses ( ( )</li>
<li>Right Parentheses ( ) )</li>
<li>Dash ( -- )</li>
<li>Return ( \n )</li>
</ul>
<p>This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token "dash", try using something like "||dash||".</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[5]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">token_lookup</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Generate a dict to turn punctuation into a token.</span>
<span class="sd">    :return: Tokenize dictionary where the key is the punctuation and the value is the token</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="n">dict_punctuation_tokens</span> <span class="o">=</span> <span class="p">{</span> <span class="s1">&#39;.&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Period||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;,&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Comma||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;&quot;&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Quotation_Mark||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;;&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Semicolon||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;!&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Exclamation_mark||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;?&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Question_mark||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;(&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Left_Parentheses||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;)&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Right_Parentheses||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;--&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Dash||&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span> <span class="p">:</span> <span class="s1">&#39;||Return||&#39;</span> <span class="p">}</span>    
    <span class="k">return</span> <span class="n">dict_punctuation_tokens</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_tokenize</span><span class="p">(</span><span class="n">token_lookup</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Preprocess-all-the-data-and-save-it">Preprocess all the data and save it<a class="anchor-link" href="#Preprocess-all-the-data-and-save-it">&#182;</a></h2><p>Running the code cell below will preprocess all the data and save it to file.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[6]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># Preprocess Training, Validation, and Testing Data</span>
<span class="n">helper</span><span class="o">.</span><span class="n">preprocess_and_save_data</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="n">token_lookup</span><span class="p">,</span> <span class="n">create_lookup_tables</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Check-Point">Check Point<a class="anchor-link" href="#Check-Point">&#182;</a></h1><p>This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[7]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">int_text</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_preprocess</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Build-the-Neural-Network">Build the Neural Network<a class="anchor-link" href="#Build-the-Neural-Network">&#182;</a></h2><p>You'll build the components necessary to build a RNN by implementing the following functions below:</p>
<ul>
<li>get_inputs</li>
<li>get_init_cell</li>
<li>get_embed</li>
<li>build_rnn</li>
<li>build_nn</li>
<li>get_batches</li>
</ul>
<h3 id="Check-the-Version-of-TensorFlow-and-Access-to-GPU">Check the Version of TensorFlow and Access to GPU<a class="anchor-link" href="#Check-the-Version-of-TensorFlow-and-Access-to-GPU">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[8]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">distutils.version</span> <span class="k">import</span> <span class="n">LooseVersion</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="c1"># Check TensorFlow Version</span>
<span class="k">assert</span> <span class="n">LooseVersion</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">LooseVersion</span><span class="p">(</span><span class="s1">&#39;1.0&#39;</span><span class="p">),</span> <span class="s1">&#39;Please use TensorFlow version 1.0 or newer&#39;</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;TensorFlow Version: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">__version__</span><span class="p">))</span>

<span class="c1"># Check for a GPU</span>
<span class="k">if</span> <span class="ow">not</span> <span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">():</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;No GPU found. Please use a GPU to train your neural network.&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Default GPU Device: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">test</span><span class="o">.</span><span class="n">gpu_device_name</span><span class="p">()))</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>TensorFlow Version: 1.0.1
Default GPU Device: /gpu:0
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Input">Input<a class="anchor-link" href="#Input">&#182;</a></h3><p>Implement the <code>get_inputs()</code> function to create TF Placeholders for the Neural Network.  It should create the following placeholders:</p>
<ul>
<li>Input text placeholder named "input" using the <a href="https://www.tensorflow.org/api_docs/python/tf/placeholder">TF Placeholder</a> <code>name</code> parameter.</li>
<li>Targets placeholder</li>
<li>Learning Rate placeholder</li>
</ul>
<p>Return the placeholders in the following tuple <code>(Input, Targets, LearningRate)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[9]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_inputs</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create TF Placeholders for input, targets, and learning rate.</span>
<span class="sd">    :return: Tuple (input, targets, learning rate)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="n">Input</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;input&#39;</span><span class="p">)</span>
    <span class="n">Targets</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span>  <span class="n">shape</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;Targets&#39;</span><span class="p">)</span>
    <span class="n">LearningRate</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">placeholder</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">shape</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;LearningRate&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Targets</span><span class="p">,</span> <span class="n">LearningRate</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_inputs</span><span class="p">(</span><span class="n">get_inputs</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-RNN-Cell-and-Initialize">Build RNN Cell and Initialize<a class="anchor-link" href="#Build-RNN-Cell-and-Initialize">&#182;</a></h3><p>Stack one or more <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell"><code>BasicLSTMCells</code></a> in a <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell"><code>MultiRNNCell</code></a>.</p>
<ul>
<li>The Rnn size should be set using <code>rnn_size</code></li>
<li>Initalize Cell State using the MultiRNNCell's <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state"><code>zero_state()</code></a> function<ul>
<li>Apply the name "initial_state" to the initial state using <a href="https://www.tensorflow.org/api_docs/python/tf/identity"><code>tf.identity()</code></a></li>
</ul>
</li>
</ul>
<p>Return the cell and initial state in the following tuple <code>(Cell, InitialState)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_init_cell</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create an RNN Cell and initialize it.</span>
<span class="sd">    :param batch_size: Size of batches</span>
<span class="sd">    :param rnn_size: Size of RNNs</span>
<span class="sd">    :return: Tuple (cell, initialize state)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="c1"># NOTE: It seems like setting the number of layers matter when training</span>
    <span class="c1">#       as well as dropout probability. It would be nice if these </span>
    <span class="c1">#       &quot;hyper-parameters&quot; were consolidated into one place so I don&#39;t have to jump around</span>
    <span class="c1">#        the notebook.</span>
    <span class="c1">#num_layers = 10</span>
    <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">keep_prob</span> <span class="o">=</span> <span class="mf">0.5</span>
    
    <span class="c1"># Your basic LSTM cell</span>
    <span class="n">lstm</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">BasicLSTMCell</span><span class="p">(</span><span class="n">rnn_size</span><span class="p">)</span>
        
    <span class="c1"># Add dropout</span>
    <span class="n">drop</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">DropoutWrapper</span><span class="p">(</span><span class="n">lstm</span><span class="p">,</span> <span class="n">output_keep_prob</span><span class="o">=</span><span class="n">keep_prob</span><span class="p">)</span>
    
    <span class="c1"># Stack up multiple LSTM layers, for deep learning</span>
    <span class="c1">#</span>
    <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell</span>
    <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell</span>
    <span class="c1">#</span>
    <span class="c1"># Note:</span>
    <span class="c1"># RNN cell composed sequentially of multiple simple cells.</span>
    <span class="c1"># Even if it is just one BasicLSTMCells you have to make it iterable by</span>
    <span class="c1"># puttint it in a dictionary.</span>
    <span class="c1">#</span>
    <span class="c1"># Note: Put 3 layers because the lecture stated that 3 layers improves 2 but</span>
    <span class="c1">#       no real difference beyound 3 unless dealing with convolution network.</span>
    <span class="c1">#       RNN is similar to a traditional feed-forward network, so I will assume</span>
    <span class="c1">#       3 layers is a good starting point.</span>
    <span class="n">Cell</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">MultiRNNCell</span><span class="p">([</span><span class="n">drop</span><span class="p">]</span> <span class="o">*</span> <span class="n">num_layers</span><span class="p">)</span>
    
    <span class="c1"># Getting an initial state of all zeros</span>
    <span class="n">InitialState</span> <span class="o">=</span> <span class="n">Cell</span><span class="o">.</span><span class="n">zero_state</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    
    <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/identity</span>
    <span class="n">InitialState</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">InitialState</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;initial_state&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Cell</span><span class="p">,</span> <span class="n">InitialState</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_init_cell</span><span class="p">(</span><span class="n">get_init_cell</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Word-Embedding">Word Embedding<a class="anchor-link" href="#Word-Embedding">&#182;</a></h3><p>Apply embedding to <code>input_data</code> using TensorFlow.  Return the embedded sequence.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_embed</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create embedding for &lt;input_data&gt;.</span>
<span class="sd">    :param input_data: TF placeholder for text input.</span>
<span class="sd">    :param vocab_size: Number of words in vocabulary.</span>
<span class="sd">    :param embed_dim: Number of embedding dimensions</span>
<span class="sd">    :return: Embedded input.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="c1"># Examples of embedding:</span>
    <span class="c1"># https://github.com/udacity/deep-learning/blob/master/embeddings/Skip-Grams-Solution.ipynb</span>
    <span class="c1"># https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb</span>
    <span class="n">embedding</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">random_uniform</span><span class="p">((</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">embedding_lookup</span><span class="p">(</span><span class="n">embedding</span><span class="p">,</span> <span class="n">input_data</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">embed</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_embed</span><span class="p">(</span><span class="n">get_embed</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-RNN">Build RNN<a class="anchor-link" href="#Build-RNN">&#182;</a></h3><p>You created a RNN Cell in the <code>get_init_cell()</code> function.  Time to use the cell to create a RNN.</p>
<ul>
<li>Build the RNN using the <a href="https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn"><code>tf.nn.dynamic_rnn()</code></a><ul>
<li>Apply the name "final_state" to the final state using <a href="https://www.tensorflow.org/api_docs/python/tf/identity"><code>tf.identity()</code></a></li>
</ul>
</li>
</ul>
<p>Return the outputs and final_state state in the following tuple <code>(Outputs, FinalState)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">build_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Create a RNN using a RNN Cell</span>
<span class="sd">    :param cell: RNN Cell</span>
<span class="sd">    :param inputs: Input text data</span>
<span class="sd">    :return: Tuple (Outputs, Final State)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn</span>
    <span class="n">Outputs</span><span class="p">,</span> <span class="n">FinalState</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dynamic_rnn</span><span class="p">(</span> <span class="n">cell</span><span class="p">,</span>
                                             <span class="n">inputs</span><span class="p">,</span>
                                             <span class="n">dtype</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">float32</span> <span class="p">)</span>
    
    <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/identity</span>
    <span class="n">FinalState</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">FinalState</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;final_state&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Outputs</span><span class="p">,</span> <span class="n">FinalState</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_build_rnn</span><span class="p">(</span><span class="n">build_rnn</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-the-Neural-Network">Build the Neural Network<a class="anchor-link" href="#Build-the-Neural-Network">&#182;</a></h3><p>Apply the functions you implemented above to:</p>
<ul>
<li>Apply embedding to <code>input_data</code> using your <code>get_embed(input_data, vocab_size, embed_dim)</code> function.</li>
<li>Build RNN using <code>cell</code> and your <code>build_rnn(cell, inputs)</code> function.</li>
<li>Apply a fully connected layer with a linear activation and <code>vocab_size</code> as the number of outputs.</li>
</ul>
<p>Return the logits and final state in the following tuple (Logits, FinalState)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="NOTE:-rnn_size-does-not-seem-to-be-used.">NOTE: rnn_size does not seem to be used.<a class="anchor-link" href="#NOTE:-rnn_size-does-not-seem-to-be-used.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[13]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">build_nn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Build part of the neural network</span>
<span class="sd">    :param cell: RNN cell</span>
<span class="sd">    :param rnn_size: Size of rnns</span>
<span class="sd">    :param input_data: Input data</span>
<span class="sd">    :param vocab_size: Vocabulary size</span>
<span class="sd">    :param embed_dim: Number of embedding dimensions</span>
<span class="sd">    :return: Tuple (Logits, FinalState)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="c1"># Apply embedding to input_data using your get_embed(input_data, vocab_size, embed_dim) function.</span>
    <span class="c1">#embed = get_embed(input_data, vocab_size, embed_dim)</span>
    <span class="n">embed</span> <span class="o">=</span> <span class="n">get_embed</span><span class="p">(</span><span class="n">input_data</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>
    
    <span class="c1"># Build RNN using cell and your build_rnn(cell, inputs) function.</span>
    <span class="n">Outputs</span><span class="p">,</span> <span class="n">FinalState</span> <span class="o">=</span> <span class="n">build_rnn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">embed</span><span class="p">)</span>

    <span class="c1"># Apply a fully connected layer with a linear activation and vocab_size as the number of outputs.</span>
    <span class="c1"># https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected</span>
    <span class="c1">#</span>
    <span class="c1"># Note: To have linear activation, pass None to activation_fn</span>
    <span class="n">Logits</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">contrib</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span> <span class="n">Outputs</span><span class="p">,</span>
                                                 <span class="n">vocab_size</span><span class="p">,</span>
                                                 <span class="n">activation_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                                                 <span class="n">weights_initializer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">truncated_normal_initializer</span><span class="p">(</span><span class="n">stddev</span><span class="o">=</span><span class="mf">0.1</span><span class="p">),</span>
                                                 <span class="n">biases_initializer</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">zeros_initializer</span><span class="p">())</span>
    
    
    <span class="k">return</span> <span class="n">Logits</span><span class="p">,</span> <span class="n">FinalState</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_build_nn</span><span class="p">(</span><span class="n">build_nn</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Batches">Batches<a class="anchor-link" href="#Batches">&#182;</a></h3><p>Implement <code>get_batches</code> to create batches of input and targets using <code>int_text</code>.  The batches should be a Numpy array with the shape <code>(number of batches, 2, batch size, sequence length)</code>. Each batch contains two elements:</p>
<ul>
<li>The first element is a single batch of <strong>input</strong> with the shape <code>[batch size, sequence length]</code></li>
<li>The second element is a single batch of <strong>targets</strong> with the shape <code>[batch size, sequence length]</code></li>
</ul>
<p>If you can't fill the last batch with enough data, drop the last batch.</p>
<p>For exmple, <code>get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)</code> would return a Numpy array of the following:</p>

<pre><code>[
  # First Batch
  [
    # Batch of Input
    [[ 1  2], [ 7  8], [13 14]]
    # Batch of targets
    [[ 2  3], [ 8  9], [14 15]]
  ]

  # Second Batch
  [
    # Batch of Input
    [[ 3  4], [ 9 10], [15 16]]
    # Batch of targets
    [[ 4  5], [10 11], [16 17]]
  ]

  # Third Batch
  [
    # Batch of Input
    [[ 5  6], [11 12], [17 18]]
    # Batch of targets
    [[ 6  7], [12 13], [18  1]]
  ]
]</code></pre>
<p>Notice that the last target value in the last batch is the first input value of the first batch. In this case, <code>1</code>. This is a common technique used when creating sequence batches, although it is rather unintuitive.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[14]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_batches</span><span class="p">(</span><span class="n">int_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return batches of input and target</span>
<span class="sd">    :param int_text: Text with the words replaced by their ids</span>
<span class="sd">    :param batch_size: The size of batch</span>
<span class="sd">    :param seq_length: The length of sequence</span>
<span class="sd">    :return: Batches as a Numpy array</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    <span class="n">chars_in_a_batch</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">seq_length</span>
    
    <span class="c1"># NOTE: In python 3, double slashes // means interger division.</span>
    <span class="c1"># https://stackoverflow.com/questions/1535596/what-is-the-reason-for-having-in-python</span>
    <span class="n">num_batches</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">int_text</span><span class="p">)</span><span class="o">//</span><span class="p">(</span><span class="n">chars_in_a_batch</span><span class="p">)</span>
    
    <span class="c1"># Use slicing to select the range.</span>
    <span class="c1"># NOTE: The target is offset by 1 from the input. That is because it is a recurrent network,</span>
    <span class="c1">#       the previous input is needed to predict the next output. The example supplied in the </span>
    <span class="c1">#       markdown cell above demonstrates this phenomenon.</span>
    <span class="n">input_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">int_text</span><span class="p">[</span> <span class="p">:</span> <span class="p">(</span><span class="n">num_batches</span> <span class="o">*</span> <span class="n">chars_in_a_batch</span><span class="p">)])</span>
    <span class="n">target_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">int_text</span><span class="p">[</span><span class="mi">1</span> <span class="p">:</span> <span class="p">(</span><span class="n">num_batches</span> <span class="o">*</span> <span class="n">chars_in_a_batch</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">])</span>
    
    <span class="c1"># The last target value in the last batch is the first input value of the first batch.</span>
    <span class="c1"># Note: I don&#39;t fully understand why this works. It was mentioned in the lession it is</span>
    <span class="c1">#       not understood why this works either. I am doing it because it was mentioned in</span>
    <span class="c1">#       the lecture and in the instruction.</span>
    <span class="n">target_data</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">input_data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> 
    
    <span class="c1"># Reshape</span>
    <span class="c1"># https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html</span>
    <span class="c1"># </span>
    <span class="c1"># Note:</span>
    <span class="c1"># One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">input_data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">target_data</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># Numpy Split</span>
    <span class="c1"># https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.split.html</span>
    <span class="c1">#</span>
    <span class="c1"># Note: This split an array into multiple sub-arrays.</span>
    <span class="c1"># NOTE: Use axis 1.</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">num_batches</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="c1"># Outputting to format (number of batches, 2, batch size, sequence length)</span>
    <span class="n">batches</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span><span class="p">)))</span>

    <span class="k">return</span> <span class="n">batches</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_batches</span><span class="p">(</span><span class="n">get_batches</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Neural-Network-Training">Neural Network Training<a class="anchor-link" href="#Neural-Network-Training">&#182;</a></h2><h3 id="Hyperparameters">Hyperparameters<a class="anchor-link" href="#Hyperparameters">&#182;</a></h3><p>Tune the following parameters:</p>
<ul>
<li>Set <code>num_epochs</code> to the number of epochs.</li>
<li>Set <code>batch_size</code> to the batch size.</li>
<li>Set <code>rnn_size</code> to the size of the RNNs.</li>
<li>Set <code>embed_dim</code> to the size of the embedding.</li>
<li>Set <code>seq_length</code> to the length of sequence.</li>
<li>Set <code>learning_rate</code> to the learning rate.</li>
<li>Set <code>show_every_n_batches</code> to the number of batches the neural network should print progress.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="NOTE:-These-were-my-original-hyperparameter-settings-that-trained-on.-I-left-them-because-I-let-it-train-for-about-5-hours-on-GPU-enabled-tensorflow-and-it-hovered-around-2.5-and-3.0-loss-value-before-I-stopped-it.-I-stopped-it-around-283-epochs-out-of-300.-It-did-not-look-like-it-was-going-to-get-better.-I-keep-it-around-for-reference-in-the-future.">NOTE: These were my original hyperparameter settings that trained on. I left them because I let it train for about 5 hours on GPU-enabled tensorflow and it hovered around 2.5 and 3.0 loss value before I stopped it. I stopped it around 283 epochs out of 300. It did not look like it was going to get better. I keep it around for reference in the future.<a class="anchor-link" href="#NOTE:-These-were-my-original-hyperparameter-settings-that-trained-on.-I-left-them-because-I-let-it-train-for-about-5-hours-on-GPU-enabled-tensorflow-and-it-hovered-around-2.5-and-3.0-loss-value-before-I-stopped-it.-I-stopped-it-around-283-epochs-out-of-300.-It-did-not-look-like-it-was-going-to-get-better.-I-keep-it-around-for-reference-in-the-future.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[15]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># # Number of Epochs</span>
<span class="c1"># num_epochs = 300</span>
<span class="c1"># # Batch Size</span>
<span class="c1"># batch_size = 32</span>
<span class="c1"># # RNN Size</span>
<span class="c1"># rnn_size = 300</span>
<span class="c1"># # Embedding Dimension Size</span>
<span class="c1"># #embed_dim = 200</span>
<span class="c1"># embed_dim = 300</span>
<span class="c1"># # Sequence Length</span>
<span class="c1"># seq_length = 10</span>
<span class="c1"># # Learning Rate</span>
<span class="c1"># learning_rate = 0.01</span>
<span class="c1"># # Show stats for every n number of batches</span>
<span class="c1"># show_every_n_batches = 10</span>

<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># save_dir = &#39;./save&#39;</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="These-are-the-hyperparameter-settings-I-used-that-worked.-It-actually-got-the-desired-target-loss-value-of-less-than-1-in-about-10-minutes-on-a-GPU-enabled-tensorflow-(as-opposed-5-hours)-at-around-epoch-12.-I-left-it-for-a-little-over-an-hour-with-100-epochs.-The-loss-seemed-to-hover-at-about-0.6-from-epoch-22-and-onward.-With-these-settings-and-the-hardward-used-to-train-the-model,-leaving-it-about-20-to-30-epochs-for-about-30-min-would-probably-best-optimize-this-configuration.-The-rest-of-epochs-are-most-likely-a-waste-of-time.-Of-course,-you-can't-know-that-when-discovering-the-settings.">These are the hyperparameter settings I used that worked. It actually got the desired target loss value of less than 1 in about 10 minutes on a GPU enabled tensorflow (as opposed 5 hours) at around epoch 12. I left it for a little over an hour with 100 epochs. The loss seemed to hover at about 0.6 from epoch 22 and onward. With these settings and the hardward used to train the model, leaving it about 20 to 30 epochs for about 30 min would probably best optimize this configuration. The rest of epochs are most likely a waste of time. Of course, you can't know that when discovering the settings.<a class="anchor-link" href="#These-are-the-hyperparameter-settings-I-used-that-worked.-It-actually-got-the-desired-target-loss-value-of-less-than-1-in-about-10-minutes-on-a-GPU-enabled-tensorflow-(as-opposed-5-hours)-at-around-epoch-12.-I-left-it-for-a-little-over-an-hour-with-100-epochs.-The-loss-seemed-to-hover-at-about-0.6-from-epoch-22-and-onward.-With-these-settings-and-the-hardward-used-to-train-the-model,-leaving-it-about-20-to-30-epochs-for-about-30-min-would-probably-best-optimize-this-configuration.-The-rest-of-epochs-are-most-likely-a-waste-of-time.-Of-course,-you-can't-know-that-when-discovering-the-settings.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[20]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Number of Epochs</span>
<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">100</span>
<span class="c1"># Batch Size</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">256</span>
<span class="c1"># RNN Size</span>
<span class="n">rnn_size</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="c1"># Embedding Dimension Size</span>
<span class="c1">#embed_dim = 200</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">1024</span>
<span class="c1"># Sequence Length</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">15</span>
<span class="c1"># Learning Rate</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="c1"># Show stats for every n number of batches</span>
<span class="n">show_every_n_batches</span> <span class="o">=</span> <span class="mi">20</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">save_dir</span> <span class="o">=</span> <span class="s1">&#39;./save&#39;</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Build-the-Graph">Build the Graph<a class="anchor-link" href="#Build-the-Graph">&#182;</a></h3><p>Build the graph using the neural network you implemented.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">from</span> <span class="nn">tensorflow.contrib</span> <span class="k">import</span> <span class="n">seq2seq</span>

<span class="n">train_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">train_graph</span><span class="o">.</span><span class="n">as_default</span><span class="p">():</span>
    <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">int_to_vocab</span><span class="p">)</span>
    <span class="n">input_text</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">lr</span> <span class="o">=</span> <span class="n">get_inputs</span><span class="p">()</span>
    <span class="n">input_data_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">input_text</span><span class="p">)</span>
    <span class="n">cell</span><span class="p">,</span> <span class="n">initial_state</span> <span class="o">=</span> <span class="n">get_init_cell</span><span class="p">(</span><span class="n">input_data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">rnn_size</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">build_nn</span><span class="p">(</span><span class="n">cell</span><span class="p">,</span> <span class="n">rnn_size</span><span class="p">,</span> <span class="n">input_text</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)</span>

    <span class="c1"># Probabilities for generating words</span>
    <span class="n">probs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;probs&#39;</span><span class="p">)</span>

    <span class="c1"># Loss function</span>
    <span class="n">cost</span> <span class="o">=</span> <span class="n">seq2seq</span><span class="o">.</span><span class="n">sequence_loss</span><span class="p">(</span>
        <span class="n">logits</span><span class="p">,</span>
        <span class="n">targets</span><span class="p">,</span>
        <span class="n">tf</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">input_data_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_data_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]))</span>

    <span class="c1"># Optimizer</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">AdamOptimizer</span><span class="p">(</span><span class="n">lr</span><span class="p">)</span>

    <span class="c1"># Gradient Clipping</span>
    <span class="n">gradients</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">compute_gradients</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
    <span class="n">capped_gradients</span> <span class="o">=</span> <span class="p">[(</span><span class="n">tf</span><span class="o">.</span><span class="n">clip_by_value</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span> <span class="n">var</span><span class="p">)</span> <span class="k">for</span> <span class="n">grad</span><span class="p">,</span> <span class="n">var</span> <span class="ow">in</span> <span class="n">gradients</span> <span class="k">if</span> <span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">]</span>
    <span class="n">train_op</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">apply_gradients</span><span class="p">(</span><span class="n">capped_gradients</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Train">Train<a class="anchor-link" href="#Train">&#182;</a></h2><p>Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the <a href="https://discussions.udacity.com/">forums</a> to see if anyone is having the same problem.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="As-mentioned-above,-this-was-my-5-hour-GPU-enabled-tensorflow-settings.-I-kept-it-for-reference.">As mentioned above, this was my 5 hour GPU-enabled tensorflow settings. I kept it for reference.<a class="anchor-link" href="#As-mentioned-above,-this-was-my-5-hour-GPU-enabled-tensorflow-settings.-I-kept-it-for-reference.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="c1"># &quot;&quot;&quot;</span>
<span class="c1"># batches = get_batches(int_text, batch_size, seq_length)</span>

<span class="c1"># with tf.Session(graph=train_graph) as sess:</span>
<span class="c1">#     sess.run(tf.global_variables_initializer())</span>

<span class="c1">#     for epoch_i in range(num_epochs):</span>
<span class="c1">#         state = sess.run(initial_state, {input_text: batches[0][0]})</span>

<span class="c1">#         for batch_i, (x, y) in enumerate(batches):</span>
<span class="c1">#             feed = {</span>
<span class="c1">#                 input_text: x,</span>
<span class="c1">#                 targets: y,</span>
<span class="c1">#                 initial_state: state,</span>
<span class="c1">#                 lr: learning_rate}</span>
<span class="c1">#             train_loss, state, _ = sess.run([cost, final_state, train_op], feed)</span>

<span class="c1">#             # Show every &lt;show_every_n_batches&gt; batches</span>
<span class="c1">#             if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:</span>
<span class="c1">#                 print(&#39;Epoch {:&gt;3} Batch {:&gt;4}/{}   train_loss = {:.3f}&#39;.format(</span>
<span class="c1">#                     epoch_i,</span>
<span class="c1">#                     batch_i,</span>
<span class="c1">#                     len(batches),</span>
<span class="c1">#                     train_loss))</span>

<span class="c1">#     # Save Model</span>
<span class="c1">#     saver = tf.train.Saver()</span>
<span class="c1">#     saver.save(sess, save_dir)</span>
<span class="c1">#     print(&#39;Model Trained and Saved&#39;)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch   0 Batch    0/215   train_loss = 8.821
Epoch   0 Batch   10/215   train_loss = 6.674
Epoch   0 Batch   20/215   train_loss = 6.200
Epoch   0 Batch   30/215   train_loss = 5.878
Epoch   0 Batch   40/215   train_loss = 6.115
Epoch   0 Batch   50/215   train_loss = 5.896
Epoch   0 Batch   60/215   train_loss = 5.906
Epoch   0 Batch   70/215   train_loss = 5.672
Epoch   0 Batch   80/215   train_loss = 5.504
Epoch   0 Batch   90/215   train_loss = 5.512
Epoch   0 Batch  100/215   train_loss = 5.502
Epoch   0 Batch  110/215   train_loss = 5.424
Epoch   0 Batch  120/215   train_loss = 5.449
Epoch   0 Batch  130/215   train_loss = 5.825
Epoch   0 Batch  140/215   train_loss = 5.918
Epoch   0 Batch  150/215   train_loss = 5.461
Epoch   0 Batch  160/215   train_loss = 5.382
Epoch   0 Batch  170/215   train_loss = 5.493
Epoch   0 Batch  180/215   train_loss = 5.547
Epoch   0 Batch  190/215   train_loss = 5.433
Epoch   0 Batch  200/215   train_loss = 5.351
Epoch   0 Batch  210/215   train_loss = 4.964
Epoch   1 Batch    5/215   train_loss = 5.696
Epoch   1 Batch   15/215   train_loss = 5.477
Epoch   1 Batch   25/215   train_loss = 5.039
Epoch   1 Batch   35/215   train_loss = 5.332
Epoch   1 Batch   45/215   train_loss = 5.079
Epoch   1 Batch   55/215   train_loss = 5.380
Epoch   1 Batch   65/215   train_loss = 5.417
Epoch   1 Batch   75/215   train_loss = 5.113
Epoch   1 Batch   85/215   train_loss = 5.237
Epoch   1 Batch   95/215   train_loss = 5.039
Epoch   1 Batch  105/215   train_loss = 5.182
Epoch   1 Batch  115/215   train_loss = 5.259
Epoch   1 Batch  125/215   train_loss = 5.074
Epoch   1 Batch  135/215   train_loss = 5.041
Epoch   1 Batch  145/215   train_loss = 4.956
Epoch   1 Batch  155/215   train_loss = 5.130
Epoch   1 Batch  165/215   train_loss = 5.216
Epoch   1 Batch  175/215   train_loss = 5.318
Epoch   1 Batch  185/215   train_loss = 5.172
Epoch   1 Batch  195/215   train_loss = 5.211
Epoch   1 Batch  205/215   train_loss = 5.269
Epoch   2 Batch    0/215   train_loss = 5.211
Epoch   2 Batch   10/215   train_loss = 5.351
Epoch   2 Batch   20/215   train_loss = 5.082
Epoch   2 Batch   30/215   train_loss = 4.864
Epoch   2 Batch   40/215   train_loss = 5.217
Epoch   2 Batch   50/215   train_loss = 5.121
Epoch   2 Batch   60/215   train_loss = 5.276
Epoch   2 Batch   70/215   train_loss = 5.082
Epoch   2 Batch   80/215   train_loss = 4.948
Epoch   2 Batch   90/215   train_loss = 5.003
Epoch   2 Batch  100/215   train_loss = 4.880
Epoch   2 Batch  110/215   train_loss = 5.009
Epoch   2 Batch  120/215   train_loss = 5.071
Epoch   2 Batch  130/215   train_loss = 5.285
Epoch   2 Batch  140/215   train_loss = 5.486
Epoch   2 Batch  150/215   train_loss = 4.990
Epoch   2 Batch  160/215   train_loss = 4.957
Epoch   2 Batch  170/215   train_loss = 4.969
Epoch   2 Batch  180/215   train_loss = 5.133
Epoch   2 Batch  190/215   train_loss = 4.985
Epoch   2 Batch  200/215   train_loss = 4.746
Epoch   2 Batch  210/215   train_loss = 4.539
Epoch   3 Batch    5/215   train_loss = 5.270
Epoch   3 Batch   15/215   train_loss = 5.217
Epoch   3 Batch   25/215   train_loss = 4.768
Epoch   3 Batch   35/215   train_loss = 5.002
Epoch   3 Batch   45/215   train_loss = 4.931
Epoch   3 Batch   55/215   train_loss = 5.109
Epoch   3 Batch   65/215   train_loss = 5.170
Epoch   3 Batch   75/215   train_loss = 4.743
Epoch   3 Batch   85/215   train_loss = 4.933
Epoch   3 Batch   95/215   train_loss = 4.792
Epoch   3 Batch  105/215   train_loss = 4.832
Epoch   3 Batch  115/215   train_loss = 4.925
Epoch   3 Batch  125/215   train_loss = 4.748
Epoch   3 Batch  135/215   train_loss = 4.696
Epoch   3 Batch  145/215   train_loss = 4.638
Epoch   3 Batch  155/215   train_loss = 4.862
Epoch   3 Batch  165/215   train_loss = 4.907
Epoch   3 Batch  175/215   train_loss = 4.856
Epoch   3 Batch  185/215   train_loss = 4.861
Epoch   3 Batch  195/215   train_loss = 4.893
Epoch   3 Batch  205/215   train_loss = 4.946
Epoch   4 Batch    0/215   train_loss = 4.788
Epoch   4 Batch   10/215   train_loss = 5.111
Epoch   4 Batch   20/215   train_loss = 4.810
Epoch   4 Batch   30/215   train_loss = 4.694
Epoch   4 Batch   40/215   train_loss = 5.049
Epoch   4 Batch   50/215   train_loss = 5.030
Epoch   4 Batch   60/215   train_loss = 5.009
Epoch   4 Batch   70/215   train_loss = 4.792
Epoch   4 Batch   80/215   train_loss = 4.759
Epoch   4 Batch   90/215   train_loss = 4.696
Epoch   4 Batch  100/215   train_loss = 4.730
Epoch   4 Batch  110/215   train_loss = 4.699
Epoch   4 Batch  120/215   train_loss = 4.811
Epoch   4 Batch  130/215   train_loss = 5.023
Epoch   4 Batch  140/215   train_loss = 5.168
Epoch   4 Batch  150/215   train_loss = 4.816
Epoch   4 Batch  160/215   train_loss = 4.671
Epoch   4 Batch  170/215   train_loss = 4.718
Epoch   4 Batch  180/215   train_loss = 4.860
Epoch   4 Batch  190/215   train_loss = 4.706
Epoch   4 Batch  200/215   train_loss = 4.531
Epoch   4 Batch  210/215   train_loss = 4.274
Epoch   5 Batch    5/215   train_loss = 5.044
Epoch   5 Batch   15/215   train_loss = 5.014
Epoch   5 Batch   25/215   train_loss = 4.636
Epoch   5 Batch   35/215   train_loss = 4.799
Epoch   5 Batch   45/215   train_loss = 4.681
Epoch   5 Batch   55/215   train_loss = 4.948
Epoch   5 Batch   65/215   train_loss = 4.893
Epoch   5 Batch   75/215   train_loss = 4.577
Epoch   5 Batch   85/215   train_loss = 4.689
Epoch   5 Batch   95/215   train_loss = 4.569
Epoch   5 Batch  105/215   train_loss = 4.617
Epoch   5 Batch  115/215   train_loss = 4.718
Epoch   5 Batch  125/215   train_loss = 4.498
Epoch   5 Batch  135/215   train_loss = 4.609
Epoch   5 Batch  145/215   train_loss = 4.415
Epoch   5 Batch  155/215   train_loss = 4.604
Epoch   5 Batch  165/215   train_loss = 4.634
Epoch   5 Batch  175/215   train_loss = 4.735
Epoch   5 Batch  185/215   train_loss = 4.714
Epoch   5 Batch  195/215   train_loss = 4.656
Epoch   5 Batch  205/215   train_loss = 4.768
Epoch   6 Batch    0/215   train_loss = 4.705
Epoch   6 Batch   10/215   train_loss = 4.976
Epoch   6 Batch   20/215   train_loss = 4.651
Epoch   6 Batch   30/215   train_loss = 4.480
Epoch   6 Batch   40/215   train_loss = 4.811
Epoch   6 Batch   50/215   train_loss = 4.811
Epoch   6 Batch   60/215   train_loss = 4.834
Epoch   6 Batch   70/215   train_loss = 4.625
Epoch   6 Batch   80/215   train_loss = 4.549
Epoch   6 Batch   90/215   train_loss = 4.564
Epoch   6 Batch  100/215   train_loss = 4.537
Epoch   6 Batch  110/215   train_loss = 4.555
Epoch   6 Batch  120/215   train_loss = 4.618
Epoch   6 Batch  130/215   train_loss = 4.898
Epoch   6 Batch  140/215   train_loss = 4.999
Epoch   6 Batch  150/215   train_loss = 4.656
Epoch   6 Batch  160/215   train_loss = 4.594
Epoch   6 Batch  170/215   train_loss = 4.493
Epoch   6 Batch  180/215   train_loss = 4.773
Epoch   6 Batch  190/215   train_loss = 4.615
Epoch   6 Batch  200/215   train_loss = 4.369
Epoch   6 Batch  210/215   train_loss = 4.223
Epoch   7 Batch    5/215   train_loss = 4.906
Epoch   7 Batch   15/215   train_loss = 4.927
Epoch   7 Batch   25/215   train_loss = 4.405
Epoch   7 Batch   35/215   train_loss = 4.699
Epoch   7 Batch   45/215   train_loss = 4.487
Epoch   7 Batch   55/215   train_loss = 4.785
Epoch   7 Batch   65/215   train_loss = 4.822
Epoch   7 Batch   75/215   train_loss = 4.417
Epoch   7 Batch   85/215   train_loss = 4.606
Epoch   7 Batch   95/215   train_loss = 4.524
Epoch   7 Batch  105/215   train_loss = 4.486
Epoch   7 Batch  115/215   train_loss = 4.604
Epoch   7 Batch  125/215   train_loss = 4.393
Epoch   7 Batch  135/215   train_loss = 4.463
Epoch   7 Batch  145/215   train_loss = 4.298
Epoch   7 Batch  155/215   train_loss = 4.509
Epoch   7 Batch  165/215   train_loss = 4.574
Epoch   7 Batch  175/215   train_loss = 4.700
Epoch   7 Batch  185/215   train_loss = 4.639
Epoch   7 Batch  195/215   train_loss = 4.527
Epoch   7 Batch  205/215   train_loss = 4.709
Epoch   8 Batch    0/215   train_loss = 4.534
Epoch   8 Batch   10/215   train_loss = 4.782
Epoch   8 Batch   20/215   train_loss = 4.586
Epoch   8 Batch   30/215   train_loss = 4.447
Epoch   8 Batch   40/215   train_loss = 4.716
Epoch   8 Batch   50/215   train_loss = 4.704
Epoch   8 Batch   60/215   train_loss = 4.648
Epoch   8 Batch   70/215   train_loss = 4.537
Epoch   8 Batch   80/215   train_loss = 4.534
Epoch   8 Batch   90/215   train_loss = 4.467
Epoch   8 Batch  100/215   train_loss = 4.499
Epoch   8 Batch  110/215   train_loss = 4.397
Epoch   8 Batch  120/215   train_loss = 4.564
Epoch   8 Batch  130/215   train_loss = 4.811
Epoch   8 Batch  140/215   train_loss = 4.851
Epoch   8 Batch  150/215   train_loss = 4.566
Epoch   8 Batch  160/215   train_loss = 4.478
Epoch   8 Batch  170/215   train_loss = 4.468
Epoch   8 Batch  180/215   train_loss = 4.646
Epoch   8 Batch  190/215   train_loss = 4.518
Epoch   8 Batch  200/215   train_loss = 4.268
Epoch   8 Batch  210/215   train_loss = 4.147
Epoch   9 Batch    5/215   train_loss = 4.776
Epoch   9 Batch   15/215   train_loss = 4.703
Epoch   9 Batch   25/215   train_loss = 4.402
Epoch   9 Batch   35/215   train_loss = 4.604
Epoch   9 Batch   45/215   train_loss = 4.421
Epoch   9 Batch   55/215   train_loss = 4.744
Epoch   9 Batch   65/215   train_loss = 4.675
Epoch   9 Batch   75/215   train_loss = 4.351
Epoch   9 Batch   85/215   train_loss = 4.491
Epoch   9 Batch   95/215   train_loss = 4.394
Epoch   9 Batch  105/215   train_loss = 4.200
Epoch   9 Batch  115/215   train_loss = 4.406
Epoch   9 Batch  125/215   train_loss = 4.173
Epoch   9 Batch  135/215   train_loss = 4.388
Epoch   9 Batch  145/215   train_loss = 4.230
Epoch   9 Batch  155/215   train_loss = 4.447
Epoch   9 Batch  165/215   train_loss = 4.425
Epoch   9 Batch  175/215   train_loss = 4.433
Epoch   9 Batch  185/215   train_loss = 4.547
Epoch   9 Batch  195/215   train_loss = 4.493
Epoch   9 Batch  205/215   train_loss = 4.524
Epoch  10 Batch    0/215   train_loss = 4.302
Epoch  10 Batch   10/215   train_loss = 4.721
Epoch  10 Batch   20/215   train_loss = 4.408
Epoch  10 Batch   30/215   train_loss = 4.313
Epoch  10 Batch   40/215   train_loss = 4.672
Epoch  10 Batch   50/215   train_loss = 4.633
Epoch  10 Batch   60/215   train_loss = 4.617
Epoch  10 Batch   70/215   train_loss = 4.346
Epoch  10 Batch   80/215   train_loss = 4.372
Epoch  10 Batch   90/215   train_loss = 4.373
Epoch  10 Batch  100/215   train_loss = 4.433
Epoch  10 Batch  110/215   train_loss = 4.352
Epoch  10 Batch  120/215   train_loss = 4.433
Epoch  10 Batch  130/215   train_loss = 4.681
Epoch  10 Batch  140/215   train_loss = 4.685
Epoch  10 Batch  150/215   train_loss = 4.410
Epoch  10 Batch  160/215   train_loss = 4.359
Epoch  10 Batch  170/215   train_loss = 4.329
Epoch  10 Batch  180/215   train_loss = 4.591
Epoch  10 Batch  190/215   train_loss = 4.379
Epoch  10 Batch  200/215   train_loss = 4.217
Epoch  10 Batch  210/215   train_loss = 3.955
Epoch  11 Batch    5/215   train_loss = 4.618
Epoch  11 Batch   15/215   train_loss = 4.480
Epoch  11 Batch   25/215   train_loss = 4.203
Epoch  11 Batch   35/215   train_loss = 4.390
Epoch  11 Batch   45/215   train_loss = 4.343
Epoch  11 Batch   55/215   train_loss = 4.461
Epoch  11 Batch   65/215   train_loss = 4.481
Epoch  11 Batch   75/215   train_loss = 4.202
Epoch  11 Batch   85/215   train_loss = 4.343
Epoch  11 Batch   95/215   train_loss = 4.237
Epoch  11 Batch  105/215   train_loss = 4.238
Epoch  11 Batch  115/215   train_loss = 4.304
Epoch  11 Batch  125/215   train_loss = 4.128
Epoch  11 Batch  135/215   train_loss = 4.375
Epoch  11 Batch  145/215   train_loss = 4.079
Epoch  11 Batch  155/215   train_loss = 4.347
Epoch  11 Batch  165/215   train_loss = 4.323
Epoch  11 Batch  175/215   train_loss = 4.370
Epoch  11 Batch  185/215   train_loss = 4.440
Epoch  11 Batch  195/215   train_loss = 4.333
Epoch  11 Batch  205/215   train_loss = 4.415
Epoch  12 Batch    0/215   train_loss = 4.315
Epoch  12 Batch   10/215   train_loss = 4.682
Epoch  12 Batch   20/215   train_loss = 4.341
Epoch  12 Batch   30/215   train_loss = 4.206
Epoch  12 Batch   40/215   train_loss = 4.614
Epoch  12 Batch   50/215   train_loss = 4.398
Epoch  12 Batch   60/215   train_loss = 4.532
Epoch  12 Batch   70/215   train_loss = 4.338
Epoch  12 Batch   80/215   train_loss = 4.225
Epoch  12 Batch   90/215   train_loss = 4.281
Epoch  12 Batch  100/215   train_loss = 4.190
Epoch  12 Batch  110/215   train_loss = 4.324
Epoch  12 Batch  120/215   train_loss = 4.304
Epoch  12 Batch  130/215   train_loss = 4.625
Epoch  12 Batch  140/215   train_loss = 4.600
Epoch  12 Batch  150/215   train_loss = 4.347
Epoch  12 Batch  160/215   train_loss = 4.301
Epoch  12 Batch  170/215   train_loss = 4.176
Epoch  12 Batch  180/215   train_loss = 4.483
Epoch  12 Batch  190/215   train_loss = 4.326
Epoch  12 Batch  200/215   train_loss = 4.006
Epoch  12 Batch  210/215   train_loss = 3.992
Epoch  13 Batch    5/215   train_loss = 4.469
Epoch  13 Batch   15/215   train_loss = 4.415
Epoch  13 Batch   25/215   train_loss = 4.190
Epoch  13 Batch   35/215   train_loss = 4.238
Epoch  13 Batch   45/215   train_loss = 4.254
Epoch  13 Batch   55/215   train_loss = 4.269
Epoch  13 Batch   65/215   train_loss = 4.440
Epoch  13 Batch   75/215   train_loss = 4.193
Epoch  13 Batch   85/215   train_loss = 4.325
Epoch  13 Batch   95/215   train_loss = 4.131
Epoch  13 Batch  105/215   train_loss = 4.086
Epoch  13 Batch  115/215   train_loss = 4.416
Epoch  13 Batch  125/215   train_loss = 4.099
Epoch  13 Batch  135/215   train_loss = 4.136
Epoch  13 Batch  145/215   train_loss = 3.992
Epoch  13 Batch  155/215   train_loss = 4.218
Epoch  13 Batch  165/215   train_loss = 4.223
Epoch  13 Batch  175/215   train_loss = 4.330
Epoch  13 Batch  185/215   train_loss = 4.302
Epoch  13 Batch  195/215   train_loss = 4.213
Epoch  13 Batch  205/215   train_loss = 4.343
Epoch  14 Batch    0/215   train_loss = 4.221
Epoch  14 Batch   10/215   train_loss = 4.460
Epoch  14 Batch   20/215   train_loss = 4.288
Epoch  14 Batch   30/215   train_loss = 4.008
Epoch  14 Batch   40/215   train_loss = 4.303
Epoch  14 Batch   50/215   train_loss = 4.286
Epoch  14 Batch   60/215   train_loss = 4.363
Epoch  14 Batch   70/215   train_loss = 4.119
Epoch  14 Batch   80/215   train_loss = 4.170
Epoch  14 Batch   90/215   train_loss = 4.222
Epoch  14 Batch  100/215   train_loss = 4.178
Epoch  14 Batch  110/215   train_loss = 4.190
Epoch  14 Batch  120/215   train_loss = 4.275
Epoch  14 Batch  130/215   train_loss = 4.517
Epoch  14 Batch  140/215   train_loss = 4.408
Epoch  14 Batch  150/215   train_loss = 4.190
Epoch  14 Batch  160/215   train_loss = 4.182
Epoch  14 Batch  170/215   train_loss = 4.136
Epoch  14 Batch  180/215   train_loss = 4.366
Epoch  14 Batch  190/215   train_loss = 4.208
Epoch  14 Batch  200/215   train_loss = 4.018
Epoch  14 Batch  210/215   train_loss = 3.858
Epoch  15 Batch    5/215   train_loss = 4.426
Epoch  15 Batch   15/215   train_loss = 4.323
Epoch  15 Batch   25/215   train_loss = 4.048
Epoch  15 Batch   35/215   train_loss = 4.249
Epoch  15 Batch   45/215   train_loss = 4.146
Epoch  15 Batch   55/215   train_loss = 4.161
Epoch  15 Batch   65/215   train_loss = 4.321
Epoch  15 Batch   75/215   train_loss = 4.047
Epoch  15 Batch   85/215   train_loss = 4.158
Epoch  15 Batch   95/215   train_loss = 4.073
Epoch  15 Batch  105/215   train_loss = 4.028
Epoch  15 Batch  115/215   train_loss = 4.237
Epoch  15 Batch  125/215   train_loss = 3.985
Epoch  15 Batch  135/215   train_loss = 4.024
Epoch  15 Batch  145/215   train_loss = 3.917
Epoch  15 Batch  155/215   train_loss = 4.090
Epoch  15 Batch  165/215   train_loss = 4.110
Epoch  15 Batch  175/215   train_loss = 4.293
Epoch  15 Batch  185/215   train_loss = 4.166
Epoch  15 Batch  195/215   train_loss = 4.154
Epoch  15 Batch  205/215   train_loss = 4.209
Epoch  16 Batch    0/215   train_loss = 4.070
Epoch  16 Batch   10/215   train_loss = 4.394
Epoch  16 Batch   20/215   train_loss = 4.112
Epoch  16 Batch   30/215   train_loss = 3.985
Epoch  16 Batch   40/215   train_loss = 4.247
Epoch  16 Batch   50/215   train_loss = 4.212
Epoch  16 Batch   60/215   train_loss = 4.317
Epoch  16 Batch   70/215   train_loss = 4.094
Epoch  16 Batch   80/215   train_loss = 4.035
Epoch  16 Batch   90/215   train_loss = 4.110
Epoch  16 Batch  100/215   train_loss = 4.201
Epoch  16 Batch  110/215   train_loss = 3.998
Epoch  16 Batch  120/215   train_loss = 4.169
Epoch  16 Batch  130/215   train_loss = 4.441
Epoch  16 Batch  140/215   train_loss = 4.249
Epoch  16 Batch  150/215   train_loss = 4.037
Epoch  16 Batch  160/215   train_loss = 4.006
Epoch  16 Batch  170/215   train_loss = 4.046
Epoch  16 Batch  180/215   train_loss = 4.275
Epoch  16 Batch  190/215   train_loss = 4.088
Epoch  16 Batch  200/215   train_loss = 3.940
Epoch  16 Batch  210/215   train_loss = 3.706
Epoch  17 Batch    5/215   train_loss = 4.335
Epoch  17 Batch   15/215   train_loss = 4.361
Epoch  17 Batch   25/215   train_loss = 3.878
Epoch  17 Batch   35/215   train_loss = 4.137
Epoch  17 Batch   45/215   train_loss = 4.113
Epoch  17 Batch   55/215   train_loss = 4.064
Epoch  17 Batch   65/215   train_loss = 4.181
Epoch  17 Batch   75/215   train_loss = 4.146
Epoch  17 Batch   85/215   train_loss = 4.122
Epoch  17 Batch   95/215   train_loss = 4.101
Epoch  17 Batch  105/215   train_loss = 3.876
Epoch  17 Batch  115/215   train_loss = 4.070
Epoch  17 Batch  125/215   train_loss = 3.846
Epoch  17 Batch  135/215   train_loss = 3.902
Epoch  17 Batch  145/215   train_loss = 3.817
Epoch  17 Batch  155/215   train_loss = 4.035
Epoch  17 Batch  165/215   train_loss = 4.031
Epoch  17 Batch  175/215   train_loss = 4.232
Epoch  17 Batch  185/215   train_loss = 4.122
Epoch  17 Batch  195/215   train_loss = 4.155
Epoch  17 Batch  205/215   train_loss = 4.157
Epoch  18 Batch    0/215   train_loss = 3.984
Epoch  18 Batch   10/215   train_loss = 4.282
Epoch  18 Batch   20/215   train_loss = 4.147
Epoch  18 Batch   30/215   train_loss = 3.987
Epoch  18 Batch   40/215   train_loss = 4.210
Epoch  18 Batch   50/215   train_loss = 4.190
Epoch  18 Batch   60/215   train_loss = 4.255
Epoch  18 Batch   70/215   train_loss = 3.971
Epoch  18 Batch   80/215   train_loss = 3.900
Epoch  18 Batch   90/215   train_loss = 4.146
Epoch  18 Batch  100/215   train_loss = 4.058
Epoch  18 Batch  110/215   train_loss = 4.068
Epoch  18 Batch  120/215   train_loss = 4.103
Epoch  18 Batch  130/215   train_loss = 4.287
Epoch  18 Batch  140/215   train_loss = 4.316
Epoch  18 Batch  150/215   train_loss = 4.047
Epoch  18 Batch  160/215   train_loss = 3.971
Epoch  18 Batch  170/215   train_loss = 3.977
Epoch  18 Batch  180/215   train_loss = 4.133
Epoch  18 Batch  190/215   train_loss = 4.117
Epoch  18 Batch  200/215   train_loss = 3.882
Epoch  18 Batch  210/215   train_loss = 3.704
Epoch  19 Batch    5/215   train_loss = 4.319
Epoch  19 Batch   15/215   train_loss = 4.081
Epoch  19 Batch   25/215   train_loss = 3.855
Epoch  19 Batch   35/215   train_loss = 3.983
Epoch  19 Batch   45/215   train_loss = 3.960
Epoch  19 Batch   55/215   train_loss = 4.038
Epoch  19 Batch   65/215   train_loss = 4.137
Epoch  19 Batch   75/215   train_loss = 4.006
Epoch  19 Batch   85/215   train_loss = 4.028
Epoch  19 Batch   95/215   train_loss = 3.873
Epoch  19 Batch  105/215   train_loss = 3.885
Epoch  19 Batch  115/215   train_loss = 3.998
Epoch  19 Batch  125/215   train_loss = 3.795
Epoch  19 Batch  135/215   train_loss = 3.923
Epoch  19 Batch  145/215   train_loss = 3.928
Epoch  19 Batch  155/215   train_loss = 4.045
Epoch  19 Batch  165/215   train_loss = 3.942
Epoch  19 Batch  175/215   train_loss = 4.062
Epoch  19 Batch  185/215   train_loss = 4.000
Epoch  19 Batch  195/215   train_loss = 4.055
Epoch  19 Batch  205/215   train_loss = 4.110
Epoch  20 Batch    0/215   train_loss = 3.879
Epoch  20 Batch   10/215   train_loss = 4.241
Epoch  20 Batch   20/215   train_loss = 4.024
Epoch  20 Batch   30/215   train_loss = 3.995
Epoch  20 Batch   40/215   train_loss = 4.172
Epoch  20 Batch   50/215   train_loss = 4.049
Epoch  20 Batch   60/215   train_loss = 4.200
Epoch  20 Batch   70/215   train_loss = 3.808
Epoch  20 Batch   80/215   train_loss = 3.865
Epoch  20 Batch   90/215   train_loss = 4.044
Epoch  20 Batch  100/215   train_loss = 3.868
Epoch  20 Batch  110/215   train_loss = 3.929
Epoch  20 Batch  120/215   train_loss = 4.002
Epoch  20 Batch  130/215   train_loss = 4.341
Epoch  20 Batch  140/215   train_loss = 4.215
Epoch  20 Batch  150/215   train_loss = 3.928
Epoch  20 Batch  160/215   train_loss = 3.973
Epoch  20 Batch  170/215   train_loss = 3.904
Epoch  20 Batch  180/215   train_loss = 4.038
Epoch  20 Batch  190/215   train_loss = 4.025
Epoch  20 Batch  200/215   train_loss = 3.824
Epoch  20 Batch  210/215   train_loss = 3.792
Epoch  21 Batch    5/215   train_loss = 4.220
Epoch  21 Batch   15/215   train_loss = 4.012
Epoch  21 Batch   25/215   train_loss = 3.911
Epoch  21 Batch   35/215   train_loss = 4.025
Epoch  21 Batch   45/215   train_loss = 3.873
Epoch  21 Batch   55/215   train_loss = 4.106
Epoch  21 Batch   65/215   train_loss = 4.123
Epoch  21 Batch   75/215   train_loss = 4.008
Epoch  21 Batch   85/215   train_loss = 4.029
Epoch  21 Batch   95/215   train_loss = 3.850
Epoch  21 Batch  105/215   train_loss = 3.885
Epoch  21 Batch  115/215   train_loss = 3.949
Epoch  21 Batch  125/215   train_loss = 3.736
Epoch  21 Batch  135/215   train_loss = 3.850
Epoch  21 Batch  145/215   train_loss = 3.796
Epoch  21 Batch  155/215   train_loss = 3.895
Epoch  21 Batch  165/215   train_loss = 3.872
Epoch  21 Batch  175/215   train_loss = 4.139
Epoch  21 Batch  185/215   train_loss = 3.972
Epoch  21 Batch  195/215   train_loss = 3.831
Epoch  21 Batch  205/215   train_loss = 4.000
Epoch  22 Batch    0/215   train_loss = 3.800
Epoch  22 Batch   10/215   train_loss = 4.170
Epoch  22 Batch   20/215   train_loss = 4.020
Epoch  22 Batch   30/215   train_loss = 3.899
Epoch  22 Batch   40/215   train_loss = 3.989
Epoch  22 Batch   50/215   train_loss = 4.056
Epoch  22 Batch   60/215   train_loss = 4.126
Epoch  22 Batch   70/215   train_loss = 3.875
Epoch  22 Batch   80/215   train_loss = 3.885
Epoch  22 Batch   90/215   train_loss = 3.847
Epoch  22 Batch  100/215   train_loss = 4.021
Epoch  22 Batch  110/215   train_loss = 3.886
Epoch  22 Batch  120/215   train_loss = 3.867
Epoch  22 Batch  130/215   train_loss = 4.299
Epoch  22 Batch  140/215   train_loss = 4.160
Epoch  22 Batch  150/215   train_loss = 3.768
Epoch  22 Batch  160/215   train_loss = 3.933
Epoch  22 Batch  170/215   train_loss = 3.856
Epoch  22 Batch  180/215   train_loss = 3.959
Epoch  22 Batch  190/215   train_loss = 3.943
Epoch  22 Batch  200/215   train_loss = 3.762
Epoch  22 Batch  210/215   train_loss = 3.459
Epoch  23 Batch    5/215   train_loss = 4.237
Epoch  23 Batch   15/215   train_loss = 4.039
Epoch  23 Batch   25/215   train_loss = 3.778
Epoch  23 Batch   35/215   train_loss = 4.049
Epoch  23 Batch   45/215   train_loss = 3.876
Epoch  23 Batch   55/215   train_loss = 3.924
Epoch  23 Batch   65/215   train_loss = 3.942
Epoch  23 Batch   75/215   train_loss = 3.799
Epoch  23 Batch   85/215   train_loss = 3.926
Epoch  23 Batch   95/215   train_loss = 3.895
Epoch  23 Batch  105/215   train_loss = 3.788
Epoch  23 Batch  115/215   train_loss = 3.861
Epoch  23 Batch  125/215   train_loss = 3.706
Epoch  23 Batch  135/215   train_loss = 3.840
Epoch  23 Batch  145/215   train_loss = 3.651
Epoch  23 Batch  155/215   train_loss = 3.908
Epoch  23 Batch  165/215   train_loss = 3.674
Epoch  23 Batch  175/215   train_loss = 3.983
Epoch  23 Batch  185/215   train_loss = 3.943
Epoch  23 Batch  195/215   train_loss = 3.751
Epoch  23 Batch  205/215   train_loss = 3.967
Epoch  24 Batch    0/215   train_loss = 3.777
Epoch  24 Batch   10/215   train_loss = 4.121
Epoch  24 Batch   20/215   train_loss = 4.047
Epoch  24 Batch   30/215   train_loss = 3.710
Epoch  24 Batch   40/215   train_loss = 3.802
Epoch  24 Batch   50/215   train_loss = 3.932
Epoch  24 Batch   60/215   train_loss = 3.991
Epoch  24 Batch   70/215   train_loss = 3.844
Epoch  24 Batch   80/215   train_loss = 3.746
Epoch  24 Batch   90/215   train_loss = 3.978
Epoch  24 Batch  100/215   train_loss = 4.018
Epoch  24 Batch  110/215   train_loss = 3.841
Epoch  24 Batch  120/215   train_loss = 3.845
Epoch  24 Batch  130/215   train_loss = 4.315
Epoch  24 Batch  140/215   train_loss = 4.054
Epoch  24 Batch  150/215   train_loss = 3.892
Epoch  24 Batch  160/215   train_loss = 3.749
Epoch  24 Batch  170/215   train_loss = 3.793
Epoch  24 Batch  180/215   train_loss = 3.935
Epoch  24 Batch  190/215   train_loss = 3.911
Epoch  24 Batch  200/215   train_loss = 3.714
Epoch  24 Batch  210/215   train_loss = 3.419
Epoch  25 Batch    5/215   train_loss = 4.192
Epoch  25 Batch   15/215   train_loss = 3.976
Epoch  25 Batch   25/215   train_loss = 3.842
Epoch  25 Batch   35/215   train_loss = 3.894
Epoch  25 Batch   45/215   train_loss = 3.853
Epoch  25 Batch   55/215   train_loss = 3.915
Epoch  25 Batch   65/215   train_loss = 3.990
Epoch  25 Batch   75/215   train_loss = 3.880
Epoch  25 Batch   85/215   train_loss = 3.823
Epoch  25 Batch   95/215   train_loss = 3.714
Epoch  25 Batch  105/215   train_loss = 3.843
Epoch  25 Batch  115/215   train_loss = 3.825
Epoch  25 Batch  125/215   train_loss = 3.589
Epoch  25 Batch  135/215   train_loss = 3.866
Epoch  25 Batch  145/215   train_loss = 3.778
Epoch  25 Batch  155/215   train_loss = 3.798
Epoch  25 Batch  165/215   train_loss = 3.685
Epoch  25 Batch  175/215   train_loss = 3.983
Epoch  25 Batch  185/215   train_loss = 3.811
Epoch  25 Batch  195/215   train_loss = 3.707
Epoch  25 Batch  205/215   train_loss = 3.931
Epoch  26 Batch    0/215   train_loss = 3.830
Epoch  26 Batch   10/215   train_loss = 3.934
Epoch  26 Batch   20/215   train_loss = 3.851
Epoch  26 Batch   30/215   train_loss = 3.709
Epoch  26 Batch   40/215   train_loss = 3.988
Epoch  26 Batch   50/215   train_loss = 4.000
Epoch  26 Batch   60/215   train_loss = 3.952
Epoch  26 Batch   70/215   train_loss = 3.848
Epoch  26 Batch   80/215   train_loss = 3.758
Epoch  26 Batch   90/215   train_loss = 3.780
Epoch  26 Batch  100/215   train_loss = 3.808
Epoch  26 Batch  110/215   train_loss = 3.674
Epoch  26 Batch  120/215   train_loss = 3.779
Epoch  26 Batch  130/215   train_loss = 4.061
Epoch  26 Batch  140/215   train_loss = 4.001
Epoch  26 Batch  150/215   train_loss = 3.805
Epoch  26 Batch  160/215   train_loss = 3.769
Epoch  26 Batch  170/215   train_loss = 3.874
Epoch  26 Batch  180/215   train_loss = 3.856
Epoch  26 Batch  190/215   train_loss = 3.803
Epoch  26 Batch  200/215   train_loss = 3.772
Epoch  26 Batch  210/215   train_loss = 3.546
Epoch  27 Batch    5/215   train_loss = 3.985
Epoch  27 Batch   15/215   train_loss = 3.761
Epoch  27 Batch   25/215   train_loss = 3.822
Epoch  27 Batch   35/215   train_loss = 3.782
Epoch  27 Batch   45/215   train_loss = 3.852
Epoch  27 Batch   55/215   train_loss = 3.795
Epoch  27 Batch   65/215   train_loss = 3.898
Epoch  27 Batch   75/215   train_loss = 3.824
Epoch  27 Batch   85/215   train_loss = 3.796
Epoch  27 Batch   95/215   train_loss = 3.735
Epoch  27 Batch  105/215   train_loss = 3.671
Epoch  27 Batch  115/215   train_loss = 3.883
Epoch  27 Batch  125/215   train_loss = 3.700
Epoch  27 Batch  135/215   train_loss = 3.858
Epoch  27 Batch  145/215   train_loss = 3.713
Epoch  27 Batch  155/215   train_loss = 3.806
Epoch  27 Batch  165/215   train_loss = 3.690
Epoch  27 Batch  175/215   train_loss = 3.818
Epoch  27 Batch  185/215   train_loss = 3.914
Epoch  27 Batch  195/215   train_loss = 3.705
Epoch  27 Batch  205/215   train_loss = 3.887
Epoch  28 Batch    0/215   train_loss = 3.703
Epoch  28 Batch   10/215   train_loss = 4.014
Epoch  28 Batch   20/215   train_loss = 3.850
Epoch  28 Batch   30/215   train_loss = 3.630
Epoch  28 Batch   40/215   train_loss = 3.899
Epoch  28 Batch   50/215   train_loss = 3.871
Epoch  28 Batch   60/215   train_loss = 3.911
Epoch  28 Batch   70/215   train_loss = 3.868
Epoch  28 Batch   80/215   train_loss = 3.673
Epoch  28 Batch   90/215   train_loss = 3.898
Epoch  28 Batch  100/215   train_loss = 3.840
Epoch  28 Batch  110/215   train_loss = 3.719
Epoch  28 Batch  120/215   train_loss = 3.778
Epoch  28 Batch  130/215   train_loss = 3.927
Epoch  28 Batch  140/215   train_loss = 3.916
Epoch  28 Batch  150/215   train_loss = 3.821
Epoch  28 Batch  160/215   train_loss = 3.616
Epoch  28 Batch  170/215   train_loss = 3.742
Epoch  28 Batch  180/215   train_loss = 3.900
Epoch  28 Batch  190/215   train_loss = 3.808
Epoch  28 Batch  200/215   train_loss = 3.617
Epoch  28 Batch  210/215   train_loss = 3.347
Epoch  29 Batch    5/215   train_loss = 4.067
Epoch  29 Batch   15/215   train_loss = 3.755
Epoch  29 Batch   25/215   train_loss = 3.688
Epoch  29 Batch   35/215   train_loss = 3.851
Epoch  29 Batch   45/215   train_loss = 3.792
Epoch  29 Batch   55/215   train_loss = 3.884
Epoch  29 Batch   65/215   train_loss = 3.856
Epoch  29 Batch   75/215   train_loss = 3.831
Epoch  29 Batch   85/215   train_loss = 3.680
Epoch  29 Batch   95/215   train_loss = 3.726
Epoch  29 Batch  105/215   train_loss = 3.708
Epoch  29 Batch  115/215   train_loss = 3.879
Epoch  29 Batch  125/215   train_loss = 3.469
Epoch  29 Batch  135/215   train_loss = 3.803
Epoch  29 Batch  145/215   train_loss = 3.684
Epoch  29 Batch  155/215   train_loss = 3.887
Epoch  29 Batch  165/215   train_loss = 3.702
Epoch  29 Batch  175/215   train_loss = 3.905
Epoch  29 Batch  185/215   train_loss = 3.854
Epoch  29 Batch  195/215   train_loss = 3.630
Epoch  29 Batch  205/215   train_loss = 3.922
Epoch  30 Batch    0/215   train_loss = 3.740
Epoch  30 Batch   10/215   train_loss = 4.076
Epoch  30 Batch   20/215   train_loss = 3.676
Epoch  30 Batch   30/215   train_loss = 3.713
Epoch  30 Batch   40/215   train_loss = 3.850
Epoch  30 Batch   50/215   train_loss = 3.668
Epoch  30 Batch   60/215   train_loss = 3.789
Epoch  30 Batch   70/215   train_loss = 3.758
Epoch  30 Batch   80/215   train_loss = 3.601
Epoch  30 Batch   90/215   train_loss = 3.622
Epoch  30 Batch  100/215   train_loss = 3.747
Epoch  30 Batch  110/215   train_loss = 3.784
Epoch  30 Batch  120/215   train_loss = 3.721
Epoch  30 Batch  130/215   train_loss = 4.169
Epoch  30 Batch  140/215   train_loss = 3.980
Epoch  30 Batch  150/215   train_loss = 3.645
Epoch  30 Batch  160/215   train_loss = 3.761
Epoch  30 Batch  170/215   train_loss = 3.676
Epoch  30 Batch  180/215   train_loss = 3.910
Epoch  30 Batch  190/215   train_loss = 3.777
Epoch  30 Batch  200/215   train_loss = 3.629
Epoch  30 Batch  210/215   train_loss = 3.367
Epoch  31 Batch    5/215   train_loss = 4.007
Epoch  31 Batch   15/215   train_loss = 3.856
Epoch  31 Batch   25/215   train_loss = 3.660
Epoch  31 Batch   35/215   train_loss = 3.846
Epoch  31 Batch   45/215   train_loss = 3.578
Epoch  31 Batch   55/215   train_loss = 3.714
Epoch  31 Batch   65/215   train_loss = 3.901
Epoch  31 Batch   75/215   train_loss = 3.710
Epoch  31 Batch   85/215   train_loss = 3.795
Epoch  31 Batch   95/215   train_loss = 3.811
Epoch  31 Batch  105/215   train_loss = 3.650
Epoch  31 Batch  115/215   train_loss = 3.677
Epoch  31 Batch  125/215   train_loss = 3.610
Epoch  31 Batch  135/215   train_loss = 3.736
Epoch  31 Batch  145/215   train_loss = 3.517
Epoch  31 Batch  155/215   train_loss = 3.770
Epoch  31 Batch  165/215   train_loss = 3.631
Epoch  31 Batch  175/215   train_loss = 3.771
Epoch  31 Batch  185/215   train_loss = 3.744
Epoch  31 Batch  195/215   train_loss = 3.722
Epoch  31 Batch  205/215   train_loss = 3.797
Epoch  32 Batch    0/215   train_loss = 3.666
Epoch  32 Batch   10/215   train_loss = 4.053
Epoch  32 Batch   20/215   train_loss = 3.690
Epoch  32 Batch   30/215   train_loss = 3.655
Epoch  32 Batch   40/215   train_loss = 3.662
Epoch  32 Batch   50/215   train_loss = 3.666
Epoch  32 Batch   60/215   train_loss = 3.938
Epoch  32 Batch   70/215   train_loss = 3.638
Epoch  32 Batch   80/215   train_loss = 3.666
Epoch  32 Batch   90/215   train_loss = 3.808
Epoch  32 Batch  100/215   train_loss = 3.721
Epoch  32 Batch  110/215   train_loss = 3.632
Epoch  32 Batch  120/215   train_loss = 3.776
Epoch  32 Batch  130/215   train_loss = 3.942
Epoch  32 Batch  140/215   train_loss = 4.016
Epoch  32 Batch  150/215   train_loss = 3.707
Epoch  32 Batch  160/215   train_loss = 3.857
Epoch  32 Batch  170/215   train_loss = 3.767
Epoch  32 Batch  180/215   train_loss = 3.749
Epoch  32 Batch  190/215   train_loss = 3.717
Epoch  32 Batch  200/215   train_loss = 3.566
Epoch  32 Batch  210/215   train_loss = 3.349
Epoch  33 Batch    5/215   train_loss = 3.981
Epoch  33 Batch   15/215   train_loss = 3.732
Epoch  33 Batch   25/215   train_loss = 3.704
Epoch  33 Batch   35/215   train_loss = 3.664
Epoch  33 Batch   45/215   train_loss = 3.744
Epoch  33 Batch   55/215   train_loss = 3.591
Epoch  33 Batch   65/215   train_loss = 3.764
Epoch  33 Batch   75/215   train_loss = 3.759
Epoch  33 Batch   85/215   train_loss = 3.687
Epoch  33 Batch   95/215   train_loss = 3.595
Epoch  33 Batch  105/215   train_loss = 3.650
Epoch  33 Batch  115/215   train_loss = 3.786
Epoch  33 Batch  125/215   train_loss = 3.580
Epoch  33 Batch  135/215   train_loss = 3.753
Epoch  33 Batch  145/215   train_loss = 3.587
Epoch  33 Batch  155/215   train_loss = 3.726
Epoch  33 Batch  165/215   train_loss = 3.579
Epoch  33 Batch  175/215   train_loss = 3.779
Epoch  33 Batch  185/215   train_loss = 3.683
Epoch  33 Batch  195/215   train_loss = 3.614
Epoch  33 Batch  205/215   train_loss = 3.683
Epoch  34 Batch    0/215   train_loss = 3.788
Epoch  34 Batch   10/215   train_loss = 4.019
Epoch  34 Batch   20/215   train_loss = 3.795
Epoch  34 Batch   30/215   train_loss = 3.627
Epoch  34 Batch   40/215   train_loss = 3.791
Epoch  34 Batch   50/215   train_loss = 3.676
Epoch  34 Batch   60/215   train_loss = 3.803
Epoch  34 Batch   70/215   train_loss = 3.777
Epoch  34 Batch   80/215   train_loss = 3.508
Epoch  34 Batch   90/215   train_loss = 3.907
Epoch  34 Batch  100/215   train_loss = 3.713
Epoch  34 Batch  110/215   train_loss = 3.656
Epoch  34 Batch  120/215   train_loss = 3.700
Epoch  34 Batch  130/215   train_loss = 3.987
Epoch  34 Batch  140/215   train_loss = 3.759
Epoch  34 Batch  150/215   train_loss = 3.683
Epoch  34 Batch  160/215   train_loss = 3.748
Epoch  34 Batch  170/215   train_loss = 3.644
Epoch  34 Batch  180/215   train_loss = 3.694
Epoch  34 Batch  190/215   train_loss = 3.748
Epoch  34 Batch  200/215   train_loss = 3.425
Epoch  34 Batch  210/215   train_loss = 3.347
Epoch  35 Batch    5/215   train_loss = 4.015
Epoch  35 Batch   15/215   train_loss = 3.813
Epoch  35 Batch   25/215   train_loss = 3.633
Epoch  35 Batch   35/215   train_loss = 3.682
Epoch  35 Batch   45/215   train_loss = 3.547
Epoch  35 Batch   55/215   train_loss = 3.641
Epoch  35 Batch   65/215   train_loss = 3.721
Epoch  35 Batch   75/215   train_loss = 3.740
Epoch  35 Batch   85/215   train_loss = 3.831
Epoch  35 Batch   95/215   train_loss = 3.565
Epoch  35 Batch  105/215   train_loss = 3.565
Epoch  35 Batch  115/215   train_loss = 3.794
Epoch  35 Batch  125/215   train_loss = 3.538
Epoch  35 Batch  135/215   train_loss = 3.749
Epoch  35 Batch  145/215   train_loss = 3.610
Epoch  35 Batch  155/215   train_loss = 3.683
Epoch  35 Batch  165/215   train_loss = 3.527
Epoch  35 Batch  175/215   train_loss = 3.696
Epoch  35 Batch  185/215   train_loss = 3.660
Epoch  35 Batch  195/215   train_loss = 3.517
Epoch  35 Batch  205/215   train_loss = 3.762
Epoch  36 Batch    0/215   train_loss = 3.624
Epoch  36 Batch   10/215   train_loss = 3.806
Epoch  36 Batch   20/215   train_loss = 3.769
Epoch  36 Batch   30/215   train_loss = 3.437
Epoch  36 Batch   40/215   train_loss = 3.668
Epoch  36 Batch   50/215   train_loss = 3.614
Epoch  36 Batch   60/215   train_loss = 3.781
Epoch  36 Batch   70/215   train_loss = 3.675
Epoch  36 Batch   80/215   train_loss = 3.635
Epoch  36 Batch   90/215   train_loss = 3.813
Epoch  36 Batch  100/215   train_loss = 3.719
Epoch  36 Batch  110/215   train_loss = 3.755
Epoch  36 Batch  120/215   train_loss = 3.713
Epoch  36 Batch  130/215   train_loss = 3.990
Epoch  36 Batch  140/215   train_loss = 3.930
Epoch  36 Batch  150/215   train_loss = 3.642
Epoch  36 Batch  160/215   train_loss = 3.592
Epoch  36 Batch  170/215   train_loss = 3.477
Epoch  36 Batch  180/215   train_loss = 3.701
Epoch  36 Batch  190/215   train_loss = 3.682
Epoch  36 Batch  200/215   train_loss = 3.501
Epoch  36 Batch  210/215   train_loss = 3.442
Epoch  37 Batch    5/215   train_loss = 3.941
Epoch  37 Batch   15/215   train_loss = 3.707
Epoch  37 Batch   25/215   train_loss = 3.521
Epoch  37 Batch   35/215   train_loss = 3.599
Epoch  37 Batch   45/215   train_loss = 3.585
Epoch  37 Batch   55/215   train_loss = 3.677
Epoch  37 Batch   65/215   train_loss = 3.627
Epoch  37 Batch   75/215   train_loss = 3.532
Epoch  37 Batch   85/215   train_loss = 3.758
Epoch  37 Batch   95/215   train_loss = 3.590
Epoch  37 Batch  105/215   train_loss = 3.566
Epoch  37 Batch  115/215   train_loss = 3.647
Epoch  37 Batch  125/215   train_loss = 3.630
Epoch  37 Batch  135/215   train_loss = 3.663
Epoch  37 Batch  145/215   train_loss = 3.503
Epoch  37 Batch  155/215   train_loss = 3.578
Epoch  37 Batch  165/215   train_loss = 3.565
Epoch  37 Batch  175/215   train_loss = 3.731
Epoch  37 Batch  185/215   train_loss = 3.584
Epoch  37 Batch  195/215   train_loss = 3.657
Epoch  37 Batch  205/215   train_loss = 3.655
Epoch  38 Batch    0/215   train_loss = 3.701
Epoch  38 Batch   10/215   train_loss = 3.967
Epoch  38 Batch   20/215   train_loss = 3.724
Epoch  38 Batch   30/215   train_loss = 3.496
Epoch  38 Batch   40/215   train_loss = 3.588
Epoch  38 Batch   50/215   train_loss = 3.600
Epoch  38 Batch   60/215   train_loss = 3.785
Epoch  38 Batch   70/215   train_loss = 3.629
Epoch  38 Batch   80/215   train_loss = 3.457
Epoch  38 Batch   90/215   train_loss = 3.693
Epoch  38 Batch  100/215   train_loss = 3.804
Epoch  38 Batch  110/215   train_loss = 3.582
Epoch  38 Batch  120/215   train_loss = 3.582
Epoch  38 Batch  130/215   train_loss = 3.962
Epoch  38 Batch  140/215   train_loss = 3.846
Epoch  38 Batch  150/215   train_loss = 3.593
Epoch  38 Batch  160/215   train_loss = 3.661
Epoch  38 Batch  170/215   train_loss = 3.510
Epoch  38 Batch  180/215   train_loss = 3.473
Epoch  38 Batch  190/215   train_loss = 3.674
Epoch  38 Batch  200/215   train_loss = 3.372
Epoch  38 Batch  210/215   train_loss = 3.251
Epoch  39 Batch    5/215   train_loss = 3.881
Epoch  39 Batch   15/215   train_loss = 3.543
Epoch  39 Batch   25/215   train_loss = 3.426
Epoch  39 Batch   35/215   train_loss = 3.690
Epoch  39 Batch   45/215   train_loss = 3.705
Epoch  39 Batch   55/215   train_loss = 3.593
Epoch  39 Batch   65/215   train_loss = 3.549
Epoch  39 Batch   75/215   train_loss = 3.616
Epoch  39 Batch   85/215   train_loss = 3.606
Epoch  39 Batch   95/215   train_loss = 3.512
Epoch  39 Batch  105/215   train_loss = 3.476
Epoch  39 Batch  115/215   train_loss = 3.736
Epoch  39 Batch  125/215   train_loss = 3.424
Epoch  39 Batch  135/215   train_loss = 3.612
Epoch  39 Batch  145/215   train_loss = 3.748
Epoch  39 Batch  155/215   train_loss = 3.699
Epoch  39 Batch  165/215   train_loss = 3.553
Epoch  39 Batch  175/215   train_loss = 3.649
Epoch  39 Batch  185/215   train_loss = 3.651
Epoch  39 Batch  195/215   train_loss = 3.535
Epoch  39 Batch  205/215   train_loss = 3.694
Epoch  40 Batch    0/215   train_loss = 3.570
Epoch  40 Batch   10/215   train_loss = 3.894
Epoch  40 Batch   20/215   train_loss = 3.602
Epoch  40 Batch   30/215   train_loss = 3.380
Epoch  40 Batch   40/215   train_loss = 3.497
Epoch  40 Batch   50/215   train_loss = 3.617
Epoch  40 Batch   60/215   train_loss = 3.703
Epoch  40 Batch   70/215   train_loss = 3.608
Epoch  40 Batch   80/215   train_loss = 3.587
Epoch  40 Batch   90/215   train_loss = 3.682
Epoch  40 Batch  100/215   train_loss = 3.771
Epoch  40 Batch  110/215   train_loss = 3.652
Epoch  40 Batch  120/215   train_loss = 3.638
Epoch  40 Batch  130/215   train_loss = 3.975
Epoch  40 Batch  140/215   train_loss = 3.806
Epoch  40 Batch  150/215   train_loss = 3.653
Epoch  40 Batch  160/215   train_loss = 3.548
Epoch  40 Batch  170/215   train_loss = 3.475
Epoch  40 Batch  180/215   train_loss = 3.644
Epoch  40 Batch  190/215   train_loss = 3.511
Epoch  40 Batch  200/215   train_loss = 3.345
Epoch  40 Batch  210/215   train_loss = 3.385
Epoch  41 Batch    5/215   train_loss = 3.672
Epoch  41 Batch   15/215   train_loss = 3.559
Epoch  41 Batch   25/215   train_loss = 3.525
Epoch  41 Batch   35/215   train_loss = 3.779
Epoch  41 Batch   45/215   train_loss = 3.510
Epoch  41 Batch   55/215   train_loss = 3.615
Epoch  41 Batch   65/215   train_loss = 3.674
Epoch  41 Batch   75/215   train_loss = 3.539
Epoch  41 Batch   85/215   train_loss = 3.603
Epoch  41 Batch   95/215   train_loss = 3.554
Epoch  41 Batch  105/215   train_loss = 3.588
Epoch  41 Batch  115/215   train_loss = 3.495
Epoch  41 Batch  125/215   train_loss = 3.464
Epoch  41 Batch  135/215   train_loss = 3.673
Epoch  41 Batch  145/215   train_loss = 3.461
Epoch  41 Batch  155/215   train_loss = 3.640
Epoch  41 Batch  165/215   train_loss = 3.457
Epoch  41 Batch  175/215   train_loss = 3.799
Epoch  41 Batch  185/215   train_loss = 3.588
Epoch  41 Batch  195/215   train_loss = 3.603
Epoch  41 Batch  205/215   train_loss = 3.598
Epoch  42 Batch    0/215   train_loss = 3.672
Epoch  42 Batch   10/215   train_loss = 3.675
Epoch  42 Batch   20/215   train_loss = 3.590
Epoch  42 Batch   30/215   train_loss = 3.484
Epoch  42 Batch   40/215   train_loss = 3.600
Epoch  42 Batch   50/215   train_loss = 3.422
Epoch  42 Batch   60/215   train_loss = 3.658
Epoch  42 Batch   70/215   train_loss = 3.549
Epoch  42 Batch   80/215   train_loss = 3.613
Epoch  42 Batch   90/215   train_loss = 3.731
Epoch  42 Batch  100/215   train_loss = 3.487
Epoch  42 Batch  110/215   train_loss = 3.471
Epoch  42 Batch  120/215   train_loss = 3.639
Epoch  42 Batch  130/215   train_loss = 3.796
Epoch  42 Batch  140/215   train_loss = 3.673
Epoch  42 Batch  150/215   train_loss = 3.536
Epoch  42 Batch  160/215   train_loss = 3.683
Epoch  42 Batch  170/215   train_loss = 3.574
Epoch  42 Batch  180/215   train_loss = 3.620
Epoch  42 Batch  190/215   train_loss = 3.633
Epoch  42 Batch  200/215   train_loss = 3.406
Epoch  42 Batch  210/215   train_loss = 3.225
Epoch  43 Batch    5/215   train_loss = 3.677
Epoch  43 Batch   15/215   train_loss = 3.597
Epoch  43 Batch   25/215   train_loss = 3.390
Epoch  43 Batch   35/215   train_loss = 3.642
Epoch  43 Batch   45/215   train_loss = 3.566
Epoch  43 Batch   55/215   train_loss = 3.646
Epoch  43 Batch   65/215   train_loss = 3.568
Epoch  43 Batch   75/215   train_loss = 3.534
Epoch  43 Batch   85/215   train_loss = 3.644
Epoch  43 Batch   95/215   train_loss = 3.583
Epoch  43 Batch  105/215   train_loss = 3.460
Epoch  43 Batch  115/215   train_loss = 3.526
Epoch  43 Batch  125/215   train_loss = 3.445
Epoch  43 Batch  135/215   train_loss = 3.601
Epoch  43 Batch  145/215   train_loss = 3.549
Epoch  43 Batch  155/215   train_loss = 3.605
Epoch  43 Batch  165/215   train_loss = 3.547
Epoch  43 Batch  175/215   train_loss = 3.828
Epoch  43 Batch  185/215   train_loss = 3.659
Epoch  43 Batch  195/215   train_loss = 3.431
Epoch  43 Batch  205/215   train_loss = 3.528
Epoch  44 Batch    0/215   train_loss = 3.590
Epoch  44 Batch   10/215   train_loss = 3.607
Epoch  44 Batch   20/215   train_loss = 3.713
Epoch  44 Batch   30/215   train_loss = 3.352
Epoch  44 Batch   40/215   train_loss = 3.457
Epoch  44 Batch   50/215   train_loss = 3.456
Epoch  44 Batch   60/215   train_loss = 3.619
Epoch  44 Batch   70/215   train_loss = 3.592
Epoch  44 Batch   80/215   train_loss = 3.543
Epoch  44 Batch   90/215   train_loss = 3.636
Epoch  44 Batch  100/215   train_loss = 3.582
Epoch  44 Batch  110/215   train_loss = 3.529
Epoch  44 Batch  120/215   train_loss = 3.610
Epoch  44 Batch  130/215   train_loss = 3.884
Epoch  44 Batch  140/215   train_loss = 3.747
Epoch  44 Batch  150/215   train_loss = 3.549
Epoch  44 Batch  160/215   train_loss = 3.677
Epoch  44 Batch  170/215   train_loss = 3.400
Epoch  44 Batch  180/215   train_loss = 3.727
Epoch  44 Batch  190/215   train_loss = 3.585
Epoch  44 Batch  200/215   train_loss = 3.409
Epoch  44 Batch  210/215   train_loss = 3.202
Epoch  45 Batch    5/215   train_loss = 3.708
Epoch  45 Batch   15/215   train_loss = 3.601
Epoch  45 Batch   25/215   train_loss = 3.520
Epoch  45 Batch   35/215   train_loss = 3.635
Epoch  45 Batch   45/215   train_loss = 3.692
Epoch  45 Batch   55/215   train_loss = 3.508
Epoch  45 Batch   65/215   train_loss = 3.590
Epoch  45 Batch   75/215   train_loss = 3.467
Epoch  45 Batch   85/215   train_loss = 3.497
Epoch  45 Batch   95/215   train_loss = 3.643
Epoch  45 Batch  105/215   train_loss = 3.605
Epoch  45 Batch  115/215   train_loss = 3.603
Epoch  45 Batch  125/215   train_loss = 3.318
Epoch  45 Batch  135/215   train_loss = 3.404
Epoch  45 Batch  145/215   train_loss = 3.470
Epoch  45 Batch  155/215   train_loss = 3.624
Epoch  45 Batch  165/215   train_loss = 3.457
Epoch  45 Batch  175/215   train_loss = 3.604
Epoch  45 Batch  185/215   train_loss = 3.571
Epoch  45 Batch  195/215   train_loss = 3.536
Epoch  45 Batch  205/215   train_loss = 3.582
Epoch  46 Batch    0/215   train_loss = 3.554
Epoch  46 Batch   10/215   train_loss = 3.741
Epoch  46 Batch   20/215   train_loss = 3.534
Epoch  46 Batch   30/215   train_loss = 3.332
Epoch  46 Batch   40/215   train_loss = 3.568
Epoch  46 Batch   50/215   train_loss = 3.614
Epoch  46 Batch   60/215   train_loss = 3.666
Epoch  46 Batch   70/215   train_loss = 3.552
Epoch  46 Batch   80/215   train_loss = 3.330
Epoch  46 Batch   90/215   train_loss = 3.538
Epoch  46 Batch  100/215   train_loss = 3.493
Epoch  46 Batch  110/215   train_loss = 3.490
Epoch  46 Batch  120/215   train_loss = 3.484
Epoch  46 Batch  130/215   train_loss = 3.861
Epoch  46 Batch  140/215   train_loss = 3.698
Epoch  46 Batch  150/215   train_loss = 3.565
Epoch  46 Batch  160/215   train_loss = 3.562
Epoch  46 Batch  170/215   train_loss = 3.464
Epoch  46 Batch  180/215   train_loss = 3.582
Epoch  46 Batch  190/215   train_loss = 3.643
Epoch  46 Batch  200/215   train_loss = 3.384
Epoch  46 Batch  210/215   train_loss = 3.225
Epoch  47 Batch    5/215   train_loss = 3.682
Epoch  47 Batch   15/215   train_loss = 3.532
Epoch  47 Batch   25/215   train_loss = 3.594
Epoch  47 Batch   35/215   train_loss = 3.549
Epoch  47 Batch   45/215   train_loss = 3.352
Epoch  47 Batch   55/215   train_loss = 3.685
Epoch  47 Batch   65/215   train_loss = 3.591
Epoch  47 Batch   75/215   train_loss = 3.558
Epoch  47 Batch   85/215   train_loss = 3.566
Epoch  47 Batch   95/215   train_loss = 3.473
Epoch  47 Batch  105/215   train_loss = 3.501
Epoch  47 Batch  115/215   train_loss = 3.496
Epoch  47 Batch  125/215   train_loss = 3.443
Epoch  47 Batch  135/215   train_loss = 3.546
Epoch  47 Batch  145/215   train_loss = 3.516
Epoch  47 Batch  155/215   train_loss = 3.601
Epoch  47 Batch  165/215   train_loss = 3.516
Epoch  47 Batch  175/215   train_loss = 3.710
Epoch  47 Batch  185/215   train_loss = 3.558
Epoch  47 Batch  195/215   train_loss = 3.312
Epoch  47 Batch  205/215   train_loss = 3.462
Epoch  48 Batch    0/215   train_loss = 3.567
Epoch  48 Batch   10/215   train_loss = 3.535
Epoch  48 Batch   20/215   train_loss = 3.599
Epoch  48 Batch   30/215   train_loss = 3.215
Epoch  48 Batch   40/215   train_loss = 3.600
Epoch  48 Batch   50/215   train_loss = 3.455
Epoch  48 Batch   60/215   train_loss = 3.604
Epoch  48 Batch   70/215   train_loss = 3.497
Epoch  48 Batch   80/215   train_loss = 3.363
Epoch  48 Batch   90/215   train_loss = 3.514
Epoch  48 Batch  100/215   train_loss = 3.555
Epoch  48 Batch  110/215   train_loss = 3.433
Epoch  48 Batch  120/215   train_loss = 3.472
Epoch  48 Batch  130/215   train_loss = 3.661
Epoch  48 Batch  140/215   train_loss = 3.720
Epoch  48 Batch  150/215   train_loss = 3.452
Epoch  48 Batch  160/215   train_loss = 3.687
Epoch  48 Batch  170/215   train_loss = 3.347
Epoch  48 Batch  180/215   train_loss = 3.495
Epoch  48 Batch  190/215   train_loss = 3.555
Epoch  48 Batch  200/215   train_loss = 3.473
Epoch  48 Batch  210/215   train_loss = 3.252
Epoch  49 Batch    5/215   train_loss = 3.606
Epoch  49 Batch   15/215   train_loss = 3.575
Epoch  49 Batch   25/215   train_loss = 3.452
Epoch  49 Batch   35/215   train_loss = 3.655
Epoch  49 Batch   45/215   train_loss = 3.564
Epoch  49 Batch   55/215   train_loss = 3.453
Epoch  49 Batch   65/215   train_loss = 3.573
Epoch  49 Batch   75/215   train_loss = 3.376
Epoch  49 Batch   85/215   train_loss = 3.474
Epoch  49 Batch   95/215   train_loss = 3.447
Epoch  49 Batch  105/215   train_loss = 3.448
Epoch  49 Batch  115/215   train_loss = 3.610
Epoch  49 Batch  125/215   train_loss = 3.407
Epoch  49 Batch  135/215   train_loss = 3.398
Epoch  49 Batch  145/215   train_loss = 3.352
Epoch  49 Batch  155/215   train_loss = 3.585
Epoch  49 Batch  165/215   train_loss = 3.508
Epoch  49 Batch  175/215   train_loss = 3.753
Epoch  49 Batch  185/215   train_loss = 3.605
Epoch  49 Batch  195/215   train_loss = 3.364
Epoch  49 Batch  205/215   train_loss = 3.351
Epoch  50 Batch    0/215   train_loss = 3.575
Epoch  50 Batch   10/215   train_loss = 3.758
Epoch  50 Batch   20/215   train_loss = 3.587
Epoch  50 Batch   30/215   train_loss = 3.420
Epoch  50 Batch   40/215   train_loss = 3.435
Epoch  50 Batch   50/215   train_loss = 3.420
Epoch  50 Batch   60/215   train_loss = 3.659
Epoch  50 Batch   70/215   train_loss = 3.501
Epoch  50 Batch   80/215   train_loss = 3.442
Epoch  50 Batch   90/215   train_loss = 3.473
Epoch  50 Batch  100/215   train_loss = 3.554
Epoch  50 Batch  110/215   train_loss = 3.545
Epoch  50 Batch  120/215   train_loss = 3.542
Epoch  50 Batch  130/215   train_loss = 3.835
Epoch  50 Batch  140/215   train_loss = 3.698
Epoch  50 Batch  150/215   train_loss = 3.429
Epoch  50 Batch  160/215   train_loss = 3.472
Epoch  50 Batch  170/215   train_loss = 3.501
Epoch  50 Batch  180/215   train_loss = 3.618
Epoch  50 Batch  190/215   train_loss = 3.493
Epoch  50 Batch  200/215   train_loss = 3.305
Epoch  50 Batch  210/215   train_loss = 3.205
Epoch  51 Batch    5/215   train_loss = 3.598
Epoch  51 Batch   15/215   train_loss = 3.497
Epoch  51 Batch   25/215   train_loss = 3.331
Epoch  51 Batch   35/215   train_loss = 3.567
Epoch  51 Batch   45/215   train_loss = 3.548
Epoch  51 Batch   55/215   train_loss = 3.422
Epoch  51 Batch   65/215   train_loss = 3.610
Epoch  51 Batch   75/215   train_loss = 3.441
Epoch  51 Batch   85/215   train_loss = 3.487
Epoch  51 Batch   95/215   train_loss = 3.455
Epoch  51 Batch  105/215   train_loss = 3.517
Epoch  51 Batch  115/215   train_loss = 3.509
Epoch  51 Batch  125/215   train_loss = 3.348
Epoch  51 Batch  135/215   train_loss = 3.472
Epoch  51 Batch  145/215   train_loss = 3.373
Epoch  51 Batch  155/215   train_loss = 3.668
Epoch  51 Batch  165/215   train_loss = 3.522
Epoch  51 Batch  175/215   train_loss = 3.582
Epoch  51 Batch  185/215   train_loss = 3.359
Epoch  51 Batch  195/215   train_loss = 3.350
Epoch  51 Batch  205/215   train_loss = 3.424
Epoch  52 Batch    0/215   train_loss = 3.661
Epoch  52 Batch   10/215   train_loss = 3.619
Epoch  52 Batch   20/215   train_loss = 3.548
Epoch  52 Batch   30/215   train_loss = 3.545
Epoch  52 Batch   40/215   train_loss = 3.535
Epoch  52 Batch   50/215   train_loss = 3.394
Epoch  52 Batch   60/215   train_loss = 3.590
Epoch  52 Batch   70/215   train_loss = 3.480
Epoch  52 Batch   80/215   train_loss = 3.421
Epoch  52 Batch   90/215   train_loss = 3.562
Epoch  52 Batch  100/215   train_loss = 3.568
Epoch  52 Batch  110/215   train_loss = 3.449
Epoch  52 Batch  120/215   train_loss = 3.533
Epoch  52 Batch  130/215   train_loss = 3.727
Epoch  52 Batch  140/215   train_loss = 3.484
Epoch  52 Batch  150/215   train_loss = 3.486
Epoch  52 Batch  160/215   train_loss = 3.518
Epoch  52 Batch  170/215   train_loss = 3.468
Epoch  52 Batch  180/215   train_loss = 3.587
Epoch  52 Batch  190/215   train_loss = 3.526
Epoch  52 Batch  200/215   train_loss = 3.382
Epoch  52 Batch  210/215   train_loss = 3.182
Epoch  53 Batch    5/215   train_loss = 3.648
Epoch  53 Batch   15/215   train_loss = 3.471
Epoch  53 Batch   25/215   train_loss = 3.434
Epoch  53 Batch   35/215   train_loss = 3.415
Epoch  53 Batch   45/215   train_loss = 3.489
Epoch  53 Batch   55/215   train_loss = 3.525
Epoch  53 Batch   65/215   train_loss = 3.646
Epoch  53 Batch   75/215   train_loss = 3.437
Epoch  53 Batch   85/215   train_loss = 3.386
Epoch  53 Batch   95/215   train_loss = 3.479
Epoch  53 Batch  105/215   train_loss = 3.460
Epoch  53 Batch  115/215   train_loss = 3.517
Epoch  53 Batch  125/215   train_loss = 3.291
Epoch  53 Batch  135/215   train_loss = 3.434
Epoch  53 Batch  145/215   train_loss = 3.448
Epoch  53 Batch  155/215   train_loss = 3.658
Epoch  53 Batch  165/215   train_loss = 3.342
Epoch  53 Batch  175/215   train_loss = 3.605
Epoch  53 Batch  185/215   train_loss = 3.512
Epoch  53 Batch  195/215   train_loss = 3.511
Epoch  53 Batch  205/215   train_loss = 3.459
Epoch  54 Batch    0/215   train_loss = 3.569
Epoch  54 Batch   10/215   train_loss = 3.675
Epoch  54 Batch   20/215   train_loss = 3.424
Epoch  54 Batch   30/215   train_loss = 3.248
Epoch  54 Batch   40/215   train_loss = 3.634
Epoch  54 Batch   50/215   train_loss = 3.512
Epoch  54 Batch   60/215   train_loss = 3.446
Epoch  54 Batch   70/215   train_loss = 3.472
Epoch  54 Batch   80/215   train_loss = 3.501
Epoch  54 Batch   90/215   train_loss = 3.437
Epoch  54 Batch  100/215   train_loss = 3.605
Epoch  54 Batch  110/215   train_loss = 3.421
Epoch  54 Batch  120/215   train_loss = 3.440
Epoch  54 Batch  130/215   train_loss = 3.848
Epoch  54 Batch  140/215   train_loss = 3.502
Epoch  54 Batch  150/215   train_loss = 3.366
Epoch  54 Batch  160/215   train_loss = 3.483
Epoch  54 Batch  170/215   train_loss = 3.457
Epoch  54 Batch  180/215   train_loss = 3.590
Epoch  54 Batch  190/215   train_loss = 3.508
Epoch  54 Batch  200/215   train_loss = 3.500
Epoch  54 Batch  210/215   train_loss = 3.194
Epoch  55 Batch    5/215   train_loss = 3.751
Epoch  55 Batch   15/215   train_loss = 3.401
Epoch  55 Batch   25/215   train_loss = 3.478
Epoch  55 Batch   35/215   train_loss = 3.373
Epoch  55 Batch   45/215   train_loss = 3.469
Epoch  55 Batch   55/215   train_loss = 3.577
Epoch  55 Batch   65/215   train_loss = 3.564
Epoch  55 Batch   75/215   train_loss = 3.523
Epoch  55 Batch   85/215   train_loss = 3.630
Epoch  55 Batch   95/215   train_loss = 3.435
Epoch  55 Batch  105/215   train_loss = 3.395
Epoch  55 Batch  115/215   train_loss = 3.394
Epoch  55 Batch  125/215   train_loss = 3.270
Epoch  55 Batch  135/215   train_loss = 3.451
Epoch  55 Batch  145/215   train_loss = 3.373
Epoch  55 Batch  155/215   train_loss = 3.352
Epoch  55 Batch  165/215   train_loss = 3.512
Epoch  55 Batch  175/215   train_loss = 3.624
Epoch  55 Batch  185/215   train_loss = 3.587
Epoch  55 Batch  195/215   train_loss = 3.544
Epoch  55 Batch  205/215   train_loss = 3.438
Epoch  56 Batch    0/215   train_loss = 3.481
Epoch  56 Batch   10/215   train_loss = 3.638
Epoch  56 Batch   20/215   train_loss = 3.510
Epoch  56 Batch   30/215   train_loss = 3.412
Epoch  56 Batch   40/215   train_loss = 3.366
Epoch  56 Batch   50/215   train_loss = 3.583
Epoch  56 Batch   60/215   train_loss = 3.628
Epoch  56 Batch   70/215   train_loss = 3.408
Epoch  56 Batch   80/215   train_loss = 3.390
Epoch  56 Batch   90/215   train_loss = 3.540
Epoch  56 Batch  100/215   train_loss = 3.459
Epoch  56 Batch  110/215   train_loss = 3.406
Epoch  56 Batch  120/215   train_loss = 3.380
Epoch  56 Batch  130/215   train_loss = 3.748
Epoch  56 Batch  140/215   train_loss = 3.467
Epoch  56 Batch  150/215   train_loss = 3.464
Epoch  56 Batch  160/215   train_loss = 3.427
Epoch  56 Batch  170/215   train_loss = 3.326
Epoch  56 Batch  180/215   train_loss = 3.503
Epoch  56 Batch  190/215   train_loss = 3.507
Epoch  56 Batch  200/215   train_loss = 3.257
Epoch  56 Batch  210/215   train_loss = 3.191
Epoch  57 Batch    5/215   train_loss = 3.481
Epoch  57 Batch   15/215   train_loss = 3.527
Epoch  57 Batch   25/215   train_loss = 3.301
Epoch  57 Batch   35/215   train_loss = 3.512
Epoch  57 Batch   45/215   train_loss = 3.521
Epoch  57 Batch   55/215   train_loss = 3.477
Epoch  57 Batch   65/215   train_loss = 3.656
Epoch  57 Batch   75/215   train_loss = 3.443
Epoch  57 Batch   85/215   train_loss = 3.304
Epoch  57 Batch   95/215   train_loss = 3.366
Epoch  57 Batch  105/215   train_loss = 3.221
Epoch  57 Batch  115/215   train_loss = 3.349
Epoch  57 Batch  125/215   train_loss = 3.133
Epoch  57 Batch  135/215   train_loss = 3.406
Epoch  57 Batch  145/215   train_loss = 3.354
Epoch  57 Batch  155/215   train_loss = 3.490
Epoch  57 Batch  165/215   train_loss = 3.430
Epoch  57 Batch  175/215   train_loss = 3.549
Epoch  57 Batch  185/215   train_loss = 3.423
Epoch  57 Batch  195/215   train_loss = 3.503
Epoch  57 Batch  205/215   train_loss = 3.474
Epoch  58 Batch    0/215   train_loss = 3.407
Epoch  58 Batch   10/215   train_loss = 3.559
Epoch  58 Batch   20/215   train_loss = 3.480
Epoch  58 Batch   30/215   train_loss = 3.326
Epoch  58 Batch   40/215   train_loss = 3.493
Epoch  58 Batch   50/215   train_loss = 3.441
Epoch  58 Batch   60/215   train_loss = 3.594
Epoch  58 Batch   70/215   train_loss = 3.482
Epoch  58 Batch   80/215   train_loss = 3.502
Epoch  58 Batch   90/215   train_loss = 3.555
Epoch  58 Batch  100/215   train_loss = 3.299
Epoch  58 Batch  110/215   train_loss = 3.357
Epoch  58 Batch  120/215   train_loss = 3.356
Epoch  58 Batch  130/215   train_loss = 3.739
Epoch  58 Batch  140/215   train_loss = 3.557
Epoch  58 Batch  150/215   train_loss = 3.557
Epoch  58 Batch  160/215   train_loss = 3.457
Epoch  58 Batch  170/215   train_loss = 3.615
Epoch  58 Batch  180/215   train_loss = 3.537
Epoch  58 Batch  190/215   train_loss = 3.380
Epoch  58 Batch  200/215   train_loss = 3.345
Epoch  58 Batch  210/215   train_loss = 3.177
Epoch  59 Batch    5/215   train_loss = 3.577
Epoch  59 Batch   15/215   train_loss = 3.362
Epoch  59 Batch   25/215   train_loss = 3.423
Epoch  59 Batch   35/215   train_loss = 3.377
Epoch  59 Batch   45/215   train_loss = 3.445
Epoch  59 Batch   55/215   train_loss = 3.359
Epoch  59 Batch   65/215   train_loss = 3.601
Epoch  59 Batch   75/215   train_loss = 3.522
Epoch  59 Batch   85/215   train_loss = 3.314
Epoch  59 Batch   95/215   train_loss = 3.317
Epoch  59 Batch  105/215   train_loss = 3.411
Epoch  59 Batch  115/215   train_loss = 3.320
Epoch  59 Batch  125/215   train_loss = 3.133
Epoch  59 Batch  135/215   train_loss = 3.411
Epoch  59 Batch  145/215   train_loss = 3.432
Epoch  59 Batch  155/215   train_loss = 3.406
Epoch  59 Batch  165/215   train_loss = 3.367
Epoch  59 Batch  175/215   train_loss = 3.637
Epoch  59 Batch  185/215   train_loss = 3.466
Epoch  59 Batch  195/215   train_loss = 3.564
Epoch  59 Batch  205/215   train_loss = 3.381
Epoch  60 Batch    0/215   train_loss = 3.395
Epoch  60 Batch   10/215   train_loss = 3.469
Epoch  60 Batch   20/215   train_loss = 3.450
Epoch  60 Batch   30/215   train_loss = 3.405
Epoch  60 Batch   40/215   train_loss = 3.515
Epoch  60 Batch   50/215   train_loss = 3.333
Epoch  60 Batch   60/215   train_loss = 3.391
Epoch  60 Batch   70/215   train_loss = 3.361
Epoch  60 Batch   80/215   train_loss = 3.508
Epoch  60 Batch   90/215   train_loss = 3.530
Epoch  60 Batch  100/215   train_loss = 3.548
Epoch  60 Batch  110/215   train_loss = 3.497
Epoch  60 Batch  120/215   train_loss = 3.547
Epoch  60 Batch  130/215   train_loss = 3.857
Epoch  60 Batch  140/215   train_loss = 3.476
Epoch  60 Batch  150/215   train_loss = 3.390
Epoch  60 Batch  160/215   train_loss = 3.658
Epoch  60 Batch  170/215   train_loss = 3.262
Epoch  60 Batch  180/215   train_loss = 3.457
Epoch  60 Batch  190/215   train_loss = 3.476
Epoch  60 Batch  200/215   train_loss = 3.265
Epoch  60 Batch  210/215   train_loss = 3.205
Epoch  61 Batch    5/215   train_loss = 3.602
Epoch  61 Batch   15/215   train_loss = 3.446
Epoch  61 Batch   25/215   train_loss = 3.361
Epoch  61 Batch   35/215   train_loss = 3.480
Epoch  61 Batch   45/215   train_loss = 3.453
Epoch  61 Batch   55/215   train_loss = 3.369
Epoch  61 Batch   65/215   train_loss = 3.465
Epoch  61 Batch   75/215   train_loss = 3.533
Epoch  61 Batch   85/215   train_loss = 3.319
Epoch  61 Batch   95/215   train_loss = 3.506
Epoch  61 Batch  105/215   train_loss = 3.319
Epoch  61 Batch  115/215   train_loss = 3.348
Epoch  61 Batch  125/215   train_loss = 3.329
Epoch  61 Batch  135/215   train_loss = 3.327
Epoch  61 Batch  145/215   train_loss = 3.281
Epoch  61 Batch  155/215   train_loss = 3.379
Epoch  61 Batch  165/215   train_loss = 3.323
Epoch  61 Batch  175/215   train_loss = 3.473
Epoch  61 Batch  185/215   train_loss = 3.566
Epoch  61 Batch  195/215   train_loss = 3.313
Epoch  61 Batch  205/215   train_loss = 3.370
Epoch  62 Batch    0/215   train_loss = 3.300
Epoch  62 Batch   10/215   train_loss = 3.519
Epoch  62 Batch   20/215   train_loss = 3.477
Epoch  62 Batch   30/215   train_loss = 3.363
Epoch  62 Batch   40/215   train_loss = 3.300
Epoch  62 Batch   50/215   train_loss = 3.293
Epoch  62 Batch   60/215   train_loss = 3.486
Epoch  62 Batch   70/215   train_loss = 3.256
Epoch  62 Batch   80/215   train_loss = 3.329
Epoch  62 Batch   90/215   train_loss = 3.403
Epoch  62 Batch  100/215   train_loss = 3.420
Epoch  62 Batch  110/215   train_loss = 3.354
Epoch  62 Batch  120/215   train_loss = 3.389
Epoch  62 Batch  130/215   train_loss = 3.684
Epoch  62 Batch  140/215   train_loss = 3.535
Epoch  62 Batch  150/215   train_loss = 3.475
Epoch  62 Batch  160/215   train_loss = 3.349
Epoch  62 Batch  170/215   train_loss = 3.263
Epoch  62 Batch  180/215   train_loss = 3.423
Epoch  62 Batch  190/215   train_loss = 3.412
Epoch  62 Batch  200/215   train_loss = 3.275
Epoch  62 Batch  210/215   train_loss = 3.173
Epoch  63 Batch    5/215   train_loss = 3.590
Epoch  63 Batch   15/215   train_loss = 3.310
Epoch  63 Batch   25/215   train_loss = 3.415
Epoch  63 Batch   35/215   train_loss = 3.446
Epoch  63 Batch   45/215   train_loss = 3.293
Epoch  63 Batch   55/215   train_loss = 3.410
Epoch  63 Batch   65/215   train_loss = 3.428
Epoch  63 Batch   75/215   train_loss = 3.475
Epoch  63 Batch   85/215   train_loss = 3.365
Epoch  63 Batch   95/215   train_loss = 3.417
Epoch  63 Batch  105/215   train_loss = 3.294
Epoch  63 Batch  115/215   train_loss = 3.152
Epoch  63 Batch  125/215   train_loss = 3.236
Epoch  63 Batch  135/215   train_loss = 3.191
Epoch  63 Batch  145/215   train_loss = 3.276
Epoch  63 Batch  155/215   train_loss = 3.572
Epoch  63 Batch  165/215   train_loss = 3.373
Epoch  63 Batch  175/215   train_loss = 3.532
Epoch  63 Batch  185/215   train_loss = 3.407
Epoch  63 Batch  195/215   train_loss = 3.351
Epoch  63 Batch  205/215   train_loss = 3.361
Epoch  64 Batch    0/215   train_loss = 3.329
Epoch  64 Batch   10/215   train_loss = 3.469
Epoch  64 Batch   20/215   train_loss = 3.472
Epoch  64 Batch   30/215   train_loss = 3.226
Epoch  64 Batch   40/215   train_loss = 3.380
Epoch  64 Batch   50/215   train_loss = 3.315
Epoch  64 Batch   60/215   train_loss = 3.609
Epoch  64 Batch   70/215   train_loss = 3.444
Epoch  64 Batch   80/215   train_loss = 3.440
Epoch  64 Batch   90/215   train_loss = 3.597
Epoch  64 Batch  100/215   train_loss = 3.313
Epoch  64 Batch  110/215   train_loss = 3.377
Epoch  64 Batch  120/215   train_loss = 3.328
Epoch  64 Batch  130/215   train_loss = 3.548
Epoch  64 Batch  140/215   train_loss = 3.479
Epoch  64 Batch  150/215   train_loss = 3.503
Epoch  64 Batch  160/215   train_loss = 3.571
Epoch  64 Batch  170/215   train_loss = 3.185
Epoch  64 Batch  180/215   train_loss = 3.408
Epoch  64 Batch  190/215   train_loss = 3.344
Epoch  64 Batch  200/215   train_loss = 3.244
Epoch  64 Batch  210/215   train_loss = 3.255
Epoch  65 Batch    5/215   train_loss = 3.513
Epoch  65 Batch   15/215   train_loss = 3.386
Epoch  65 Batch   25/215   train_loss = 3.379
Epoch  65 Batch   35/215   train_loss = 3.409
Epoch  65 Batch   45/215   train_loss = 3.296
Epoch  65 Batch   55/215   train_loss = 3.269
Epoch  65 Batch   65/215   train_loss = 3.429
Epoch  65 Batch   75/215   train_loss = 3.409
Epoch  65 Batch   85/215   train_loss = 3.262
Epoch  65 Batch   95/215   train_loss = 3.428
Epoch  65 Batch  105/215   train_loss = 3.219
Epoch  65 Batch  115/215   train_loss = 3.334
Epoch  65 Batch  125/215   train_loss = 3.215
Epoch  65 Batch  135/215   train_loss = 3.346
Epoch  65 Batch  145/215   train_loss = 3.392
Epoch  65 Batch  155/215   train_loss = 3.529
Epoch  65 Batch  165/215   train_loss = 3.302
Epoch  65 Batch  175/215   train_loss = 3.510
Epoch  65 Batch  185/215   train_loss = 3.415
Epoch  65 Batch  195/215   train_loss = 3.384
Epoch  65 Batch  205/215   train_loss = 3.316
Epoch  66 Batch    0/215   train_loss = 3.346
Epoch  66 Batch   10/215   train_loss = 3.459
Epoch  66 Batch   20/215   train_loss = 3.324
Epoch  66 Batch   30/215   train_loss = 3.272
Epoch  66 Batch   40/215   train_loss = 3.333
Epoch  66 Batch   50/215   train_loss = 3.388
Epoch  66 Batch   60/215   train_loss = 3.443
Epoch  66 Batch   70/215   train_loss = 3.289
Epoch  66 Batch   80/215   train_loss = 3.304
Epoch  66 Batch   90/215   train_loss = 3.510
Epoch  66 Batch  100/215   train_loss = 3.465
Epoch  66 Batch  110/215   train_loss = 3.330
Epoch  66 Batch  120/215   train_loss = 3.359
Epoch  66 Batch  130/215   train_loss = 3.659
Epoch  66 Batch  140/215   train_loss = 3.522
Epoch  66 Batch  150/215   train_loss = 3.573
Epoch  66 Batch  160/215   train_loss = 3.444
Epoch  66 Batch  170/215   train_loss = 3.191
Epoch  66 Batch  180/215   train_loss = 3.470
Epoch  66 Batch  190/215   train_loss = 3.542
Epoch  66 Batch  200/215   train_loss = 3.298
Epoch  66 Batch  210/215   train_loss = 3.287
Epoch  67 Batch    5/215   train_loss = 3.394
Epoch  67 Batch   15/215   train_loss = 3.314
Epoch  67 Batch   25/215   train_loss = 3.438
Epoch  67 Batch   35/215   train_loss = 3.458
Epoch  67 Batch   45/215   train_loss = 3.296
Epoch  67 Batch   55/215   train_loss = 3.393
Epoch  67 Batch   65/215   train_loss = 3.414
Epoch  67 Batch   75/215   train_loss = 3.450
Epoch  67 Batch   85/215   train_loss = 3.359
Epoch  67 Batch   95/215   train_loss = 3.265
Epoch  67 Batch  105/215   train_loss = 3.420
Epoch  67 Batch  115/215   train_loss = 3.253
Epoch  67 Batch  125/215   train_loss = 3.153
Epoch  67 Batch  135/215   train_loss = 3.170
Epoch  67 Batch  145/215   train_loss = 3.301
Epoch  67 Batch  155/215   train_loss = 3.550
Epoch  67 Batch  165/215   train_loss = 3.395
Epoch  67 Batch  175/215   train_loss = 3.453
Epoch  67 Batch  185/215   train_loss = 3.454
Epoch  67 Batch  195/215   train_loss = 3.428
Epoch  67 Batch  205/215   train_loss = 3.404
Epoch  68 Batch    0/215   train_loss = 3.382
Epoch  68 Batch   10/215   train_loss = 3.572
Epoch  68 Batch   20/215   train_loss = 3.427
Epoch  68 Batch   30/215   train_loss = 3.254
Epoch  68 Batch   40/215   train_loss = 3.252
Epoch  68 Batch   50/215   train_loss = 3.359
Epoch  68 Batch   60/215   train_loss = 3.553
Epoch  68 Batch   70/215   train_loss = 3.302
Epoch  68 Batch   80/215   train_loss = 3.350
Epoch  68 Batch   90/215   train_loss = 3.422
Epoch  68 Batch  100/215   train_loss = 3.424
Epoch  68 Batch  110/215   train_loss = 3.262
Epoch  68 Batch  120/215   train_loss = 3.440
Epoch  68 Batch  130/215   train_loss = 3.438
Epoch  68 Batch  140/215   train_loss = 3.405
Epoch  68 Batch  150/215   train_loss = 3.304
Epoch  68 Batch  160/215   train_loss = 3.436
Epoch  68 Batch  170/215   train_loss = 3.441
Epoch  68 Batch  180/215   train_loss = 3.328
Epoch  68 Batch  190/215   train_loss = 3.329
Epoch  68 Batch  200/215   train_loss = 3.312
Epoch  68 Batch  210/215   train_loss = 3.127
Epoch  69 Batch    5/215   train_loss = 3.466
Epoch  69 Batch   15/215   train_loss = 3.441
Epoch  69 Batch   25/215   train_loss = 3.299
Epoch  69 Batch   35/215   train_loss = 3.478
Epoch  69 Batch   45/215   train_loss = 3.291
Epoch  69 Batch   55/215   train_loss = 3.356
Epoch  69 Batch   65/215   train_loss = 3.361
Epoch  69 Batch   75/215   train_loss = 3.518
Epoch  69 Batch   85/215   train_loss = 3.327
Epoch  69 Batch   95/215   train_loss = 3.303
Epoch  69 Batch  105/215   train_loss = 3.204
Epoch  69 Batch  115/215   train_loss = 3.289
Epoch  69 Batch  125/215   train_loss = 3.313
Epoch  69 Batch  135/215   train_loss = 3.286
Epoch  69 Batch  145/215   train_loss = 3.276
Epoch  69 Batch  155/215   train_loss = 3.432
Epoch  69 Batch  165/215   train_loss = 3.408
Epoch  69 Batch  175/215   train_loss = 3.543
Epoch  69 Batch  185/215   train_loss = 3.473
Epoch  69 Batch  195/215   train_loss = 3.405
Epoch  69 Batch  205/215   train_loss = 3.220
Epoch  70 Batch    0/215   train_loss = 3.345
Epoch  70 Batch   10/215   train_loss = 3.497
Epoch  70 Batch   20/215   train_loss = 3.273
Epoch  70 Batch   30/215   train_loss = 3.325
Epoch  70 Batch   40/215   train_loss = 3.284
Epoch  70 Batch   50/215   train_loss = 3.271
Epoch  70 Batch   60/215   train_loss = 3.424
Epoch  70 Batch   70/215   train_loss = 3.468
Epoch  70 Batch   80/215   train_loss = 3.393
Epoch  70 Batch   90/215   train_loss = 3.449
Epoch  70 Batch  100/215   train_loss = 3.476
Epoch  70 Batch  110/215   train_loss = 3.339
Epoch  70 Batch  120/215   train_loss = 3.392
Epoch  70 Batch  130/215   train_loss = 3.506
Epoch  70 Batch  140/215   train_loss = 3.454
Epoch  70 Batch  150/215   train_loss = 3.381
Epoch  70 Batch  160/215   train_loss = 3.412
Epoch  70 Batch  170/215   train_loss = 3.243
Epoch  70 Batch  180/215   train_loss = 3.374
Epoch  70 Batch  190/215   train_loss = 3.621
Epoch  70 Batch  200/215   train_loss = 3.206
Epoch  70 Batch  210/215   train_loss = 3.157
Epoch  71 Batch    5/215   train_loss = 3.397
Epoch  71 Batch   15/215   train_loss = 3.343
Epoch  71 Batch   25/215   train_loss = 3.254
Epoch  71 Batch   35/215   train_loss = 3.300
Epoch  71 Batch   45/215   train_loss = 3.333
Epoch  71 Batch   55/215   train_loss = 3.220
Epoch  71 Batch   65/215   train_loss = 3.459
Epoch  71 Batch   75/215   train_loss = 3.370
Epoch  71 Batch   85/215   train_loss = 3.193
Epoch  71 Batch   95/215   train_loss = 3.258
Epoch  71 Batch  105/215   train_loss = 3.206
Epoch  71 Batch  115/215   train_loss = 3.233
Epoch  71 Batch  125/215   train_loss = 3.144
Epoch  71 Batch  135/215   train_loss = 3.330
Epoch  71 Batch  145/215   train_loss = 3.293
Epoch  71 Batch  155/215   train_loss = 3.464
Epoch  71 Batch  165/215   train_loss = 3.270
Epoch  71 Batch  175/215   train_loss = 3.519
Epoch  71 Batch  185/215   train_loss = 3.343
Epoch  71 Batch  195/215   train_loss = 3.295
Epoch  71 Batch  205/215   train_loss = 3.259
Epoch  72 Batch    0/215   train_loss = 3.177
Epoch  72 Batch   10/215   train_loss = 3.520
Epoch  72 Batch   20/215   train_loss = 3.321
Epoch  72 Batch   30/215   train_loss = 3.487
Epoch  72 Batch   40/215   train_loss = 3.140
Epoch  72 Batch   50/215   train_loss = 3.265
Epoch  72 Batch   60/215   train_loss = 3.453
Epoch  72 Batch   70/215   train_loss = 3.318
Epoch  72 Batch   80/215   train_loss = 3.337
Epoch  72 Batch   90/215   train_loss = 3.341
Epoch  72 Batch  100/215   train_loss = 3.466
Epoch  72 Batch  110/215   train_loss = 3.494
Epoch  72 Batch  120/215   train_loss = 3.393
Epoch  72 Batch  130/215   train_loss = 3.631
Epoch  72 Batch  140/215   train_loss = 3.470
Epoch  72 Batch  150/215   train_loss = 3.375
Epoch  72 Batch  160/215   train_loss = 3.450
Epoch  72 Batch  170/215   train_loss = 3.287
Epoch  72 Batch  180/215   train_loss = 3.273
Epoch  72 Batch  190/215   train_loss = 3.356
Epoch  72 Batch  200/215   train_loss = 3.096
Epoch  72 Batch  210/215   train_loss = 3.075
Epoch  73 Batch    5/215   train_loss = 3.472
Epoch  73 Batch   15/215   train_loss = 3.315
Epoch  73 Batch   25/215   train_loss = 3.322
Epoch  73 Batch   35/215   train_loss = 3.351
Epoch  73 Batch   45/215   train_loss = 3.376
Epoch  73 Batch   55/215   train_loss = 3.234
Epoch  73 Batch   65/215   train_loss = 3.575
Epoch  73 Batch   75/215   train_loss = 3.382
Epoch  73 Batch   85/215   train_loss = 3.222
Epoch  73 Batch   95/215   train_loss = 3.256
Epoch  73 Batch  105/215   train_loss = 3.113
Epoch  73 Batch  115/215   train_loss = 3.363
Epoch  73 Batch  125/215   train_loss = 3.253
Epoch  73 Batch  135/215   train_loss = 3.193
Epoch  73 Batch  145/215   train_loss = 3.326
Epoch  73 Batch  155/215   train_loss = 3.581
Epoch  73 Batch  165/215   train_loss = 3.204
Epoch  73 Batch  175/215   train_loss = 3.444
Epoch  73 Batch  185/215   train_loss = 3.302
Epoch  73 Batch  195/215   train_loss = 3.455
Epoch  73 Batch  205/215   train_loss = 3.304
Epoch  74 Batch    0/215   train_loss = 3.325
Epoch  74 Batch   10/215   train_loss = 3.463
Epoch  74 Batch   20/215   train_loss = 3.493
Epoch  74 Batch   30/215   train_loss = 3.429
Epoch  74 Batch   40/215   train_loss = 3.360
Epoch  74 Batch   50/215   train_loss = 3.302
Epoch  74 Batch   60/215   train_loss = 3.462
Epoch  74 Batch   70/215   train_loss = 3.446
Epoch  74 Batch   80/215   train_loss = 3.259
Epoch  74 Batch   90/215   train_loss = 3.433
Epoch  74 Batch  100/215   train_loss = 3.254
Epoch  74 Batch  110/215   train_loss = 3.273
Epoch  74 Batch  120/215   train_loss = 3.406
Epoch  74 Batch  130/215   train_loss = 3.660
Epoch  74 Batch  140/215   train_loss = 3.591
Epoch  74 Batch  150/215   train_loss = 3.257
Epoch  74 Batch  160/215   train_loss = 3.284
Epoch  74 Batch  170/215   train_loss = 3.441
Epoch  74 Batch  180/215   train_loss = 3.298
Epoch  74 Batch  190/215   train_loss = 3.294
Epoch  74 Batch  200/215   train_loss = 3.455
Epoch  74 Batch  210/215   train_loss = 3.186
Epoch  75 Batch    5/215   train_loss = 3.263
Epoch  75 Batch   15/215   train_loss = 3.179
Epoch  75 Batch   25/215   train_loss = 3.273
Epoch  75 Batch   35/215   train_loss = 3.295
Epoch  75 Batch   45/215   train_loss = 3.263
Epoch  75 Batch   55/215   train_loss = 3.278
Epoch  75 Batch   65/215   train_loss = 3.306
Epoch  75 Batch   75/215   train_loss = 3.168
Epoch  75 Batch   85/215   train_loss = 3.194
Epoch  75 Batch   95/215   train_loss = 3.296
Epoch  75 Batch  105/215   train_loss = 3.184
Epoch  75 Batch  115/215   train_loss = 3.191
Epoch  75 Batch  125/215   train_loss = 3.145
Epoch  75 Batch  135/215   train_loss = 3.135
Epoch  75 Batch  145/215   train_loss = 3.169
Epoch  75 Batch  155/215   train_loss = 3.421
Epoch  75 Batch  165/215   train_loss = 3.300
Epoch  75 Batch  175/215   train_loss = 3.369
Epoch  75 Batch  185/215   train_loss = 3.393
Epoch  75 Batch  195/215   train_loss = 3.492
Epoch  75 Batch  205/215   train_loss = 3.256
Epoch  76 Batch    0/215   train_loss = 3.426
Epoch  76 Batch   10/215   train_loss = 3.283
Epoch  76 Batch   20/215   train_loss = 3.282
Epoch  76 Batch   30/215   train_loss = 3.250
Epoch  76 Batch   40/215   train_loss = 3.300
Epoch  76 Batch   50/215   train_loss = 3.152
Epoch  76 Batch   60/215   train_loss = 3.333
Epoch  76 Batch   70/215   train_loss = 3.418
Epoch  76 Batch   80/215   train_loss = 3.241
Epoch  76 Batch   90/215   train_loss = 3.278
Epoch  76 Batch  100/215   train_loss = 3.291
Epoch  76 Batch  110/215   train_loss = 3.136
Epoch  76 Batch  120/215   train_loss = 3.347
Epoch  76 Batch  130/215   train_loss = 3.484
Epoch  76 Batch  140/215   train_loss = 3.448
Epoch  76 Batch  150/215   train_loss = 3.463
Epoch  76 Batch  160/215   train_loss = 3.414
Epoch  76 Batch  170/215   train_loss = 3.136
Epoch  76 Batch  180/215   train_loss = 3.340
Epoch  76 Batch  190/215   train_loss = 3.368
Epoch  76 Batch  200/215   train_loss = 3.275
Epoch  76 Batch  210/215   train_loss = 3.071
Epoch  77 Batch    5/215   train_loss = 3.494
Epoch  77 Batch   15/215   train_loss = 3.297
Epoch  77 Batch   25/215   train_loss = 3.258
Epoch  77 Batch   35/215   train_loss = 3.394
Epoch  77 Batch   45/215   train_loss = 3.271
Epoch  77 Batch   55/215   train_loss = 3.313
Epoch  77 Batch   65/215   train_loss = 3.184
Epoch  77 Batch   75/215   train_loss = 3.237
Epoch  77 Batch   85/215   train_loss = 3.336
Epoch  77 Batch   95/215   train_loss = 3.182
Epoch  77 Batch  105/215   train_loss = 3.278
Epoch  77 Batch  115/215   train_loss = 3.330
Epoch  77 Batch  125/215   train_loss = 3.228
Epoch  77 Batch  135/215   train_loss = 3.172
Epoch  77 Batch  145/215   train_loss = 3.152
Epoch  77 Batch  155/215   train_loss = 3.468
Epoch  77 Batch  165/215   train_loss = 3.351
Epoch  77 Batch  175/215   train_loss = 3.505
Epoch  77 Batch  185/215   train_loss = 3.268
Epoch  77 Batch  195/215   train_loss = 3.320
Epoch  77 Batch  205/215   train_loss = 3.222
Epoch  78 Batch    0/215   train_loss = 3.387
Epoch  78 Batch   10/215   train_loss = 3.454
Epoch  78 Batch   20/215   train_loss = 3.294
Epoch  78 Batch   30/215   train_loss = 3.309
Epoch  78 Batch   40/215   train_loss = 3.146
Epoch  78 Batch   50/215   train_loss = 3.197
Epoch  78 Batch   60/215   train_loss = 3.429
Epoch  78 Batch   70/215   train_loss = 3.409
Epoch  78 Batch   80/215   train_loss = 3.197
Epoch  78 Batch   90/215   train_loss = 3.260
Epoch  78 Batch  100/215   train_loss = 3.377
Epoch  78 Batch  110/215   train_loss = 3.302
Epoch  78 Batch  120/215   train_loss = 3.457
Epoch  78 Batch  130/215   train_loss = 3.679
Epoch  78 Batch  140/215   train_loss = 3.412
Epoch  78 Batch  150/215   train_loss = 3.266
Epoch  78 Batch  160/215   train_loss = 3.471
Epoch  78 Batch  170/215   train_loss = 3.166
Epoch  78 Batch  180/215   train_loss = 3.289
Epoch  78 Batch  190/215   train_loss = 3.470
Epoch  78 Batch  200/215   train_loss = 3.105
Epoch  78 Batch  210/215   train_loss = 3.074
Epoch  79 Batch    5/215   train_loss = 3.652
Epoch  79 Batch   15/215   train_loss = 3.352
Epoch  79 Batch   25/215   train_loss = 3.202
Epoch  79 Batch   35/215   train_loss = 3.474
Epoch  79 Batch   45/215   train_loss = 3.170
Epoch  79 Batch   55/215   train_loss = 3.296
Epoch  79 Batch   65/215   train_loss = 3.464
Epoch  79 Batch   75/215   train_loss = 3.362
Epoch  79 Batch   85/215   train_loss = 3.227
Epoch  79 Batch   95/215   train_loss = 3.337
Epoch  79 Batch  105/215   train_loss = 3.258
Epoch  79 Batch  115/215   train_loss = 3.237
Epoch  79 Batch  125/215   train_loss = 3.229
Epoch  79 Batch  135/215   train_loss = 3.163
Epoch  79 Batch  145/215   train_loss = 3.167
Epoch  79 Batch  155/215   train_loss = 3.288
Epoch  79 Batch  165/215   train_loss = 3.309
Epoch  79 Batch  175/215   train_loss = 3.445
Epoch  79 Batch  185/215   train_loss = 3.405
Epoch  79 Batch  195/215   train_loss = 3.218
Epoch  79 Batch  205/215   train_loss = 3.378
Epoch  80 Batch    0/215   train_loss = 3.404
Epoch  80 Batch   10/215   train_loss = 3.334
Epoch  80 Batch   20/215   train_loss = 3.233
Epoch  80 Batch   30/215   train_loss = 3.274
Epoch  80 Batch   40/215   train_loss = 3.222
Epoch  80 Batch   50/215   train_loss = 3.206
Epoch  80 Batch   60/215   train_loss = 3.495
Epoch  80 Batch   70/215   train_loss = 3.389
Epoch  80 Batch   80/215   train_loss = 3.201
Epoch  80 Batch   90/215   train_loss = 3.366
Epoch  80 Batch  100/215   train_loss = 3.173
Epoch  80 Batch  110/215   train_loss = 3.161
Epoch  80 Batch  120/215   train_loss = 3.361
Epoch  80 Batch  130/215   train_loss = 3.516
Epoch  80 Batch  140/215   train_loss = 3.340
Epoch  80 Batch  150/215   train_loss = 3.476
Epoch  80 Batch  160/215   train_loss = 3.372
Epoch  80 Batch  170/215   train_loss = 3.202
Epoch  80 Batch  180/215   train_loss = 3.284
Epoch  80 Batch  190/215   train_loss = 3.257
Epoch  80 Batch  200/215   train_loss = 3.291
Epoch  80 Batch  210/215   train_loss = 3.116
Epoch  81 Batch    5/215   train_loss = 3.517
Epoch  81 Batch   15/215   train_loss = 3.348
Epoch  81 Batch   25/215   train_loss = 3.223
Epoch  81 Batch   35/215   train_loss = 3.236
Epoch  81 Batch   45/215   train_loss = 3.107
Epoch  81 Batch   55/215   train_loss = 3.201
Epoch  81 Batch   65/215   train_loss = 3.190
Epoch  81 Batch   75/215   train_loss = 3.363
Epoch  81 Batch   85/215   train_loss = 3.296
Epoch  81 Batch   95/215   train_loss = 3.188
Epoch  81 Batch  105/215   train_loss = 3.097
Epoch  81 Batch  115/215   train_loss = 3.120
Epoch  81 Batch  125/215   train_loss = 3.072
Epoch  81 Batch  135/215   train_loss = 3.243
Epoch  81 Batch  145/215   train_loss = 3.225
Epoch  81 Batch  155/215   train_loss = 3.349
Epoch  81 Batch  165/215   train_loss = 3.391
Epoch  81 Batch  175/215   train_loss = 3.300
Epoch  81 Batch  185/215   train_loss = 3.363
Epoch  81 Batch  195/215   train_loss = 3.367
Epoch  81 Batch  205/215   train_loss = 3.289
Epoch  82 Batch    0/215   train_loss = 3.234
Epoch  82 Batch   10/215   train_loss = 3.393
Epoch  82 Batch   20/215   train_loss = 3.220
Epoch  82 Batch   30/215   train_loss = 3.107
Epoch  82 Batch   40/215   train_loss = 3.243
Epoch  82 Batch   50/215   train_loss = 3.174
Epoch  82 Batch   60/215   train_loss = 3.452
Epoch  82 Batch   70/215   train_loss = 3.319
Epoch  82 Batch   80/215   train_loss = 3.275
Epoch  82 Batch   90/215   train_loss = 3.385
Epoch  82 Batch  100/215   train_loss = 3.404
Epoch  82 Batch  110/215   train_loss = 3.296
Epoch  82 Batch  120/215   train_loss = 3.281
Epoch  82 Batch  130/215   train_loss = 3.410
Epoch  82 Batch  140/215   train_loss = 3.438
Epoch  82 Batch  150/215   train_loss = 3.389
Epoch  82 Batch  160/215   train_loss = 3.458
Epoch  82 Batch  170/215   train_loss = 3.261
Epoch  82 Batch  180/215   train_loss = 3.476
Epoch  82 Batch  190/215   train_loss = 3.179
Epoch  82 Batch  200/215   train_loss = 3.150
Epoch  82 Batch  210/215   train_loss = 3.139
Epoch  83 Batch    5/215   train_loss = 3.402
Epoch  83 Batch   15/215   train_loss = 3.303
Epoch  83 Batch   25/215   train_loss = 3.257
Epoch  83 Batch   35/215   train_loss = 3.474
Epoch  83 Batch   45/215   train_loss = 3.110
Epoch  83 Batch   55/215   train_loss = 3.266
Epoch  83 Batch   65/215   train_loss = 3.312
Epoch  83 Batch   75/215   train_loss = 3.282
Epoch  83 Batch   85/215   train_loss = 3.258
Epoch  83 Batch   95/215   train_loss = 3.035
Epoch  83 Batch  105/215   train_loss = 3.115
Epoch  83 Batch  115/215   train_loss = 3.133
Epoch  83 Batch  125/215   train_loss = 3.023
Epoch  83 Batch  135/215   train_loss = 3.281
Epoch  83 Batch  145/215   train_loss = 3.309
Epoch  83 Batch  155/215   train_loss = 3.359
Epoch  83 Batch  165/215   train_loss = 3.358
Epoch  83 Batch  175/215   train_loss = 3.412
Epoch  83 Batch  185/215   train_loss = 3.280
Epoch  83 Batch  195/215   train_loss = 3.314
Epoch  83 Batch  205/215   train_loss = 3.198
Epoch  84 Batch    0/215   train_loss = 3.309
Epoch  84 Batch   10/215   train_loss = 3.251
Epoch  84 Batch   20/215   train_loss = 3.282
Epoch  84 Batch   30/215   train_loss = 3.375
Epoch  84 Batch   40/215   train_loss = 3.182
Epoch  84 Batch   50/215   train_loss = 3.187
Epoch  84 Batch   60/215   train_loss = 3.302
Epoch  84 Batch   70/215   train_loss = 3.167
Epoch  84 Batch   80/215   train_loss = 3.197
Epoch  84 Batch   90/215   train_loss = 3.323
Epoch  84 Batch  100/215   train_loss = 3.358
Epoch  84 Batch  110/215   train_loss = 3.326
Epoch  84 Batch  120/215   train_loss = 3.411
Epoch  84 Batch  130/215   train_loss = 3.465
Epoch  84 Batch  140/215   train_loss = 3.412
Epoch  84 Batch  150/215   train_loss = 3.217
Epoch  84 Batch  160/215   train_loss = 3.510
Epoch  84 Batch  170/215   train_loss = 3.249
Epoch  84 Batch  180/215   train_loss = 3.143
Epoch  84 Batch  190/215   train_loss = 3.411
Epoch  84 Batch  200/215   train_loss = 3.190
Epoch  84 Batch  210/215   train_loss = 3.150
Epoch  85 Batch    5/215   train_loss = 3.301
Epoch  85 Batch   15/215   train_loss = 3.150
Epoch  85 Batch   25/215   train_loss = 3.127
Epoch  85 Batch   35/215   train_loss = 3.192
Epoch  85 Batch   45/215   train_loss = 3.254
Epoch  85 Batch   55/215   train_loss = 3.195
Epoch  85 Batch   65/215   train_loss = 3.326
Epoch  85 Batch   75/215   train_loss = 3.258
Epoch  85 Batch   85/215   train_loss = 3.092
Epoch  85 Batch   95/215   train_loss = 3.151
Epoch  85 Batch  105/215   train_loss = 3.017
Epoch  85 Batch  115/215   train_loss = 3.225
Epoch  85 Batch  125/215   train_loss = 3.041
Epoch  85 Batch  135/215   train_loss = 3.134
Epoch  85 Batch  145/215   train_loss = 3.283
Epoch  85 Batch  155/215   train_loss = 3.412
Epoch  85 Batch  165/215   train_loss = 3.211
Epoch  85 Batch  175/215   train_loss = 3.431
Epoch  85 Batch  185/215   train_loss = 3.255
Epoch  85 Batch  195/215   train_loss = 3.183
Epoch  85 Batch  205/215   train_loss = 3.273
Epoch  86 Batch    0/215   train_loss = 3.233
Epoch  86 Batch   10/215   train_loss = 3.459
Epoch  86 Batch   20/215   train_loss = 3.340
Epoch  86 Batch   30/215   train_loss = 3.174
Epoch  86 Batch   40/215   train_loss = 3.343
Epoch  86 Batch   50/215   train_loss = 3.187
Epoch  86 Batch   60/215   train_loss = 3.329
Epoch  86 Batch   70/215   train_loss = 3.179
Epoch  86 Batch   80/215   train_loss = 3.214
Epoch  86 Batch   90/215   train_loss = 3.237
Epoch  86 Batch  100/215   train_loss = 3.415
Epoch  86 Batch  110/215   train_loss = 3.235
Epoch  86 Batch  120/215   train_loss = 3.213
Epoch  86 Batch  130/215   train_loss = 3.389
Epoch  86 Batch  140/215   train_loss = 3.276
Epoch  86 Batch  150/215   train_loss = 3.354
Epoch  86 Batch  160/215   train_loss = 3.314
Epoch  86 Batch  170/215   train_loss = 3.068
Epoch  86 Batch  180/215   train_loss = 3.237
Epoch  86 Batch  190/215   train_loss = 3.395
Epoch  86 Batch  200/215   train_loss = 3.006
Epoch  86 Batch  210/215   train_loss = 3.037
Epoch  87 Batch    5/215   train_loss = 3.339
Epoch  87 Batch   15/215   train_loss = 3.201
Epoch  87 Batch   25/215   train_loss = 3.112
Epoch  87 Batch   35/215   train_loss = 3.232
Epoch  87 Batch   45/215   train_loss = 3.108
Epoch  87 Batch   55/215   train_loss = 3.280
Epoch  87 Batch   65/215   train_loss = 3.296
Epoch  87 Batch   75/215   train_loss = 3.235
Epoch  87 Batch   85/215   train_loss = 3.198
Epoch  87 Batch   95/215   train_loss = 3.328
Epoch  87 Batch  105/215   train_loss = 3.161
Epoch  87 Batch  115/215   train_loss = 3.185
Epoch  87 Batch  125/215   train_loss = 3.078
Epoch  87 Batch  135/215   train_loss = 3.334
Epoch  87 Batch  145/215   train_loss = 3.012
Epoch  87 Batch  155/215   train_loss = 3.230
Epoch  87 Batch  165/215   train_loss = 3.308
Epoch  87 Batch  175/215   train_loss = 3.363
Epoch  87 Batch  185/215   train_loss = 3.239
Epoch  87 Batch  195/215   train_loss = 3.198
Epoch  87 Batch  205/215   train_loss = 3.249
Epoch  88 Batch    0/215   train_loss = 3.242
Epoch  88 Batch   10/215   train_loss = 3.397
Epoch  88 Batch   20/215   train_loss = 3.164
Epoch  88 Batch   30/215   train_loss = 3.232
Epoch  88 Batch   40/215   train_loss = 3.145
Epoch  88 Batch   50/215   train_loss = 3.026
Epoch  88 Batch   60/215   train_loss = 3.407
Epoch  88 Batch   70/215   train_loss = 3.218
Epoch  88 Batch   80/215   train_loss = 3.280
Epoch  88 Batch   90/215   train_loss = 3.240
Epoch  88 Batch  100/215   train_loss = 3.314
Epoch  88 Batch  110/215   train_loss = 3.151
Epoch  88 Batch  120/215   train_loss = 3.298
Epoch  88 Batch  130/215   train_loss = 3.563
Epoch  88 Batch  140/215   train_loss = 3.486
Epoch  88 Batch  150/215   train_loss = 3.212
Epoch  88 Batch  160/215   train_loss = 3.219
Epoch  88 Batch  170/215   train_loss = 3.197
Epoch  88 Batch  180/215   train_loss = 3.179
Epoch  88 Batch  190/215   train_loss = 3.192
Epoch  88 Batch  200/215   train_loss = 3.193
Epoch  88 Batch  210/215   train_loss = 3.088
Epoch  89 Batch    5/215   train_loss = 3.261
Epoch  89 Batch   15/215   train_loss = 3.153
Epoch  89 Batch   25/215   train_loss = 3.182
Epoch  89 Batch   35/215   train_loss = 3.360
Epoch  89 Batch   45/215   train_loss = 3.174
Epoch  89 Batch   55/215   train_loss = 3.171
Epoch  89 Batch   65/215   train_loss = 3.318
Epoch  89 Batch   75/215   train_loss = 3.227
Epoch  89 Batch   85/215   train_loss = 3.122
Epoch  89 Batch   95/215   train_loss = 3.306
Epoch  89 Batch  105/215   train_loss = 3.143
Epoch  89 Batch  115/215   train_loss = 3.080
Epoch  89 Batch  125/215   train_loss = 2.961
Epoch  89 Batch  135/215   train_loss = 3.167
Epoch  89 Batch  145/215   train_loss = 3.185
Epoch  89 Batch  155/215   train_loss = 3.257
Epoch  89 Batch  165/215   train_loss = 3.319
Epoch  89 Batch  175/215   train_loss = 3.530
Epoch  89 Batch  185/215   train_loss = 3.236
Epoch  89 Batch  195/215   train_loss = 3.261
Epoch  89 Batch  205/215   train_loss = 3.336
Epoch  90 Batch    0/215   train_loss = 3.244
Epoch  90 Batch   10/215   train_loss = 3.401
Epoch  90 Batch   20/215   train_loss = 3.217
Epoch  90 Batch   30/215   train_loss = 3.125
Epoch  90 Batch   40/215   train_loss = 3.129
Epoch  90 Batch   50/215   train_loss = 3.063
Epoch  90 Batch   60/215   train_loss = 3.327
Epoch  90 Batch   70/215   train_loss = 3.123
Epoch  90 Batch   80/215   train_loss = 3.166
Epoch  90 Batch   90/215   train_loss = 3.294
Epoch  90 Batch  100/215   train_loss = 3.264
Epoch  90 Batch  110/215   train_loss = 3.181
Epoch  90 Batch  120/215   train_loss = 3.161
Epoch  90 Batch  130/215   train_loss = 3.459
Epoch  90 Batch  140/215   train_loss = 3.396
Epoch  90 Batch  150/215   train_loss = 3.276
Epoch  90 Batch  160/215   train_loss = 3.403
Epoch  90 Batch  170/215   train_loss = 3.168
Epoch  90 Batch  180/215   train_loss = 3.295
Epoch  90 Batch  190/215   train_loss = 3.216
Epoch  90 Batch  200/215   train_loss = 3.289
Epoch  90 Batch  210/215   train_loss = 3.127
Epoch  91 Batch    5/215   train_loss = 3.275
Epoch  91 Batch   15/215   train_loss = 3.092
Epoch  91 Batch   25/215   train_loss = 3.128
Epoch  91 Batch   35/215   train_loss = 3.172
Epoch  91 Batch   45/215   train_loss = 3.099
Epoch  91 Batch   55/215   train_loss = 3.328
Epoch  91 Batch   65/215   train_loss = 3.229
Epoch  91 Batch   75/215   train_loss = 3.321
Epoch  91 Batch   85/215   train_loss = 3.311
Epoch  91 Batch   95/215   train_loss = 3.125
Epoch  91 Batch  105/215   train_loss = 3.174
Epoch  91 Batch  115/215   train_loss = 3.024
Epoch  91 Batch  125/215   train_loss = 2.915
Epoch  91 Batch  135/215   train_loss = 3.030
Epoch  91 Batch  145/215   train_loss = 3.196
Epoch  91 Batch  155/215   train_loss = 3.446
Epoch  91 Batch  165/215   train_loss = 3.130
Epoch  91 Batch  175/215   train_loss = 3.627
Epoch  91 Batch  185/215   train_loss = 3.231
Epoch  91 Batch  195/215   train_loss = 3.025
Epoch  91 Batch  205/215   train_loss = 3.257
Epoch  92 Batch    0/215   train_loss = 3.386
Epoch  92 Batch   10/215   train_loss = 3.387
Epoch  92 Batch   20/215   train_loss = 3.021
Epoch  92 Batch   30/215   train_loss = 2.999
Epoch  92 Batch   40/215   train_loss = 3.155
Epoch  92 Batch   50/215   train_loss = 3.047
Epoch  92 Batch   60/215   train_loss = 3.354
Epoch  92 Batch   70/215   train_loss = 3.115
Epoch  92 Batch   80/215   train_loss = 3.143
Epoch  92 Batch   90/215   train_loss = 3.301
Epoch  92 Batch  100/215   train_loss = 3.423
Epoch  92 Batch  110/215   train_loss = 3.175
Epoch  92 Batch  120/215   train_loss = 3.382
Epoch  92 Batch  130/215   train_loss = 3.387
Epoch  92 Batch  140/215   train_loss = 3.219
Epoch  92 Batch  150/215   train_loss = 3.212
Epoch  92 Batch  160/215   train_loss = 3.293
Epoch  92 Batch  170/215   train_loss = 3.201
Epoch  92 Batch  180/215   train_loss = 3.392
Epoch  92 Batch  190/215   train_loss = 3.280
Epoch  92 Batch  200/215   train_loss = 3.006
Epoch  92 Batch  210/215   train_loss = 3.024
Epoch  93 Batch    5/215   train_loss = 3.258
Epoch  93 Batch   15/215   train_loss = 3.198
Epoch  93 Batch   25/215   train_loss = 3.164
Epoch  93 Batch   35/215   train_loss = 3.334
Epoch  93 Batch   45/215   train_loss = 3.253
Epoch  93 Batch   55/215   train_loss = 3.154
Epoch  93 Batch   65/215   train_loss = 3.313
Epoch  93 Batch   75/215   train_loss = 3.054
Epoch  93 Batch   85/215   train_loss = 3.041
Epoch  93 Batch   95/215   train_loss = 3.166
Epoch  93 Batch  105/215   train_loss = 3.284
Epoch  93 Batch  115/215   train_loss = 3.101
Epoch  93 Batch  125/215   train_loss = 3.026
Epoch  93 Batch  135/215   train_loss = 3.098
Epoch  93 Batch  145/215   train_loss = 3.251
Epoch  93 Batch  155/215   train_loss = 3.286
Epoch  93 Batch  165/215   train_loss = 3.157
Epoch  93 Batch  175/215   train_loss = 3.442
Epoch  93 Batch  185/215   train_loss = 3.267
Epoch  93 Batch  195/215   train_loss = 3.131
Epoch  93 Batch  205/215   train_loss = 3.235
Epoch  94 Batch    0/215   train_loss = 3.158
Epoch  94 Batch   10/215   train_loss = 3.393
Epoch  94 Batch   20/215   train_loss = 3.149
Epoch  94 Batch   30/215   train_loss = 3.111
Epoch  94 Batch   40/215   train_loss = 3.050
Epoch  94 Batch   50/215   train_loss = 3.244
Epoch  94 Batch   60/215   train_loss = 3.252
Epoch  94 Batch   70/215   train_loss = 3.197
Epoch  94 Batch   80/215   train_loss = 3.058
Epoch  94 Batch   90/215   train_loss = 3.272
Epoch  94 Batch  100/215   train_loss = 3.457
Epoch  94 Batch  110/215   train_loss = 3.174
Epoch  94 Batch  120/215   train_loss = 3.328
Epoch  94 Batch  130/215   train_loss = 3.306
Epoch  94 Batch  140/215   train_loss = 3.255
Epoch  94 Batch  150/215   train_loss = 3.253
Epoch  94 Batch  160/215   train_loss = 3.312
Epoch  94 Batch  170/215   train_loss = 3.158
Epoch  94 Batch  180/215   train_loss = 3.105
Epoch  94 Batch  190/215   train_loss = 3.278
Epoch  94 Batch  200/215   train_loss = 3.136
Epoch  94 Batch  210/215   train_loss = 2.923
Epoch  95 Batch    5/215   train_loss = 3.224
Epoch  95 Batch   15/215   train_loss = 3.222
Epoch  95 Batch   25/215   train_loss = 3.198
Epoch  95 Batch   35/215   train_loss = 3.272
Epoch  95 Batch   45/215   train_loss = 3.099
Epoch  95 Batch   55/215   train_loss = 3.173
Epoch  95 Batch   65/215   train_loss = 3.371
Epoch  95 Batch   75/215   train_loss = 3.209
Epoch  95 Batch   85/215   train_loss = 3.164
Epoch  95 Batch   95/215   train_loss = 3.265
Epoch  95 Batch  105/215   train_loss = 3.268
Epoch  95 Batch  115/215   train_loss = 3.025
Epoch  95 Batch  125/215   train_loss = 2.980
Epoch  95 Batch  135/215   train_loss = 3.075
Epoch  95 Batch  145/215   train_loss = 3.110
Epoch  95 Batch  155/215   train_loss = 3.240
Epoch  95 Batch  165/215   train_loss = 3.216
Epoch  95 Batch  175/215   train_loss = 3.420
Epoch  95 Batch  185/215   train_loss = 3.274
Epoch  95 Batch  195/215   train_loss = 3.208
Epoch  95 Batch  205/215   train_loss = 3.109
Epoch  96 Batch    0/215   train_loss = 3.109
Epoch  96 Batch   10/215   train_loss = 3.218
Epoch  96 Batch   20/215   train_loss = 3.167
Epoch  96 Batch   30/215   train_loss = 3.071
Epoch  96 Batch   40/215   train_loss = 3.242
Epoch  96 Batch   50/215   train_loss = 3.267
Epoch  96 Batch   60/215   train_loss = 3.312
Epoch  96 Batch   70/215   train_loss = 3.155
Epoch  96 Batch   80/215   train_loss = 3.225
Epoch  96 Batch   90/215   train_loss = 3.240
Epoch  96 Batch  100/215   train_loss = 3.224
Epoch  96 Batch  110/215   train_loss = 3.245
Epoch  96 Batch  120/215   train_loss = 3.282
Epoch  96 Batch  130/215   train_loss = 3.326
Epoch  96 Batch  140/215   train_loss = 3.190
Epoch  96 Batch  150/215   train_loss = 3.063
Epoch  96 Batch  160/215   train_loss = 3.363
Epoch  96 Batch  170/215   train_loss = 3.147
Epoch  96 Batch  180/215   train_loss = 3.186
Epoch  96 Batch  190/215   train_loss = 3.252
Epoch  96 Batch  200/215   train_loss = 3.151
Epoch  96 Batch  210/215   train_loss = 2.913
Epoch  97 Batch    5/215   train_loss = 3.239
Epoch  97 Batch   15/215   train_loss = 3.021
Epoch  97 Batch   25/215   train_loss = 3.213
Epoch  97 Batch   35/215   train_loss = 3.357
Epoch  97 Batch   45/215   train_loss = 3.128
Epoch  97 Batch   55/215   train_loss = 3.087
Epoch  97 Batch   65/215   train_loss = 3.202
Epoch  97 Batch   75/215   train_loss = 3.140
Epoch  97 Batch   85/215   train_loss = 3.122
Epoch  97 Batch   95/215   train_loss = 3.294
Epoch  97 Batch  105/215   train_loss = 3.154
Epoch  97 Batch  115/215   train_loss = 3.073
Epoch  97 Batch  125/215   train_loss = 3.008
Epoch  97 Batch  135/215   train_loss = 3.079
Epoch  97 Batch  145/215   train_loss = 3.252
Epoch  97 Batch  155/215   train_loss = 3.094
Epoch  97 Batch  165/215   train_loss = 3.082
Epoch  97 Batch  175/215   train_loss = 3.368
Epoch  97 Batch  185/215   train_loss = 3.326
Epoch  97 Batch  195/215   train_loss = 3.173
Epoch  97 Batch  205/215   train_loss = 3.245
Epoch  98 Batch    0/215   train_loss = 3.252
Epoch  98 Batch   10/215   train_loss = 3.394
Epoch  98 Batch   20/215   train_loss = 3.340
Epoch  98 Batch   30/215   train_loss = 3.010
Epoch  98 Batch   40/215   train_loss = 3.071
Epoch  98 Batch   50/215   train_loss = 3.195
Epoch  98 Batch   60/215   train_loss = 3.187
Epoch  98 Batch   70/215   train_loss = 3.096
Epoch  98 Batch   80/215   train_loss = 3.160
Epoch  98 Batch   90/215   train_loss = 3.331
Epoch  98 Batch  100/215   train_loss = 3.283
Epoch  98 Batch  110/215   train_loss = 3.200
Epoch  98 Batch  120/215   train_loss = 3.245
Epoch  98 Batch  130/215   train_loss = 3.368
Epoch  98 Batch  140/215   train_loss = 3.075
Epoch  98 Batch  150/215   train_loss = 3.241
Epoch  98 Batch  160/215   train_loss = 3.288
Epoch  98 Batch  170/215   train_loss = 3.179
Epoch  98 Batch  180/215   train_loss = 3.218
Epoch  98 Batch  190/215   train_loss = 3.252
Epoch  98 Batch  200/215   train_loss = 3.105
Epoch  98 Batch  210/215   train_loss = 3.133
Epoch  99 Batch    5/215   train_loss = 3.294
Epoch  99 Batch   15/215   train_loss = 3.032
Epoch  99 Batch   25/215   train_loss = 3.275
Epoch  99 Batch   35/215   train_loss = 3.313
Epoch  99 Batch   45/215   train_loss = 3.163
Epoch  99 Batch   55/215   train_loss = 3.041
Epoch  99 Batch   65/215   train_loss = 3.281
Epoch  99 Batch   75/215   train_loss = 3.069
Epoch  99 Batch   85/215   train_loss = 3.068
Epoch  99 Batch   95/215   train_loss = 3.025
Epoch  99 Batch  105/215   train_loss = 3.059
Epoch  99 Batch  115/215   train_loss = 3.020
Epoch  99 Batch  125/215   train_loss = 2.945
Epoch  99 Batch  135/215   train_loss = 2.934
Epoch  99 Batch  145/215   train_loss = 3.198
Epoch  99 Batch  155/215   train_loss = 3.120
Epoch  99 Batch  165/215   train_loss = 3.214
Epoch  99 Batch  175/215   train_loss = 3.407
Epoch  99 Batch  185/215   train_loss = 3.296
Epoch  99 Batch  195/215   train_loss = 2.975
Epoch  99 Batch  205/215   train_loss = 3.257
Epoch 100 Batch    0/215   train_loss = 3.208
Epoch 100 Batch   10/215   train_loss = 3.204
Epoch 100 Batch   20/215   train_loss = 3.206
Epoch 100 Batch   30/215   train_loss = 3.149
Epoch 100 Batch   40/215   train_loss = 3.079
Epoch 100 Batch   50/215   train_loss = 3.040
Epoch 100 Batch   60/215   train_loss = 3.193
Epoch 100 Batch   70/215   train_loss = 3.038
Epoch 100 Batch   80/215   train_loss = 3.174
Epoch 100 Batch   90/215   train_loss = 3.271
Epoch 100 Batch  100/215   train_loss = 3.123
Epoch 100 Batch  110/215   train_loss = 3.100
Epoch 100 Batch  120/215   train_loss = 3.078
Epoch 100 Batch  130/215   train_loss = 3.412
Epoch 100 Batch  140/215   train_loss = 3.271
Epoch 100 Batch  150/215   train_loss = 3.076
Epoch 100 Batch  160/215   train_loss = 3.255
Epoch 100 Batch  170/215   train_loss = 3.130
Epoch 100 Batch  180/215   train_loss = 3.217
Epoch 100 Batch  190/215   train_loss = 3.302
Epoch 100 Batch  200/215   train_loss = 3.126
Epoch 100 Batch  210/215   train_loss = 3.062
Epoch 101 Batch    5/215   train_loss = 3.272
Epoch 101 Batch   15/215   train_loss = 3.210
Epoch 101 Batch   25/215   train_loss = 3.097
Epoch 101 Batch   35/215   train_loss = 3.288
Epoch 101 Batch   45/215   train_loss = 3.249
Epoch 101 Batch   55/215   train_loss = 3.082
Epoch 101 Batch   65/215   train_loss = 3.135
Epoch 101 Batch   75/215   train_loss = 3.169
Epoch 101 Batch   85/215   train_loss = 3.060
Epoch 101 Batch   95/215   train_loss = 3.141
Epoch 101 Batch  105/215   train_loss = 3.104
Epoch 101 Batch  115/215   train_loss = 3.063
Epoch 101 Batch  125/215   train_loss = 2.943
Epoch 101 Batch  135/215   train_loss = 3.069
Epoch 101 Batch  145/215   train_loss = 2.892
Epoch 101 Batch  155/215   train_loss = 3.260
Epoch 101 Batch  165/215   train_loss = 3.055
Epoch 101 Batch  175/215   train_loss = 3.364
Epoch 101 Batch  185/215   train_loss = 3.331
Epoch 101 Batch  195/215   train_loss = 3.132
Epoch 101 Batch  205/215   train_loss = 3.090
Epoch 102 Batch    0/215   train_loss = 3.276
Epoch 102 Batch   10/215   train_loss = 3.346
Epoch 102 Batch   20/215   train_loss = 3.054
Epoch 102 Batch   30/215   train_loss = 3.041
Epoch 102 Batch   40/215   train_loss = 3.287
Epoch 102 Batch   50/215   train_loss = 3.115
Epoch 102 Batch   60/215   train_loss = 3.260
Epoch 102 Batch   70/215   train_loss = 3.167
Epoch 102 Batch   80/215   train_loss = 3.212
Epoch 102 Batch   90/215   train_loss = 3.272
Epoch 102 Batch  100/215   train_loss = 3.359
Epoch 102 Batch  110/215   train_loss = 2.978
Epoch 102 Batch  120/215   train_loss = 3.227
Epoch 102 Batch  130/215   train_loss = 3.257
Epoch 102 Batch  140/215   train_loss = 3.128
Epoch 102 Batch  150/215   train_loss = 3.091
Epoch 102 Batch  160/215   train_loss = 3.314
Epoch 102 Batch  170/215   train_loss = 3.193
Epoch 102 Batch  180/215   train_loss = 3.232
Epoch 102 Batch  190/215   train_loss = 3.149
Epoch 102 Batch  200/215   train_loss = 2.971
Epoch 102 Batch  210/215   train_loss = 3.120
Epoch 103 Batch    5/215   train_loss = 3.076
Epoch 103 Batch   15/215   train_loss = 3.018
Epoch 103 Batch   25/215   train_loss = 3.016
Epoch 103 Batch   35/215   train_loss = 3.219
Epoch 103 Batch   45/215   train_loss = 3.145
Epoch 103 Batch   55/215   train_loss = 3.095
Epoch 103 Batch   65/215   train_loss = 3.200
Epoch 103 Batch   75/215   train_loss = 3.226
Epoch 103 Batch   85/215   train_loss = 3.035
Epoch 103 Batch   95/215   train_loss = 3.266
Epoch 103 Batch  105/215   train_loss = 3.226
Epoch 103 Batch  115/215   train_loss = 3.034
Epoch 103 Batch  125/215   train_loss = 2.981
Epoch 103 Batch  135/215   train_loss = 3.057
Epoch 103 Batch  145/215   train_loss = 3.148
Epoch 103 Batch  155/215   train_loss = 3.190
Epoch 103 Batch  165/215   train_loss = 3.309
Epoch 103 Batch  175/215   train_loss = 3.417
Epoch 103 Batch  185/215   train_loss = 3.368
Epoch 103 Batch  195/215   train_loss = 3.136
Epoch 103 Batch  205/215   train_loss = 3.164
Epoch 104 Batch    0/215   train_loss = 3.105
Epoch 104 Batch   10/215   train_loss = 3.428
Epoch 104 Batch   20/215   train_loss = 3.217
Epoch 104 Batch   30/215   train_loss = 3.124
Epoch 104 Batch   40/215   train_loss = 3.149
Epoch 104 Batch   50/215   train_loss = 3.038
Epoch 104 Batch   60/215   train_loss = 3.223
Epoch 104 Batch   70/215   train_loss = 2.948
Epoch 104 Batch   80/215   train_loss = 3.004
Epoch 104 Batch   90/215   train_loss = 3.249
Epoch 104 Batch  100/215   train_loss = 3.086
Epoch 104 Batch  110/215   train_loss = 2.999
Epoch 104 Batch  120/215   train_loss = 3.237
Epoch 104 Batch  130/215   train_loss = 3.438
Epoch 104 Batch  140/215   train_loss = 3.003
Epoch 104 Batch  150/215   train_loss = 3.018
Epoch 104 Batch  160/215   train_loss = 3.309
Epoch 104 Batch  170/215   train_loss = 2.996
Epoch 104 Batch  180/215   train_loss = 3.104
Epoch 104 Batch  190/215   train_loss = 3.255
Epoch 104 Batch  200/215   train_loss = 3.112
Epoch 104 Batch  210/215   train_loss = 2.933
Epoch 105 Batch    5/215   train_loss = 3.209
Epoch 105 Batch   15/215   train_loss = 3.191
Epoch 105 Batch   25/215   train_loss = 3.227
Epoch 105 Batch   35/215   train_loss = 3.243
Epoch 105 Batch   45/215   train_loss = 3.081
Epoch 105 Batch   55/215   train_loss = 3.087
Epoch 105 Batch   65/215   train_loss = 3.131
Epoch 105 Batch   75/215   train_loss = 3.027
Epoch 105 Batch   85/215   train_loss = 3.059
Epoch 105 Batch   95/215   train_loss = 3.082
Epoch 105 Batch  105/215   train_loss = 3.005
Epoch 105 Batch  115/215   train_loss = 3.089
Epoch 105 Batch  125/215   train_loss = 2.874
Epoch 105 Batch  135/215   train_loss = 3.193
Epoch 105 Batch  145/215   train_loss = 3.328
Epoch 105 Batch  155/215   train_loss = 3.141
Epoch 105 Batch  165/215   train_loss = 3.118
Epoch 105 Batch  175/215   train_loss = 3.504
Epoch 105 Batch  185/215   train_loss = 3.141
Epoch 105 Batch  195/215   train_loss = 3.146
Epoch 105 Batch  205/215   train_loss = 3.277
Epoch 106 Batch    0/215   train_loss = 3.220
Epoch 106 Batch   10/215   train_loss = 3.308
Epoch 106 Batch   20/215   train_loss = 3.159
Epoch 106 Batch   30/215   train_loss = 3.066
Epoch 106 Batch   40/215   train_loss = 3.171
Epoch 106 Batch   50/215   train_loss = 3.292
Epoch 106 Batch   60/215   train_loss = 3.248
Epoch 106 Batch   70/215   train_loss = 2.984
Epoch 106 Batch   80/215   train_loss = 3.142
Epoch 106 Batch   90/215   train_loss = 3.087
Epoch 106 Batch  100/215   train_loss = 3.170
Epoch 106 Batch  110/215   train_loss = 3.132
Epoch 106 Batch  120/215   train_loss = 3.153
Epoch 106 Batch  130/215   train_loss = 3.386
Epoch 106 Batch  140/215   train_loss = 3.125
Epoch 106 Batch  150/215   train_loss = 3.046
Epoch 106 Batch  160/215   train_loss = 3.263
Epoch 106 Batch  170/215   train_loss = 3.128
Epoch 106 Batch  180/215   train_loss = 3.129
Epoch 106 Batch  190/215   train_loss = 3.299
Epoch 106 Batch  200/215   train_loss = 2.962
Epoch 106 Batch  210/215   train_loss = 3.001
Epoch 107 Batch    5/215   train_loss = 3.097
Epoch 107 Batch   15/215   train_loss = 3.032
Epoch 107 Batch   25/215   train_loss = 3.178
Epoch 107 Batch   35/215   train_loss = 3.218
Epoch 107 Batch   45/215   train_loss = 3.115
Epoch 107 Batch   55/215   train_loss = 3.066
Epoch 107 Batch   65/215   train_loss = 3.283
Epoch 107 Batch   75/215   train_loss = 2.963
Epoch 107 Batch   85/215   train_loss = 3.096
Epoch 107 Batch   95/215   train_loss = 3.050
Epoch 107 Batch  105/215   train_loss = 2.958
Epoch 107 Batch  115/215   train_loss = 3.023
Epoch 107 Batch  125/215   train_loss = 3.095
Epoch 107 Batch  135/215   train_loss = 3.183
Epoch 107 Batch  145/215   train_loss = 3.083
Epoch 107 Batch  155/215   train_loss = 3.150
Epoch 107 Batch  165/215   train_loss = 2.934
Epoch 107 Batch  175/215   train_loss = 3.406
Epoch 107 Batch  185/215   train_loss = 3.174
Epoch 107 Batch  195/215   train_loss = 2.915
Epoch 107 Batch  205/215   train_loss = 3.084
Epoch 108 Batch    0/215   train_loss = 3.248
Epoch 108 Batch   10/215   train_loss = 3.114
Epoch 108 Batch   20/215   train_loss = 3.219
Epoch 108 Batch   30/215   train_loss = 3.020
Epoch 108 Batch   40/215   train_loss = 3.158
Epoch 108 Batch   50/215   train_loss = 3.021
Epoch 108 Batch   60/215   train_loss = 3.293
Epoch 108 Batch   70/215   train_loss = 3.220
Epoch 108 Batch   80/215   train_loss = 3.115
Epoch 108 Batch   90/215   train_loss = 3.112
Epoch 108 Batch  100/215   train_loss = 3.305
Epoch 108 Batch  110/215   train_loss = 3.134
Epoch 108 Batch  120/215   train_loss = 2.987
Epoch 108 Batch  130/215   train_loss = 3.374
Epoch 108 Batch  140/215   train_loss = 3.148
Epoch 108 Batch  150/215   train_loss = 3.077
Epoch 108 Batch  160/215   train_loss = 3.230
Epoch 108 Batch  170/215   train_loss = 3.025
Epoch 108 Batch  180/215   train_loss = 3.027
Epoch 108 Batch  190/215   train_loss = 3.182
Epoch 108 Batch  200/215   train_loss = 3.051
Epoch 108 Batch  210/215   train_loss = 2.984
Epoch 109 Batch    5/215   train_loss = 3.298
Epoch 109 Batch   15/215   train_loss = 3.209
Epoch 109 Batch   25/215   train_loss = 3.094
Epoch 109 Batch   35/215   train_loss = 3.210
Epoch 109 Batch   45/215   train_loss = 3.080
Epoch 109 Batch   55/215   train_loss = 3.165
Epoch 109 Batch   65/215   train_loss = 3.034
Epoch 109 Batch   75/215   train_loss = 3.063
Epoch 109 Batch   85/215   train_loss = 2.968
Epoch 109 Batch   95/215   train_loss = 3.080
Epoch 109 Batch  105/215   train_loss = 2.996
Epoch 109 Batch  115/215   train_loss = 3.083
Epoch 109 Batch  125/215   train_loss = 2.943
Epoch 109 Batch  135/215   train_loss = 3.003
Epoch 109 Batch  145/215   train_loss = 3.032
Epoch 109 Batch  155/215   train_loss = 3.069
Epoch 109 Batch  165/215   train_loss = 2.968
Epoch 109 Batch  175/215   train_loss = 3.369
Epoch 109 Batch  185/215   train_loss = 3.098
Epoch 109 Batch  195/215   train_loss = 3.013
Epoch 109 Batch  205/215   train_loss = 3.048
Epoch 110 Batch    0/215   train_loss = 3.394
Epoch 110 Batch   10/215   train_loss = 3.307
Epoch 110 Batch   20/215   train_loss = 3.215
Epoch 110 Batch   30/215   train_loss = 3.059
Epoch 110 Batch   40/215   train_loss = 3.074
Epoch 110 Batch   50/215   train_loss = 2.914
Epoch 110 Batch   60/215   train_loss = 3.230
Epoch 110 Batch   70/215   train_loss = 2.969
Epoch 110 Batch   80/215   train_loss = 2.997
Epoch 110 Batch   90/215   train_loss = 3.234
Epoch 110 Batch  100/215   train_loss = 3.052
Epoch 110 Batch  110/215   train_loss = 3.017
Epoch 110 Batch  120/215   train_loss = 3.300
Epoch 110 Batch  130/215   train_loss = 3.326
Epoch 110 Batch  140/215   train_loss = 3.306
Epoch 110 Batch  150/215   train_loss = 3.301
Epoch 110 Batch  160/215   train_loss = 3.149
Epoch 110 Batch  170/215   train_loss = 3.167
Epoch 110 Batch  180/215   train_loss = 3.160
Epoch 110 Batch  190/215   train_loss = 3.168
Epoch 110 Batch  200/215   train_loss = 2.969
Epoch 110 Batch  210/215   train_loss = 2.930
Epoch 111 Batch    5/215   train_loss = 3.222
Epoch 111 Batch   15/215   train_loss = 2.955
Epoch 111 Batch   25/215   train_loss = 2.999
Epoch 111 Batch   35/215   train_loss = 3.153
Epoch 111 Batch   45/215   train_loss = 3.221
Epoch 111 Batch   55/215   train_loss = 3.044
Epoch 111 Batch   65/215   train_loss = 2.990
Epoch 111 Batch   75/215   train_loss = 3.058
Epoch 111 Batch   85/215   train_loss = 3.082
Epoch 111 Batch   95/215   train_loss = 3.060
Epoch 111 Batch  105/215   train_loss = 2.979
Epoch 111 Batch  115/215   train_loss = 2.958
Epoch 111 Batch  125/215   train_loss = 2.962
Epoch 111 Batch  135/215   train_loss = 3.300
Epoch 111 Batch  145/215   train_loss = 3.087
Epoch 111 Batch  155/215   train_loss = 3.120
Epoch 111 Batch  165/215   train_loss = 3.106
Epoch 111 Batch  175/215   train_loss = 3.313
Epoch 111 Batch  185/215   train_loss = 3.192
Epoch 111 Batch  195/215   train_loss = 3.135
Epoch 111 Batch  205/215   train_loss = 3.270
Epoch 112 Batch    0/215   train_loss = 3.130
Epoch 112 Batch   10/215   train_loss = 3.198
Epoch 112 Batch   20/215   train_loss = 3.115
Epoch 112 Batch   30/215   train_loss = 3.092
Epoch 112 Batch   40/215   train_loss = 3.303
Epoch 112 Batch   50/215   train_loss = 2.982
Epoch 112 Batch   60/215   train_loss = 3.055
Epoch 112 Batch   70/215   train_loss = 3.077
Epoch 112 Batch   80/215   train_loss = 2.970
Epoch 112 Batch   90/215   train_loss = 3.258
Epoch 112 Batch  100/215   train_loss = 3.314
Epoch 112 Batch  110/215   train_loss = 3.082
Epoch 112 Batch  120/215   train_loss = 3.182
Epoch 112 Batch  130/215   train_loss = 3.360
Epoch 112 Batch  140/215   train_loss = 3.129
Epoch 112 Batch  150/215   train_loss = 3.079
Epoch 112 Batch  160/215   train_loss = 2.982
Epoch 112 Batch  170/215   train_loss = 3.132
Epoch 112 Batch  180/215   train_loss = 3.167
Epoch 112 Batch  190/215   train_loss = 3.226
Epoch 112 Batch  200/215   train_loss = 2.899
Epoch 112 Batch  210/215   train_loss = 3.050
Epoch 113 Batch    5/215   train_loss = 3.181
Epoch 113 Batch   15/215   train_loss = 3.131
Epoch 113 Batch   25/215   train_loss = 2.998
Epoch 113 Batch   35/215   train_loss = 3.155
Epoch 113 Batch   45/215   train_loss = 3.213
Epoch 113 Batch   55/215   train_loss = 3.029
Epoch 113 Batch   65/215   train_loss = 3.187
Epoch 113 Batch   75/215   train_loss = 3.085
Epoch 113 Batch   85/215   train_loss = 3.052
Epoch 113 Batch   95/215   train_loss = 3.223
Epoch 113 Batch  105/215   train_loss = 2.923
Epoch 113 Batch  115/215   train_loss = 2.991
Epoch 113 Batch  125/215   train_loss = 2.849
Epoch 113 Batch  135/215   train_loss = 3.275
Epoch 113 Batch  145/215   train_loss = 3.051
Epoch 113 Batch  155/215   train_loss = 3.128
Epoch 113 Batch  165/215   train_loss = 3.101
Epoch 113 Batch  175/215   train_loss = 3.278
Epoch 113 Batch  185/215   train_loss = 3.098
Epoch 113 Batch  195/215   train_loss = 3.151
Epoch 113 Batch  205/215   train_loss = 3.117
Epoch 114 Batch    0/215   train_loss = 3.335
Epoch 114 Batch   10/215   train_loss = 3.430
Epoch 114 Batch   20/215   train_loss = 3.034
Epoch 114 Batch   30/215   train_loss = 3.105
Epoch 114 Batch   40/215   train_loss = 3.108
Epoch 114 Batch   50/215   train_loss = 3.039
Epoch 114 Batch   60/215   train_loss = 3.152
Epoch 114 Batch   70/215   train_loss = 3.182
Epoch 114 Batch   80/215   train_loss = 3.092
Epoch 114 Batch   90/215   train_loss = 3.168
Epoch 114 Batch  100/215   train_loss = 3.182
Epoch 114 Batch  110/215   train_loss = 3.067
Epoch 114 Batch  120/215   train_loss = 3.169
Epoch 114 Batch  130/215   train_loss = 3.367
Epoch 114 Batch  140/215   train_loss = 3.206
Epoch 114 Batch  150/215   train_loss = 3.008
Epoch 114 Batch  160/215   train_loss = 3.113
Epoch 114 Batch  170/215   train_loss = 3.058
Epoch 114 Batch  180/215   train_loss = 3.194
Epoch 114 Batch  190/215   train_loss = 3.275
Epoch 114 Batch  200/215   train_loss = 3.014
Epoch 114 Batch  210/215   train_loss = 2.878
Epoch 115 Batch    5/215   train_loss = 3.066
Epoch 115 Batch   15/215   train_loss = 2.995
Epoch 115 Batch   25/215   train_loss = 2.988
Epoch 115 Batch   35/215   train_loss = 3.148
Epoch 115 Batch   45/215   train_loss = 2.953
Epoch 115 Batch   55/215   train_loss = 3.128
Epoch 115 Batch   65/215   train_loss = 3.285
Epoch 115 Batch   75/215   train_loss = 2.978
Epoch 115 Batch   85/215   train_loss = 3.029
Epoch 115 Batch   95/215   train_loss = 3.126
Epoch 115 Batch  105/215   train_loss = 2.881
Epoch 115 Batch  115/215   train_loss = 2.970
Epoch 115 Batch  125/215   train_loss = 3.114
Epoch 115 Batch  135/215   train_loss = 3.102
Epoch 115 Batch  145/215   train_loss = 2.932
Epoch 115 Batch  155/215   train_loss = 3.108
Epoch 115 Batch  165/215   train_loss = 2.985
Epoch 115 Batch  175/215   train_loss = 3.191
Epoch 115 Batch  185/215   train_loss = 3.159
Epoch 115 Batch  195/215   train_loss = 3.078
Epoch 115 Batch  205/215   train_loss = 3.140
Epoch 116 Batch    0/215   train_loss = 3.145
Epoch 116 Batch   10/215   train_loss = 3.354
Epoch 116 Batch   20/215   train_loss = 3.011
Epoch 116 Batch   30/215   train_loss = 2.974
Epoch 116 Batch   40/215   train_loss = 3.191
Epoch 116 Batch   50/215   train_loss = 3.080
Epoch 116 Batch   60/215   train_loss = 3.200
Epoch 116 Batch   70/215   train_loss = 2.935
Epoch 116 Batch   80/215   train_loss = 3.128
Epoch 116 Batch   90/215   train_loss = 3.049
Epoch 116 Batch  100/215   train_loss = 3.062
Epoch 116 Batch  110/215   train_loss = 3.130
Epoch 116 Batch  120/215   train_loss = 3.245
Epoch 116 Batch  130/215   train_loss = 3.292
Epoch 116 Batch  140/215   train_loss = 3.249
Epoch 116 Batch  150/215   train_loss = 3.096
Epoch 116 Batch  160/215   train_loss = 3.189
Epoch 116 Batch  170/215   train_loss = 3.020
Epoch 116 Batch  180/215   train_loss = 3.197
Epoch 116 Batch  190/215   train_loss = 3.145
Epoch 116 Batch  200/215   train_loss = 2.914
Epoch 116 Batch  210/215   train_loss = 2.852
Epoch 117 Batch    5/215   train_loss = 3.195
Epoch 117 Batch   15/215   train_loss = 3.059
Epoch 117 Batch   25/215   train_loss = 3.041
Epoch 117 Batch   35/215   train_loss = 3.216
Epoch 117 Batch   45/215   train_loss = 2.959
Epoch 117 Batch   55/215   train_loss = 3.173
Epoch 117 Batch   65/215   train_loss = 3.123
Epoch 117 Batch   75/215   train_loss = 2.979
Epoch 117 Batch   85/215   train_loss = 3.206
Epoch 117 Batch   95/215   train_loss = 2.979
Epoch 117 Batch  105/215   train_loss = 2.934
Epoch 117 Batch  115/215   train_loss = 2.998
Epoch 117 Batch  125/215   train_loss = 2.825
Epoch 117 Batch  135/215   train_loss = 3.101
Epoch 117 Batch  145/215   train_loss = 3.164
Epoch 117 Batch  155/215   train_loss = 3.254
Epoch 117 Batch  165/215   train_loss = 3.186
Epoch 117 Batch  175/215   train_loss = 3.303
Epoch 117 Batch  185/215   train_loss = 3.110
Epoch 117 Batch  195/215   train_loss = 3.050
Epoch 117 Batch  205/215   train_loss = 3.007
Epoch 118 Batch    0/215   train_loss = 3.196
Epoch 118 Batch   10/215   train_loss = 3.220
Epoch 118 Batch   20/215   train_loss = 3.169
Epoch 118 Batch   30/215   train_loss = 3.041
Epoch 118 Batch   40/215   train_loss = 3.047
Epoch 118 Batch   50/215   train_loss = 2.900
Epoch 118 Batch   60/215   train_loss = 3.150
Epoch 118 Batch   70/215   train_loss = 2.976
Epoch 118 Batch   80/215   train_loss = 3.254
Epoch 118 Batch   90/215   train_loss = 3.123
Epoch 118 Batch  100/215   train_loss = 3.226
Epoch 118 Batch  110/215   train_loss = 3.058
Epoch 118 Batch  120/215   train_loss = 3.311
Epoch 118 Batch  130/215   train_loss = 3.190
Epoch 118 Batch  140/215   train_loss = 3.202
Epoch 118 Batch  150/215   train_loss = 3.131
Epoch 118 Batch  160/215   train_loss = 3.288
Epoch 118 Batch  170/215   train_loss = 3.000
Epoch 118 Batch  180/215   train_loss = 3.161
Epoch 118 Batch  190/215   train_loss = 3.140
Epoch 118 Batch  200/215   train_loss = 2.986
Epoch 118 Batch  210/215   train_loss = 2.826
Epoch 119 Batch    5/215   train_loss = 3.306
Epoch 119 Batch   15/215   train_loss = 3.027
Epoch 119 Batch   25/215   train_loss = 3.006
Epoch 119 Batch   35/215   train_loss = 3.132
Epoch 119 Batch   45/215   train_loss = 3.012
Epoch 119 Batch   55/215   train_loss = 2.979
Epoch 119 Batch   65/215   train_loss = 3.082
Epoch 119 Batch   75/215   train_loss = 3.035
Epoch 119 Batch   85/215   train_loss = 3.083
Epoch 119 Batch   95/215   train_loss = 2.979
Epoch 119 Batch  105/215   train_loss = 2.900
Epoch 119 Batch  115/215   train_loss = 2.929
Epoch 119 Batch  125/215   train_loss = 2.920
Epoch 119 Batch  135/215   train_loss = 3.000
Epoch 119 Batch  145/215   train_loss = 3.108
Epoch 119 Batch  155/215   train_loss = 3.172
Epoch 119 Batch  165/215   train_loss = 2.932
Epoch 119 Batch  175/215   train_loss = 3.328
Epoch 119 Batch  185/215   train_loss = 3.181
Epoch 119 Batch  195/215   train_loss = 3.010
Epoch 119 Batch  205/215   train_loss = 3.063
Epoch 120 Batch    0/215   train_loss = 3.262
Epoch 120 Batch   10/215   train_loss = 3.129
Epoch 120 Batch   20/215   train_loss = 3.157
Epoch 120 Batch   30/215   train_loss = 3.046
Epoch 120 Batch   40/215   train_loss = 3.059
Epoch 120 Batch   50/215   train_loss = 2.993
Epoch 120 Batch   60/215   train_loss = 3.214
Epoch 120 Batch   70/215   train_loss = 3.019
Epoch 120 Batch   80/215   train_loss = 3.069
Epoch 120 Batch   90/215   train_loss = 3.083
Epoch 120 Batch  100/215   train_loss = 3.147
Epoch 120 Batch  110/215   train_loss = 2.955
Epoch 120 Batch  120/215   train_loss = 2.998
Epoch 120 Batch  130/215   train_loss = 3.177
Epoch 120 Batch  140/215   train_loss = 2.997
Epoch 120 Batch  150/215   train_loss = 2.962
Epoch 120 Batch  160/215   train_loss = 3.135
Epoch 120 Batch  170/215   train_loss = 3.045
Epoch 120 Batch  180/215   train_loss = 3.061
Epoch 120 Batch  190/215   train_loss = 2.954
Epoch 120 Batch  200/215   train_loss = 3.068
Epoch 120 Batch  210/215   train_loss = 2.885
Epoch 121 Batch    5/215   train_loss = 3.062
Epoch 121 Batch   15/215   train_loss = 3.109
Epoch 121 Batch   25/215   train_loss = 2.978
Epoch 121 Batch   35/215   train_loss = 3.336
Epoch 121 Batch   45/215   train_loss = 2.953
Epoch 121 Batch   55/215   train_loss = 2.811
Epoch 121 Batch   65/215   train_loss = 3.230
Epoch 121 Batch   75/215   train_loss = 2.995
Epoch 121 Batch   85/215   train_loss = 3.117
Epoch 121 Batch   95/215   train_loss = 3.008
Epoch 121 Batch  105/215   train_loss = 2.988
Epoch 121 Batch  115/215   train_loss = 2.816
Epoch 121 Batch  125/215   train_loss = 2.878
Epoch 121 Batch  135/215   train_loss = 3.096
Epoch 121 Batch  145/215   train_loss = 2.976
Epoch 121 Batch  155/215   train_loss = 3.024
Epoch 121 Batch  165/215   train_loss = 3.050
Epoch 121 Batch  175/215   train_loss = 3.380
Epoch 121 Batch  185/215   train_loss = 3.087
Epoch 121 Batch  195/215   train_loss = 2.952
Epoch 121 Batch  205/215   train_loss = 3.063
Epoch 122 Batch    0/215   train_loss = 3.133
Epoch 122 Batch   10/215   train_loss = 3.560
Epoch 122 Batch   20/215   train_loss = 3.100
Epoch 122 Batch   30/215   train_loss = 3.038
Epoch 122 Batch   40/215   train_loss = 3.175
Epoch 122 Batch   50/215   train_loss = 2.978
Epoch 122 Batch   60/215   train_loss = 3.148
Epoch 122 Batch   70/215   train_loss = 3.023
Epoch 122 Batch   80/215   train_loss = 2.966
Epoch 122 Batch   90/215   train_loss = 3.215
Epoch 122 Batch  100/215   train_loss = 3.037
Epoch 122 Batch  110/215   train_loss = 3.083
Epoch 122 Batch  120/215   train_loss = 3.113
Epoch 122 Batch  130/215   train_loss = 3.280
Epoch 122 Batch  140/215   train_loss = 3.073
Epoch 122 Batch  150/215   train_loss = 3.012
Epoch 122 Batch  160/215   train_loss = 3.207
Epoch 122 Batch  170/215   train_loss = 3.150
Epoch 122 Batch  180/215   train_loss = 3.058
Epoch 122 Batch  190/215   train_loss = 3.095
Epoch 122 Batch  200/215   train_loss = 2.886
Epoch 122 Batch  210/215   train_loss = 2.994
Epoch 123 Batch    5/215   train_loss = 3.115
Epoch 123 Batch   15/215   train_loss = 3.005
Epoch 123 Batch   25/215   train_loss = 3.020
Epoch 123 Batch   35/215   train_loss = 3.177
Epoch 123 Batch   45/215   train_loss = 2.990
Epoch 123 Batch   55/215   train_loss = 2.845
Epoch 123 Batch   65/215   train_loss = 3.284
Epoch 123 Batch   75/215   train_loss = 3.213
Epoch 123 Batch   85/215   train_loss = 2.940
Epoch 123 Batch   95/215   train_loss = 3.056
Epoch 123 Batch  105/215   train_loss = 2.940
Epoch 123 Batch  115/215   train_loss = 2.802
Epoch 123 Batch  125/215   train_loss = 3.030
Epoch 123 Batch  135/215   train_loss = 3.070
Epoch 123 Batch  145/215   train_loss = 2.960
Epoch 123 Batch  155/215   train_loss = 3.263
Epoch 123 Batch  165/215   train_loss = 3.052
Epoch 123 Batch  175/215   train_loss = 3.382
Epoch 123 Batch  185/215   train_loss = 3.061
Epoch 123 Batch  195/215   train_loss = 2.885
Epoch 123 Batch  205/215   train_loss = 3.042
Epoch 124 Batch    0/215   train_loss = 3.165
Epoch 124 Batch   10/215   train_loss = 3.100
Epoch 124 Batch   20/215   train_loss = 3.020
Epoch 124 Batch   30/215   train_loss = 3.038
Epoch 124 Batch   40/215   train_loss = 3.158
Epoch 124 Batch   50/215   train_loss = 2.948
Epoch 124 Batch   60/215   train_loss = 3.024
Epoch 124 Batch   70/215   train_loss = 2.964
Epoch 124 Batch   80/215   train_loss = 3.237
Epoch 124 Batch   90/215   train_loss = 3.053
Epoch 124 Batch  100/215   train_loss = 3.157
Epoch 124 Batch  110/215   train_loss = 2.980
Epoch 124 Batch  120/215   train_loss = 3.102
Epoch 124 Batch  130/215   train_loss = 3.365
Epoch 124 Batch  140/215   train_loss = 3.066
Epoch 124 Batch  150/215   train_loss = 3.078
Epoch 124 Batch  160/215   train_loss = 3.109
Epoch 124 Batch  170/215   train_loss = 2.844
Epoch 124 Batch  180/215   train_loss = 3.026
Epoch 124 Batch  190/215   train_loss = 3.262
Epoch 124 Batch  200/215   train_loss = 2.948
Epoch 124 Batch  210/215   train_loss = 2.725
Epoch 125 Batch    5/215   train_loss = 3.134
Epoch 125 Batch   15/215   train_loss = 3.093
Epoch 125 Batch   25/215   train_loss = 2.989
Epoch 125 Batch   35/215   train_loss = 3.022
Epoch 125 Batch   45/215   train_loss = 3.130
Epoch 125 Batch   55/215   train_loss = 2.989
Epoch 125 Batch   65/215   train_loss = 3.106
Epoch 125 Batch   75/215   train_loss = 3.112
Epoch 125 Batch   85/215   train_loss = 2.927
Epoch 125 Batch   95/215   train_loss = 3.031
Epoch 125 Batch  105/215   train_loss = 2.941
Epoch 125 Batch  115/215   train_loss = 2.837
Epoch 125 Batch  125/215   train_loss = 2.865
Epoch 125 Batch  135/215   train_loss = 2.932
Epoch 125 Batch  145/215   train_loss = 3.046
Epoch 125 Batch  155/215   train_loss = 3.094
Epoch 125 Batch  165/215   train_loss = 3.007
Epoch 125 Batch  175/215   train_loss = 3.161
Epoch 125 Batch  185/215   train_loss = 3.239
Epoch 125 Batch  195/215   train_loss = 2.835
Epoch 125 Batch  205/215   train_loss = 3.194
Epoch 126 Batch    0/215   train_loss = 3.124
Epoch 126 Batch   10/215   train_loss = 3.178
Epoch 126 Batch   20/215   train_loss = 3.144
Epoch 126 Batch   30/215   train_loss = 3.132
Epoch 126 Batch   40/215   train_loss = 2.999
Epoch 126 Batch   50/215   train_loss = 3.135
Epoch 126 Batch   60/215   train_loss = 3.242
Epoch 126 Batch   70/215   train_loss = 3.050
Epoch 126 Batch   80/215   train_loss = 3.025
Epoch 126 Batch   90/215   train_loss = 3.258
Epoch 126 Batch  100/215   train_loss = 3.187
Epoch 126 Batch  110/215   train_loss = 3.101
Epoch 126 Batch  120/215   train_loss = 3.014
Epoch 126 Batch  130/215   train_loss = 3.161
Epoch 126 Batch  140/215   train_loss = 3.048
Epoch 126 Batch  150/215   train_loss = 3.024
Epoch 126 Batch  160/215   train_loss = 3.001
Epoch 126 Batch  170/215   train_loss = 3.163
Epoch 126 Batch  180/215   train_loss = 3.129
Epoch 126 Batch  190/215   train_loss = 3.090
Epoch 126 Batch  200/215   train_loss = 3.097
Epoch 126 Batch  210/215   train_loss = 2.928
Epoch 127 Batch    5/215   train_loss = 3.164
Epoch 127 Batch   15/215   train_loss = 3.098
Epoch 127 Batch   25/215   train_loss = 3.000
Epoch 127 Batch   35/215   train_loss = 2.998
Epoch 127 Batch   45/215   train_loss = 3.129
Epoch 127 Batch   55/215   train_loss = 2.964
Epoch 127 Batch   65/215   train_loss = 3.149
Epoch 127 Batch   75/215   train_loss = 2.918
Epoch 127 Batch   85/215   train_loss = 2.972
Epoch 127 Batch   95/215   train_loss = 2.995
Epoch 127 Batch  105/215   train_loss = 2.941
Epoch 127 Batch  115/215   train_loss = 2.793
Epoch 127 Batch  125/215   train_loss = 2.931
Epoch 127 Batch  135/215   train_loss = 2.884
Epoch 127 Batch  145/215   train_loss = 3.000
Epoch 127 Batch  155/215   train_loss = 3.259
Epoch 127 Batch  165/215   train_loss = 3.155
Epoch 127 Batch  175/215   train_loss = 3.245
Epoch 127 Batch  185/215   train_loss = 3.200
Epoch 127 Batch  195/215   train_loss = 3.020
Epoch 127 Batch  205/215   train_loss = 3.029
Epoch 128 Batch    0/215   train_loss = 3.144
Epoch 128 Batch   10/215   train_loss = 3.171
Epoch 128 Batch   20/215   train_loss = 3.152
Epoch 128 Batch   30/215   train_loss = 2.756
Epoch 128 Batch   40/215   train_loss = 3.039
Epoch 128 Batch   50/215   train_loss = 3.005
Epoch 128 Batch   60/215   train_loss = 3.192
Epoch 128 Batch   70/215   train_loss = 3.014
Epoch 128 Batch   80/215   train_loss = 2.947
Epoch 128 Batch   90/215   train_loss = 3.092
Epoch 128 Batch  100/215   train_loss = 3.265
Epoch 128 Batch  110/215   train_loss = 3.078
Epoch 128 Batch  120/215   train_loss = 3.063
Epoch 128 Batch  130/215   train_loss = 3.114
Epoch 128 Batch  140/215   train_loss = 3.013
Epoch 128 Batch  150/215   train_loss = 3.035
Epoch 128 Batch  160/215   train_loss = 3.057
Epoch 128 Batch  170/215   train_loss = 2.970
Epoch 128 Batch  180/215   train_loss = 3.059
Epoch 128 Batch  190/215   train_loss = 3.070
Epoch 128 Batch  200/215   train_loss = 2.842
Epoch 128 Batch  210/215   train_loss = 3.105
Epoch 129 Batch    5/215   train_loss = 3.224
Epoch 129 Batch   15/215   train_loss = 2.952
Epoch 129 Batch   25/215   train_loss = 3.089
Epoch 129 Batch   35/215   train_loss = 3.233
Epoch 129 Batch   45/215   train_loss = 3.043
Epoch 129 Batch   55/215   train_loss = 2.977
Epoch 129 Batch   65/215   train_loss = 3.020
Epoch 129 Batch   75/215   train_loss = 2.995
Epoch 129 Batch   85/215   train_loss = 3.015
Epoch 129 Batch   95/215   train_loss = 3.029
Epoch 129 Batch  105/215   train_loss = 3.164
Epoch 129 Batch  115/215   train_loss = 2.979
Epoch 129 Batch  125/215   train_loss = 2.928
Epoch 129 Batch  135/215   train_loss = 2.888
Epoch 129 Batch  145/215   train_loss = 3.047
Epoch 129 Batch  155/215   train_loss = 3.226
Epoch 129 Batch  165/215   train_loss = 2.998
Epoch 129 Batch  175/215   train_loss = 3.308
Epoch 129 Batch  185/215   train_loss = 3.276
Epoch 129 Batch  195/215   train_loss = 2.882
Epoch 129 Batch  205/215   train_loss = 3.091
Epoch 130 Batch    0/215   train_loss = 3.212
Epoch 130 Batch   10/215   train_loss = 3.061
Epoch 130 Batch   20/215   train_loss = 3.052
Epoch 130 Batch   30/215   train_loss = 2.966
Epoch 130 Batch   40/215   train_loss = 2.965
Epoch 130 Batch   50/215   train_loss = 2.972
Epoch 130 Batch   60/215   train_loss = 3.176
Epoch 130 Batch   70/215   train_loss = 2.955
Epoch 130 Batch   80/215   train_loss = 3.106
Epoch 130 Batch   90/215   train_loss = 3.137
Epoch 130 Batch  100/215   train_loss = 3.013
Epoch 130 Batch  110/215   train_loss = 3.050
Epoch 130 Batch  120/215   train_loss = 2.939
Epoch 130 Batch  130/215   train_loss = 3.183
Epoch 130 Batch  140/215   train_loss = 3.045
Epoch 130 Batch  150/215   train_loss = 3.096
Epoch 130 Batch  160/215   train_loss = 3.177
Epoch 130 Batch  170/215   train_loss = 2.914
Epoch 130 Batch  180/215   train_loss = 3.004
Epoch 130 Batch  190/215   train_loss = 3.024
Epoch 130 Batch  200/215   train_loss = 2.879
Epoch 130 Batch  210/215   train_loss = 2.911
Epoch 131 Batch    5/215   train_loss = 3.163
Epoch 131 Batch   15/215   train_loss = 2.909
Epoch 131 Batch   25/215   train_loss = 2.836
Epoch 131 Batch   35/215   train_loss = 3.103
Epoch 131 Batch   45/215   train_loss = 2.963
Epoch 131 Batch   55/215   train_loss = 2.887
Epoch 131 Batch   65/215   train_loss = 2.912
Epoch 131 Batch   75/215   train_loss = 2.977
Epoch 131 Batch   85/215   train_loss = 2.977
Epoch 131 Batch   95/215   train_loss = 3.125
Epoch 131 Batch  105/215   train_loss = 2.924
Epoch 131 Batch  115/215   train_loss = 3.004
Epoch 131 Batch  125/215   train_loss = 2.938
Epoch 131 Batch  135/215   train_loss = 2.895
Epoch 131 Batch  145/215   train_loss = 3.084
Epoch 131 Batch  155/215   train_loss = 3.119
Epoch 131 Batch  165/215   train_loss = 2.924
Epoch 131 Batch  175/215   train_loss = 3.067
Epoch 131 Batch  185/215   train_loss = 3.085
Epoch 131 Batch  195/215   train_loss = 2.850
Epoch 131 Batch  205/215   train_loss = 2.995
Epoch 132 Batch    0/215   train_loss = 3.104
Epoch 132 Batch   10/215   train_loss = 3.188
Epoch 132 Batch   20/215   train_loss = 2.991
Epoch 132 Batch   30/215   train_loss = 3.005
Epoch 132 Batch   40/215   train_loss = 3.003
Epoch 132 Batch   50/215   train_loss = 3.017
Epoch 132 Batch   60/215   train_loss = 3.252
Epoch 132 Batch   70/215   train_loss = 2.970
Epoch 132 Batch   80/215   train_loss = 2.968
Epoch 132 Batch   90/215   train_loss = 3.256
Epoch 132 Batch  100/215   train_loss = 2.997
Epoch 132 Batch  110/215   train_loss = 2.926
Epoch 132 Batch  120/215   train_loss = 2.939
Epoch 132 Batch  130/215   train_loss = 3.200
Epoch 132 Batch  140/215   train_loss = 2.995
Epoch 132 Batch  150/215   train_loss = 3.043
Epoch 132 Batch  160/215   train_loss = 2.948
Epoch 132 Batch  170/215   train_loss = 3.100
Epoch 132 Batch  180/215   train_loss = 3.246
Epoch 132 Batch  190/215   train_loss = 3.165
Epoch 132 Batch  200/215   train_loss = 2.871
Epoch 132 Batch  210/215   train_loss = 2.986
Epoch 133 Batch    5/215   train_loss = 3.225
Epoch 133 Batch   15/215   train_loss = 2.839
Epoch 133 Batch   25/215   train_loss = 3.086
Epoch 133 Batch   35/215   train_loss = 3.049
Epoch 133 Batch   45/215   train_loss = 2.939
Epoch 133 Batch   55/215   train_loss = 3.025
Epoch 133 Batch   65/215   train_loss = 2.893
Epoch 133 Batch   75/215   train_loss = 3.030
Epoch 133 Batch   85/215   train_loss = 3.068
Epoch 133 Batch   95/215   train_loss = 2.943
Epoch 133 Batch  105/215   train_loss = 3.127
Epoch 133 Batch  115/215   train_loss = 3.056
Epoch 133 Batch  125/215   train_loss = 2.986
Epoch 133 Batch  135/215   train_loss = 2.976
Epoch 133 Batch  145/215   train_loss = 2.951
Epoch 133 Batch  155/215   train_loss = 3.043
Epoch 133 Batch  165/215   train_loss = 3.003
Epoch 133 Batch  175/215   train_loss = 3.197
Epoch 133 Batch  185/215   train_loss = 3.130
Epoch 133 Batch  195/215   train_loss = 3.089
Epoch 133 Batch  205/215   train_loss = 3.094
Epoch 134 Batch    0/215   train_loss = 3.228
Epoch 134 Batch   10/215   train_loss = 3.259
Epoch 134 Batch   20/215   train_loss = 3.169
Epoch 134 Batch   30/215   train_loss = 2.885
Epoch 134 Batch   40/215   train_loss = 2.905
Epoch 134 Batch   50/215   train_loss = 2.945
Epoch 134 Batch   60/215   train_loss = 3.070
Epoch 134 Batch   70/215   train_loss = 2.966
Epoch 134 Batch   80/215   train_loss = 2.986
Epoch 134 Batch   90/215   train_loss = 3.240
Epoch 134 Batch  100/215   train_loss = 2.949
Epoch 134 Batch  110/215   train_loss = 3.018
Epoch 134 Batch  120/215   train_loss = 2.977
Epoch 134 Batch  130/215   train_loss = 3.385
Epoch 134 Batch  140/215   train_loss = 3.133
Epoch 134 Batch  150/215   train_loss = 2.809
Epoch 134 Batch  160/215   train_loss = 2.963
Epoch 134 Batch  170/215   train_loss = 2.946
Epoch 134 Batch  180/215   train_loss = 3.028
Epoch 134 Batch  190/215   train_loss = 3.067
Epoch 134 Batch  200/215   train_loss = 2.905
Epoch 134 Batch  210/215   train_loss = 2.827
Epoch 135 Batch    5/215   train_loss = 3.073
Epoch 135 Batch   15/215   train_loss = 2.886
Epoch 135 Batch   25/215   train_loss = 3.055
Epoch 135 Batch   35/215   train_loss = 3.070
Epoch 135 Batch   45/215   train_loss = 2.999
Epoch 135 Batch   55/215   train_loss = 3.059
Epoch 135 Batch   65/215   train_loss = 3.008
Epoch 135 Batch   75/215   train_loss = 3.085
Epoch 135 Batch   85/215   train_loss = 2.996
Epoch 135 Batch   95/215   train_loss = 2.922
Epoch 135 Batch  105/215   train_loss = 2.985
Epoch 135 Batch  115/215   train_loss = 2.861
Epoch 135 Batch  125/215   train_loss = 2.894
Epoch 135 Batch  135/215   train_loss = 2.915
Epoch 135 Batch  145/215   train_loss = 2.975
Epoch 135 Batch  155/215   train_loss = 3.090
Epoch 135 Batch  165/215   train_loss = 2.931
Epoch 135 Batch  175/215   train_loss = 3.240
Epoch 135 Batch  185/215   train_loss = 3.180
Epoch 135 Batch  195/215   train_loss = 3.021
Epoch 135 Batch  205/215   train_loss = 3.072
Epoch 136 Batch    0/215   train_loss = 3.052
Epoch 136 Batch   10/215   train_loss = 3.264
Epoch 136 Batch   20/215   train_loss = 3.013
Epoch 136 Batch   30/215   train_loss = 2.986
Epoch 136 Batch   40/215   train_loss = 2.999
Epoch 136 Batch   50/215   train_loss = 3.013
Epoch 136 Batch   60/215   train_loss = 3.130
Epoch 136 Batch   70/215   train_loss = 2.925
Epoch 136 Batch   80/215   train_loss = 3.028
Epoch 136 Batch   90/215   train_loss = 3.126
Epoch 136 Batch  100/215   train_loss = 3.031
Epoch 136 Batch  110/215   train_loss = 2.913
Epoch 136 Batch  120/215   train_loss = 2.952
Epoch 136 Batch  130/215   train_loss = 3.155
Epoch 136 Batch  140/215   train_loss = 3.155
Epoch 136 Batch  150/215   train_loss = 3.045
Epoch 136 Batch  160/215   train_loss = 3.056
Epoch 136 Batch  170/215   train_loss = 2.958
Epoch 136 Batch  180/215   train_loss = 3.160
Epoch 136 Batch  190/215   train_loss = 3.119
Epoch 136 Batch  200/215   train_loss = 3.047
Epoch 136 Batch  210/215   train_loss = 2.953
Epoch 137 Batch    5/215   train_loss = 3.095
Epoch 137 Batch   15/215   train_loss = 3.019
Epoch 137 Batch   25/215   train_loss = 3.049
Epoch 137 Batch   35/215   train_loss = 2.994
Epoch 137 Batch   45/215   train_loss = 2.968
Epoch 137 Batch   55/215   train_loss = 2.926
Epoch 137 Batch   65/215   train_loss = 2.960
Epoch 137 Batch   75/215   train_loss = 3.006
Epoch 137 Batch   85/215   train_loss = 2.941
Epoch 137 Batch   95/215   train_loss = 3.110
Epoch 137 Batch  105/215   train_loss = 2.908
Epoch 137 Batch  115/215   train_loss = 2.933
Epoch 137 Batch  125/215   train_loss = 2.941
Epoch 137 Batch  135/215   train_loss = 2.929
Epoch 137 Batch  145/215   train_loss = 2.882
Epoch 137 Batch  155/215   train_loss = 3.121
Epoch 137 Batch  165/215   train_loss = 2.998
Epoch 137 Batch  175/215   train_loss = 3.201
Epoch 137 Batch  185/215   train_loss = 2.981
Epoch 137 Batch  195/215   train_loss = 2.783
Epoch 137 Batch  205/215   train_loss = 3.089
Epoch 138 Batch    0/215   train_loss = 3.177
Epoch 138 Batch   10/215   train_loss = 3.091
Epoch 138 Batch   20/215   train_loss = 3.174
Epoch 138 Batch   30/215   train_loss = 2.889
Epoch 138 Batch   40/215   train_loss = 2.858
Epoch 138 Batch   50/215   train_loss = 3.055
Epoch 138 Batch   60/215   train_loss = 3.094
Epoch 138 Batch   70/215   train_loss = 2.928
Epoch 138 Batch   80/215   train_loss = 2.998
Epoch 138 Batch   90/215   train_loss = 3.010
Epoch 138 Batch  100/215   train_loss = 3.132
Epoch 138 Batch  110/215   train_loss = 3.007
Epoch 138 Batch  120/215   train_loss = 3.122
Epoch 138 Batch  130/215   train_loss = 3.214
Epoch 138 Batch  140/215   train_loss = 2.997
Epoch 138 Batch  150/215   train_loss = 2.943
Epoch 138 Batch  160/215   train_loss = 3.189
Epoch 138 Batch  170/215   train_loss = 2.892
Epoch 138 Batch  180/215   train_loss = 3.142
Epoch 138 Batch  190/215   train_loss = 3.019
Epoch 138 Batch  200/215   train_loss = 2.974
Epoch 138 Batch  210/215   train_loss = 2.885
Epoch 139 Batch    5/215   train_loss = 3.133
Epoch 139 Batch   15/215   train_loss = 3.005
Epoch 139 Batch   25/215   train_loss = 3.004
Epoch 139 Batch   35/215   train_loss = 2.979
Epoch 139 Batch   45/215   train_loss = 2.937
Epoch 139 Batch   55/215   train_loss = 2.904
Epoch 139 Batch   65/215   train_loss = 3.014
Epoch 139 Batch   75/215   train_loss = 3.004
Epoch 139 Batch   85/215   train_loss = 3.041
Epoch 139 Batch   95/215   train_loss = 3.055
Epoch 139 Batch  105/215   train_loss = 2.888
Epoch 139 Batch  115/215   train_loss = 3.027
Epoch 139 Batch  125/215   train_loss = 2.915
Epoch 139 Batch  135/215   train_loss = 3.104
Epoch 139 Batch  145/215   train_loss = 3.138
Epoch 139 Batch  155/215   train_loss = 3.116
Epoch 139 Batch  165/215   train_loss = 2.948
Epoch 139 Batch  175/215   train_loss = 3.190
Epoch 139 Batch  185/215   train_loss = 3.145
Epoch 139 Batch  195/215   train_loss = 2.960
Epoch 139 Batch  205/215   train_loss = 3.083
Epoch 140 Batch    0/215   train_loss = 3.034
Epoch 140 Batch   10/215   train_loss = 3.120
Epoch 140 Batch   20/215   train_loss = 2.997
Epoch 140 Batch   30/215   train_loss = 2.868
Epoch 140 Batch   40/215   train_loss = 3.042
Epoch 140 Batch   50/215   train_loss = 2.997
Epoch 140 Batch   60/215   train_loss = 3.025
Epoch 140 Batch   70/215   train_loss = 2.972
Epoch 140 Batch   80/215   train_loss = 3.048
Epoch 140 Batch   90/215   train_loss = 3.043
Epoch 140 Batch  100/215   train_loss = 3.002
Epoch 140 Batch  110/215   train_loss = 2.969
Epoch 140 Batch  120/215   train_loss = 3.124
Epoch 140 Batch  130/215   train_loss = 3.239
Epoch 140 Batch  140/215   train_loss = 3.005
Epoch 140 Batch  150/215   train_loss = 2.836
Epoch 140 Batch  160/215   train_loss = 2.899
Epoch 140 Batch  170/215   train_loss = 2.962
Epoch 140 Batch  180/215   train_loss = 3.015
Epoch 140 Batch  190/215   train_loss = 3.199
Epoch 140 Batch  200/215   train_loss = 2.874
Epoch 140 Batch  210/215   train_loss = 2.841
Epoch 141 Batch    5/215   train_loss = 3.170
Epoch 141 Batch   15/215   train_loss = 2.961
Epoch 141 Batch   25/215   train_loss = 2.973
Epoch 141 Batch   35/215   train_loss = 3.055
Epoch 141 Batch   45/215   train_loss = 3.062
Epoch 141 Batch   55/215   train_loss = 3.015
Epoch 141 Batch   65/215   train_loss = 3.183
Epoch 141 Batch   75/215   train_loss = 2.980
Epoch 141 Batch   85/215   train_loss = 2.922
Epoch 141 Batch   95/215   train_loss = 2.946
Epoch 141 Batch  105/215   train_loss = 2.988
Epoch 141 Batch  115/215   train_loss = 3.007
Epoch 141 Batch  125/215   train_loss = 2.919
Epoch 141 Batch  135/215   train_loss = 3.041
Epoch 141 Batch  145/215   train_loss = 3.061
Epoch 141 Batch  155/215   train_loss = 3.061
Epoch 141 Batch  165/215   train_loss = 2.909
Epoch 141 Batch  175/215   train_loss = 3.053
Epoch 141 Batch  185/215   train_loss = 2.884
Epoch 141 Batch  195/215   train_loss = 3.042
Epoch 141 Batch  205/215   train_loss = 2.869
Epoch 142 Batch    0/215   train_loss = 3.103
Epoch 142 Batch   10/215   train_loss = 3.127
Epoch 142 Batch   20/215   train_loss = 2.944
Epoch 142 Batch   30/215   train_loss = 2.945
Epoch 142 Batch   40/215   train_loss = 2.914
Epoch 142 Batch   50/215   train_loss = 3.024
Epoch 142 Batch   60/215   train_loss = 3.045
Epoch 142 Batch   70/215   train_loss = 2.893
Epoch 142 Batch   80/215   train_loss = 2.821
Epoch 142 Batch   90/215   train_loss = 2.962
Epoch 142 Batch  100/215   train_loss = 3.063
Epoch 142 Batch  110/215   train_loss = 2.921
Epoch 142 Batch  120/215   train_loss = 3.220
Epoch 142 Batch  130/215   train_loss = 3.386
Epoch 142 Batch  140/215   train_loss = 3.161
Epoch 142 Batch  150/215   train_loss = 3.133
Epoch 142 Batch  160/215   train_loss = 2.876
Epoch 142 Batch  170/215   train_loss = 2.993
Epoch 142 Batch  180/215   train_loss = 3.036
Epoch 142 Batch  190/215   train_loss = 3.207
Epoch 142 Batch  200/215   train_loss = 2.888
Epoch 142 Batch  210/215   train_loss = 2.832
Epoch 143 Batch    5/215   train_loss = 3.023
Epoch 143 Batch   15/215   train_loss = 3.020
Epoch 143 Batch   25/215   train_loss = 3.018
Epoch 143 Batch   35/215   train_loss = 3.031
Epoch 143 Batch   45/215   train_loss = 3.020
Epoch 143 Batch   55/215   train_loss = 2.998
Epoch 143 Batch   65/215   train_loss = 2.886
Epoch 143 Batch   75/215   train_loss = 2.988
Epoch 143 Batch   85/215   train_loss = 2.893
Epoch 143 Batch   95/215   train_loss = 2.815
Epoch 143 Batch  105/215   train_loss = 2.977
Epoch 143 Batch  115/215   train_loss = 2.884
Epoch 143 Batch  125/215   train_loss = 2.943
Epoch 143 Batch  135/215   train_loss = 2.905
Epoch 143 Batch  145/215   train_loss = 3.009
Epoch 143 Batch  155/215   train_loss = 3.084
Epoch 143 Batch  165/215   train_loss = 3.095
Epoch 143 Batch  175/215   train_loss = 3.238
Epoch 143 Batch  185/215   train_loss = 2.985
Epoch 143 Batch  195/215   train_loss = 2.752
Epoch 143 Batch  205/215   train_loss = 2.917
Epoch 144 Batch    0/215   train_loss = 3.021
Epoch 144 Batch   10/215   train_loss = 3.094
Epoch 144 Batch   20/215   train_loss = 2.955
Epoch 144 Batch   30/215   train_loss = 2.912
Epoch 144 Batch   40/215   train_loss = 3.095
Epoch 144 Batch   50/215   train_loss = 2.961
Epoch 144 Batch   60/215   train_loss = 3.217
Epoch 144 Batch   70/215   train_loss = 2.996
Epoch 144 Batch   80/215   train_loss = 3.036
Epoch 144 Batch   90/215   train_loss = 3.023
Epoch 144 Batch  100/215   train_loss = 3.200
Epoch 144 Batch  110/215   train_loss = 2.952
Epoch 144 Batch  120/215   train_loss = 2.901
Epoch 144 Batch  130/215   train_loss = 3.166
Epoch 144 Batch  140/215   train_loss = 3.233
Epoch 144 Batch  150/215   train_loss = 2.922
Epoch 144 Batch  160/215   train_loss = 3.224
Epoch 144 Batch  170/215   train_loss = 2.951
Epoch 144 Batch  180/215   train_loss = 2.892
Epoch 144 Batch  190/215   train_loss = 3.161
Epoch 144 Batch  200/215   train_loss = 2.911
Epoch 144 Batch  210/215   train_loss = 2.905
Epoch 145 Batch    5/215   train_loss = 3.148
Epoch 145 Batch   15/215   train_loss = 2.972
Epoch 145 Batch   25/215   train_loss = 3.064
Epoch 145 Batch   35/215   train_loss = 3.089
Epoch 145 Batch   45/215   train_loss = 2.936
Epoch 145 Batch   55/215   train_loss = 2.858
Epoch 145 Batch   65/215   train_loss = 3.190
Epoch 145 Batch   75/215   train_loss = 2.895
Epoch 145 Batch   85/215   train_loss = 2.984
Epoch 145 Batch   95/215   train_loss = 2.939
Epoch 145 Batch  105/215   train_loss = 3.058
Epoch 145 Batch  115/215   train_loss = 2.783
Epoch 145 Batch  125/215   train_loss = 2.913
Epoch 145 Batch  135/215   train_loss = 3.072
Epoch 145 Batch  145/215   train_loss = 3.095
Epoch 145 Batch  155/215   train_loss = 3.096
Epoch 145 Batch  165/215   train_loss = 3.057
Epoch 145 Batch  175/215   train_loss = 3.183
Epoch 145 Batch  185/215   train_loss = 3.019
Epoch 145 Batch  195/215   train_loss = 2.760
Epoch 145 Batch  205/215   train_loss = 3.008
Epoch 146 Batch    0/215   train_loss = 3.125
Epoch 146 Batch   10/215   train_loss = 3.120
Epoch 146 Batch   20/215   train_loss = 2.987
Epoch 146 Batch   30/215   train_loss = 2.834
Epoch 146 Batch   40/215   train_loss = 2.723
Epoch 146 Batch   50/215   train_loss = 3.016
Epoch 146 Batch   60/215   train_loss = 3.161
Epoch 146 Batch   70/215   train_loss = 2.811
Epoch 146 Batch   80/215   train_loss = 2.952
Epoch 146 Batch   90/215   train_loss = 2.972
Epoch 146 Batch  100/215   train_loss = 3.024
Epoch 146 Batch  110/215   train_loss = 2.871
Epoch 146 Batch  120/215   train_loss = 3.166
Epoch 146 Batch  130/215   train_loss = 3.172
Epoch 146 Batch  140/215   train_loss = 2.982
Epoch 146 Batch  150/215   train_loss = 2.856
Epoch 146 Batch  160/215   train_loss = 2.968
Epoch 146 Batch  170/215   train_loss = 2.980
Epoch 146 Batch  180/215   train_loss = 3.029
Epoch 146 Batch  190/215   train_loss = 3.047
Epoch 146 Batch  200/215   train_loss = 2.962
Epoch 146 Batch  210/215   train_loss = 2.836
Epoch 147 Batch    5/215   train_loss = 3.053
Epoch 147 Batch   15/215   train_loss = 2.881
Epoch 147 Batch   25/215   train_loss = 2.908
Epoch 147 Batch   35/215   train_loss = 3.069
Epoch 147 Batch   45/215   train_loss = 2.881
Epoch 147 Batch   55/215   train_loss = 2.977
Epoch 147 Batch   65/215   train_loss = 3.185
Epoch 147 Batch   75/215   train_loss = 2.969
Epoch 147 Batch   85/215   train_loss = 2.850
Epoch 147 Batch   95/215   train_loss = 2.968
Epoch 147 Batch  105/215   train_loss = 2.986
Epoch 147 Batch  115/215   train_loss = 2.785
Epoch 147 Batch  125/215   train_loss = 2.897
Epoch 147 Batch  135/215   train_loss = 2.899
Epoch 147 Batch  145/215   train_loss = 3.043
Epoch 147 Batch  155/215   train_loss = 3.002
Epoch 147 Batch  165/215   train_loss = 3.029
Epoch 147 Batch  175/215   train_loss = 3.200
Epoch 147 Batch  185/215   train_loss = 3.004
Epoch 147 Batch  195/215   train_loss = 2.904
Epoch 147 Batch  205/215   train_loss = 3.049
Epoch 148 Batch    0/215   train_loss = 3.014
Epoch 148 Batch   10/215   train_loss = 2.958
Epoch 148 Batch   20/215   train_loss = 3.042
Epoch 148 Batch   30/215   train_loss = 2.858
Epoch 148 Batch   40/215   train_loss = 3.047
Epoch 148 Batch   50/215   train_loss = 2.794
Epoch 148 Batch   60/215   train_loss = 3.117
Epoch 148 Batch   70/215   train_loss = 2.851
Epoch 148 Batch   80/215   train_loss = 2.951
Epoch 148 Batch   90/215   train_loss = 3.079
Epoch 148 Batch  100/215   train_loss = 2.965
Epoch 148 Batch  110/215   train_loss = 2.885
Epoch 148 Batch  120/215   train_loss = 3.054
Epoch 148 Batch  130/215   train_loss = 3.104
Epoch 148 Batch  140/215   train_loss = 3.228
Epoch 148 Batch  150/215   train_loss = 2.890
Epoch 148 Batch  160/215   train_loss = 2.977
Epoch 148 Batch  170/215   train_loss = 2.975
Epoch 148 Batch  180/215   train_loss = 2.967
Epoch 148 Batch  190/215   train_loss = 2.987
Epoch 148 Batch  200/215   train_loss = 2.939
Epoch 148 Batch  210/215   train_loss = 2.752
Epoch 149 Batch    5/215   train_loss = 3.187
Epoch 149 Batch   15/215   train_loss = 3.083
Epoch 149 Batch   25/215   train_loss = 2.952
Epoch 149 Batch   35/215   train_loss = 3.144
Epoch 149 Batch   45/215   train_loss = 2.941
Epoch 149 Batch   55/215   train_loss = 2.992
Epoch 149 Batch   65/215   train_loss = 3.205
Epoch 149 Batch   75/215   train_loss = 3.087
Epoch 149 Batch   85/215   train_loss = 2.935
Epoch 149 Batch   95/215   train_loss = 2.809
Epoch 149 Batch  105/215   train_loss = 2.753
Epoch 149 Batch  115/215   train_loss = 2.992
Epoch 149 Batch  125/215   train_loss = 2.791
Epoch 149 Batch  135/215   train_loss = 3.010
Epoch 149 Batch  145/215   train_loss = 3.062
Epoch 149 Batch  155/215   train_loss = 3.110
Epoch 149 Batch  165/215   train_loss = 2.975
Epoch 149 Batch  175/215   train_loss = 3.060
Epoch 149 Batch  185/215   train_loss = 3.109
Epoch 149 Batch  195/215   train_loss = 2.839
Epoch 149 Batch  205/215   train_loss = 2.885
Epoch 150 Batch    0/215   train_loss = 2.927
Epoch 150 Batch   10/215   train_loss = 3.074
Epoch 150 Batch   20/215   train_loss = 2.971
Epoch 150 Batch   30/215   train_loss = 2.945
Epoch 150 Batch   40/215   train_loss = 2.919
Epoch 150 Batch   50/215   train_loss = 2.942
Epoch 150 Batch   60/215   train_loss = 3.027
Epoch 150 Batch   70/215   train_loss = 2.944
Epoch 150 Batch   80/215   train_loss = 2.996
Epoch 150 Batch   90/215   train_loss = 3.076
Epoch 150 Batch  100/215   train_loss = 2.883
Epoch 150 Batch  110/215   train_loss = 2.865
Epoch 150 Batch  120/215   train_loss = 3.023
Epoch 150 Batch  130/215   train_loss = 3.112
Epoch 150 Batch  140/215   train_loss = 3.171
Epoch 150 Batch  150/215   train_loss = 2.928
Epoch 150 Batch  160/215   train_loss = 2.986
Epoch 150 Batch  170/215   train_loss = 2.883
Epoch 150 Batch  180/215   train_loss = 2.995
Epoch 150 Batch  190/215   train_loss = 3.031
Epoch 150 Batch  200/215   train_loss = 2.950
Epoch 150 Batch  210/215   train_loss = 2.916
Epoch 151 Batch    5/215   train_loss = 2.944
Epoch 151 Batch   15/215   train_loss = 2.918
Epoch 151 Batch   25/215   train_loss = 2.930
Epoch 151 Batch   35/215   train_loss = 3.126
Epoch 151 Batch   45/215   train_loss = 3.259
Epoch 151 Batch   55/215   train_loss = 2.834
Epoch 151 Batch   65/215   train_loss = 2.997
Epoch 151 Batch   75/215   train_loss = 2.990
Epoch 151 Batch   85/215   train_loss = 3.097
Epoch 151 Batch   95/215   train_loss = 2.865
Epoch 151 Batch  105/215   train_loss = 2.887
Epoch 151 Batch  115/215   train_loss = 2.799
Epoch 151 Batch  125/215   train_loss = 2.904
Epoch 151 Batch  135/215   train_loss = 3.004
Epoch 151 Batch  145/215   train_loss = 2.831
Epoch 151 Batch  155/215   train_loss = 3.177
Epoch 151 Batch  165/215   train_loss = 2.975
Epoch 151 Batch  175/215   train_loss = 3.304
Epoch 151 Batch  185/215   train_loss = 2.884
Epoch 151 Batch  195/215   train_loss = 2.766
Epoch 151 Batch  205/215   train_loss = 2.930
Epoch 152 Batch    0/215   train_loss = 2.955
Epoch 152 Batch   10/215   train_loss = 2.984
Epoch 152 Batch   20/215   train_loss = 2.956
Epoch 152 Batch   30/215   train_loss = 2.806
Epoch 152 Batch   40/215   train_loss = 2.996
Epoch 152 Batch   50/215   train_loss = 2.698
Epoch 152 Batch   60/215   train_loss = 2.986
Epoch 152 Batch   70/215   train_loss = 2.806
Epoch 152 Batch   80/215   train_loss = 2.996
Epoch 152 Batch   90/215   train_loss = 3.036
Epoch 152 Batch  100/215   train_loss = 2.972
Epoch 152 Batch  110/215   train_loss = 2.906
Epoch 152 Batch  120/215   train_loss = 2.978
Epoch 152 Batch  130/215   train_loss = 3.075
Epoch 152 Batch  140/215   train_loss = 3.130
Epoch 152 Batch  150/215   train_loss = 2.875
Epoch 152 Batch  160/215   train_loss = 3.142
Epoch 152 Batch  170/215   train_loss = 2.989
Epoch 152 Batch  180/215   train_loss = 2.983
Epoch 152 Batch  190/215   train_loss = 3.079
Epoch 152 Batch  200/215   train_loss = 2.929
Epoch 152 Batch  210/215   train_loss = 2.915
Epoch 153 Batch    5/215   train_loss = 3.102
Epoch 153 Batch   15/215   train_loss = 2.795
Epoch 153 Batch   25/215   train_loss = 3.093
Epoch 153 Batch   35/215   train_loss = 3.225
Epoch 153 Batch   45/215   train_loss = 2.867
Epoch 153 Batch   55/215   train_loss = 2.845
Epoch 153 Batch   65/215   train_loss = 3.019
Epoch 153 Batch   75/215   train_loss = 2.959
Epoch 153 Batch   85/215   train_loss = 2.817
Epoch 153 Batch   95/215   train_loss = 2.869
Epoch 153 Batch  105/215   train_loss = 2.824
Epoch 153 Batch  115/215   train_loss = 2.749
Epoch 153 Batch  125/215   train_loss = 3.042
Epoch 153 Batch  135/215   train_loss = 3.025
Epoch 153 Batch  145/215   train_loss = 3.052
Epoch 153 Batch  155/215   train_loss = 3.128
Epoch 153 Batch  165/215   train_loss = 2.752
Epoch 153 Batch  175/215   train_loss = 3.344
Epoch 153 Batch  185/215   train_loss = 3.047
Epoch 153 Batch  195/215   train_loss = 2.699
Epoch 153 Batch  205/215   train_loss = 2.841
Epoch 154 Batch    0/215   train_loss = 3.040
Epoch 154 Batch   10/215   train_loss = 2.950
Epoch 154 Batch   20/215   train_loss = 2.930
Epoch 154 Batch   30/215   train_loss = 2.733
Epoch 154 Batch   40/215   train_loss = 2.878
Epoch 154 Batch   50/215   train_loss = 2.897
Epoch 154 Batch   60/215   train_loss = 3.089
Epoch 154 Batch   70/215   train_loss = 2.839
Epoch 154 Batch   80/215   train_loss = 2.903
Epoch 154 Batch   90/215   train_loss = 3.070
Epoch 154 Batch  100/215   train_loss = 3.040
Epoch 154 Batch  110/215   train_loss = 2.963
Epoch 154 Batch  120/215   train_loss = 3.045
Epoch 154 Batch  130/215   train_loss = 3.123
Epoch 154 Batch  140/215   train_loss = 2.905
Epoch 154 Batch  150/215   train_loss = 2.956
Epoch 154 Batch  160/215   train_loss = 3.201
Epoch 154 Batch  170/215   train_loss = 2.924
Epoch 154 Batch  180/215   train_loss = 2.838
Epoch 154 Batch  190/215   train_loss = 3.127
Epoch 154 Batch  200/215   train_loss = 2.880
Epoch 154 Batch  210/215   train_loss = 2.795
Epoch 155 Batch    5/215   train_loss = 2.989
Epoch 155 Batch   15/215   train_loss = 2.866
Epoch 155 Batch   25/215   train_loss = 2.942
Epoch 155 Batch   35/215   train_loss = 3.209
Epoch 155 Batch   45/215   train_loss = 2.896
Epoch 155 Batch   55/215   train_loss = 3.107
Epoch 155 Batch   65/215   train_loss = 3.248
Epoch 155 Batch   75/215   train_loss = 2.914
Epoch 155 Batch   85/215   train_loss = 2.886
Epoch 155 Batch   95/215   train_loss = 2.954
Epoch 155 Batch  105/215   train_loss = 2.806
Epoch 155 Batch  115/215   train_loss = 2.690
Epoch 155 Batch  125/215   train_loss = 2.925
Epoch 155 Batch  135/215   train_loss = 2.888
Epoch 155 Batch  145/215   train_loss = 3.019
Epoch 155 Batch  155/215   train_loss = 3.125
Epoch 155 Batch  165/215   train_loss = 2.884
Epoch 155 Batch  175/215   train_loss = 3.093
Epoch 155 Batch  185/215   train_loss = 2.824
Epoch 155 Batch  195/215   train_loss = 2.796
Epoch 155 Batch  205/215   train_loss = 2.945
Epoch 156 Batch    0/215   train_loss = 3.062
Epoch 156 Batch   10/215   train_loss = 2.915
Epoch 156 Batch   20/215   train_loss = 2.997
Epoch 156 Batch   30/215   train_loss = 2.968
Epoch 156 Batch   40/215   train_loss = 3.007
Epoch 156 Batch   50/215   train_loss = 2.757
Epoch 156 Batch   60/215   train_loss = 3.083
Epoch 156 Batch   70/215   train_loss = 2.839
Epoch 156 Batch   80/215   train_loss = 2.992
Epoch 156 Batch   90/215   train_loss = 3.156
Epoch 156 Batch  100/215   train_loss = 2.967
Epoch 156 Batch  110/215   train_loss = 2.805
Epoch 156 Batch  120/215   train_loss = 2.984
Epoch 156 Batch  130/215   train_loss = 3.155
Epoch 156 Batch  140/215   train_loss = 3.122
Epoch 156 Batch  150/215   train_loss = 2.896
Epoch 156 Batch  160/215   train_loss = 2.798
Epoch 156 Batch  170/215   train_loss = 2.843
Epoch 156 Batch  180/215   train_loss = 3.016
Epoch 156 Batch  190/215   train_loss = 3.090
Epoch 156 Batch  200/215   train_loss = 2.816
Epoch 156 Batch  210/215   train_loss = 2.729
Epoch 157 Batch    5/215   train_loss = 3.014
Epoch 157 Batch   15/215   train_loss = 2.927
Epoch 157 Batch   25/215   train_loss = 2.927
Epoch 157 Batch   35/215   train_loss = 2.946
Epoch 157 Batch   45/215   train_loss = 2.903
Epoch 157 Batch   55/215   train_loss = 2.854
Epoch 157 Batch   65/215   train_loss = 3.200
Epoch 157 Batch   75/215   train_loss = 2.855
Epoch 157 Batch   85/215   train_loss = 2.920
Epoch 157 Batch   95/215   train_loss = 3.023
Epoch 157 Batch  105/215   train_loss = 2.746
Epoch 157 Batch  115/215   train_loss = 2.842
Epoch 157 Batch  125/215   train_loss = 2.706
Epoch 157 Batch  135/215   train_loss = 2.887
Epoch 157 Batch  145/215   train_loss = 2.877
Epoch 157 Batch  155/215   train_loss = 2.953
Epoch 157 Batch  165/215   train_loss = 3.108
Epoch 157 Batch  175/215   train_loss = 3.017
Epoch 157 Batch  185/215   train_loss = 3.028
Epoch 157 Batch  195/215   train_loss = 2.878
Epoch 157 Batch  205/215   train_loss = 2.908
Epoch 158 Batch    0/215   train_loss = 2.934
Epoch 158 Batch   10/215   train_loss = 3.075
Epoch 158 Batch   20/215   train_loss = 2.966
Epoch 158 Batch   30/215   train_loss = 2.888
Epoch 158 Batch   40/215   train_loss = 2.805
Epoch 158 Batch   50/215   train_loss = 2.992
Epoch 158 Batch   60/215   train_loss = 3.173
Epoch 158 Batch   70/215   train_loss = 2.766
Epoch 158 Batch   80/215   train_loss = 3.026
Epoch 158 Batch   90/215   train_loss = 3.149
Epoch 158 Batch  100/215   train_loss = 2.883
Epoch 158 Batch  110/215   train_loss = 2.885
Epoch 158 Batch  120/215   train_loss = 3.020
Epoch 158 Batch  130/215   train_loss = 3.101
Epoch 158 Batch  140/215   train_loss = 3.095
Epoch 158 Batch  150/215   train_loss = 3.005
Epoch 158 Batch  160/215   train_loss = 2.845
Epoch 158 Batch  170/215   train_loss = 2.850
Epoch 158 Batch  180/215   train_loss = 3.158
Epoch 158 Batch  190/215   train_loss = 2.944
Epoch 158 Batch  200/215   train_loss = 2.859
Epoch 158 Batch  210/215   train_loss = 2.832
Epoch 159 Batch    5/215   train_loss = 2.993
Epoch 159 Batch   15/215   train_loss = 2.957
Epoch 159 Batch   25/215   train_loss = 2.729
Epoch 159 Batch   35/215   train_loss = 3.041
Epoch 159 Batch   45/215   train_loss = 2.997
Epoch 159 Batch   55/215   train_loss = 2.939
Epoch 159 Batch   65/215   train_loss = 2.799
Epoch 159 Batch   75/215   train_loss = 2.943
Epoch 159 Batch   85/215   train_loss = 2.700
Epoch 159 Batch   95/215   train_loss = 2.926
Epoch 159 Batch  105/215   train_loss = 2.837
Epoch 159 Batch  115/215   train_loss = 2.933
Epoch 159 Batch  125/215   train_loss = 2.741
Epoch 159 Batch  135/215   train_loss = 2.882
Epoch 159 Batch  145/215   train_loss = 2.791
Epoch 159 Batch  155/215   train_loss = 3.081
Epoch 159 Batch  165/215   train_loss = 2.909
Epoch 159 Batch  175/215   train_loss = 3.125
Epoch 159 Batch  185/215   train_loss = 3.040
Epoch 159 Batch  195/215   train_loss = 2.800
Epoch 159 Batch  205/215   train_loss = 2.897
Epoch 160 Batch    0/215   train_loss = 3.228
Epoch 160 Batch   10/215   train_loss = 3.097
Epoch 160 Batch   20/215   train_loss = 2.968
Epoch 160 Batch   30/215   train_loss = 2.774
Epoch 160 Batch   40/215   train_loss = 2.917
Epoch 160 Batch   50/215   train_loss = 2.835
Epoch 160 Batch   60/215   train_loss = 3.095
Epoch 160 Batch   70/215   train_loss = 2.925
Epoch 160 Batch   80/215   train_loss = 2.980
Epoch 160 Batch   90/215   train_loss = 2.955
Epoch 160 Batch  100/215   train_loss = 3.016
Epoch 160 Batch  110/215   train_loss = 2.857
Epoch 160 Batch  120/215   train_loss = 2.988
Epoch 160 Batch  130/215   train_loss = 3.171
Epoch 160 Batch  140/215   train_loss = 3.005
Epoch 160 Batch  150/215   train_loss = 3.069
Epoch 160 Batch  160/215   train_loss = 3.094
Epoch 160 Batch  170/215   train_loss = 2.912
Epoch 160 Batch  180/215   train_loss = 2.933
Epoch 160 Batch  190/215   train_loss = 3.043
Epoch 160 Batch  200/215   train_loss = 2.876
Epoch 160 Batch  210/215   train_loss = 2.760
Epoch 161 Batch    5/215   train_loss = 3.022
Epoch 161 Batch   15/215   train_loss = 2.848
Epoch 161 Batch   25/215   train_loss = 2.910
Epoch 161 Batch   35/215   train_loss = 3.282
Epoch 161 Batch   45/215   train_loss = 3.133
Epoch 161 Batch   55/215   train_loss = 2.814
Epoch 161 Batch   65/215   train_loss = 3.044
Epoch 161 Batch   75/215   train_loss = 2.894
Epoch 161 Batch   85/215   train_loss = 2.905
Epoch 161 Batch   95/215   train_loss = 2.818
Epoch 161 Batch  105/215   train_loss = 2.792
Epoch 161 Batch  115/215   train_loss = 2.816
Epoch 161 Batch  125/215   train_loss = 2.899
Epoch 161 Batch  135/215   train_loss = 2.928
Epoch 161 Batch  145/215   train_loss = 2.974
Epoch 161 Batch  155/215   train_loss = 3.067
Epoch 161 Batch  165/215   train_loss = 3.079
Epoch 161 Batch  175/215   train_loss = 3.177
Epoch 161 Batch  185/215   train_loss = 3.013
Epoch 161 Batch  195/215   train_loss = 3.003
Epoch 161 Batch  205/215   train_loss = 2.883
Epoch 162 Batch    0/215   train_loss = 2.947
Epoch 162 Batch   10/215   train_loss = 3.284
Epoch 162 Batch   20/215   train_loss = 2.843
Epoch 162 Batch   30/215   train_loss = 2.965
Epoch 162 Batch   40/215   train_loss = 2.867
Epoch 162 Batch   50/215   train_loss = 2.786
Epoch 162 Batch   60/215   train_loss = 2.945
Epoch 162 Batch   70/215   train_loss = 2.697
Epoch 162 Batch   80/215   train_loss = 2.823
Epoch 162 Batch   90/215   train_loss = 2.928
Epoch 162 Batch  100/215   train_loss = 2.978
Epoch 162 Batch  110/215   train_loss = 3.080
Epoch 162 Batch  120/215   train_loss = 3.068
Epoch 162 Batch  130/215   train_loss = 3.143
Epoch 162 Batch  140/215   train_loss = 2.983
Epoch 162 Batch  150/215   train_loss = 2.975
Epoch 162 Batch  160/215   train_loss = 2.922
Epoch 162 Batch  170/215   train_loss = 3.022
Epoch 162 Batch  180/215   train_loss = 3.126
Epoch 162 Batch  190/215   train_loss = 2.936
Epoch 162 Batch  200/215   train_loss = 3.052
Epoch 162 Batch  210/215   train_loss = 2.878
Epoch 163 Batch    5/215   train_loss = 2.906
Epoch 163 Batch   15/215   train_loss = 2.981
Epoch 163 Batch   25/215   train_loss = 2.923
Epoch 163 Batch   35/215   train_loss = 3.030
Epoch 163 Batch   45/215   train_loss = 2.860
Epoch 163 Batch   55/215   train_loss = 3.020
Epoch 163 Batch   65/215   train_loss = 3.017
Epoch 163 Batch   75/215   train_loss = 2.929
Epoch 163 Batch   85/215   train_loss = 2.779
Epoch 163 Batch   95/215   train_loss = 2.926
Epoch 163 Batch  105/215   train_loss = 2.950
Epoch 163 Batch  115/215   train_loss = 3.004
Epoch 163 Batch  125/215   train_loss = 2.836
Epoch 163 Batch  135/215   train_loss = 2.947
Epoch 163 Batch  145/215   train_loss = 2.892
Epoch 163 Batch  155/215   train_loss = 3.007
Epoch 163 Batch  165/215   train_loss = 2.947
Epoch 163 Batch  175/215   train_loss = 3.211
Epoch 163 Batch  185/215   train_loss = 3.070
Epoch 163 Batch  195/215   train_loss = 2.806
Epoch 163 Batch  205/215   train_loss = 2.939
Epoch 164 Batch    0/215   train_loss = 3.030
Epoch 164 Batch   10/215   train_loss = 2.959
Epoch 164 Batch   20/215   train_loss = 2.862
Epoch 164 Batch   30/215   train_loss = 2.852
Epoch 164 Batch   40/215   train_loss = 2.853
Epoch 164 Batch   50/215   train_loss = 2.890
Epoch 164 Batch   60/215   train_loss = 2.963
Epoch 164 Batch   70/215   train_loss = 2.752
Epoch 164 Batch   80/215   train_loss = 2.964
Epoch 164 Batch   90/215   train_loss = 3.068
Epoch 164 Batch  100/215   train_loss = 3.078
Epoch 164 Batch  110/215   train_loss = 3.025
Epoch 164 Batch  120/215   train_loss = 2.931
Epoch 164 Batch  130/215   train_loss = 3.058
Epoch 164 Batch  140/215   train_loss = 2.987
Epoch 164 Batch  150/215   train_loss = 2.932
Epoch 164 Batch  160/215   train_loss = 2.932
Epoch 164 Batch  170/215   train_loss = 2.944
Epoch 164 Batch  180/215   train_loss = 2.733
Epoch 164 Batch  190/215   train_loss = 3.000
Epoch 164 Batch  200/215   train_loss = 2.916
Epoch 164 Batch  210/215   train_loss = 2.855
Epoch 165 Batch    5/215   train_loss = 2.935
Epoch 165 Batch   15/215   train_loss = 2.954
Epoch 165 Batch   25/215   train_loss = 2.820
Epoch 165 Batch   35/215   train_loss = 3.019
Epoch 165 Batch   45/215   train_loss = 2.991
Epoch 165 Batch   55/215   train_loss = 2.878
Epoch 165 Batch   65/215   train_loss = 3.113
Epoch 165 Batch   75/215   train_loss = 2.859
Epoch 165 Batch   85/215   train_loss = 2.752
Epoch 165 Batch   95/215   train_loss = 2.995
Epoch 165 Batch  105/215   train_loss = 2.827
Epoch 165 Batch  115/215   train_loss = 2.684
Epoch 165 Batch  125/215   train_loss = 2.849
Epoch 165 Batch  135/215   train_loss = 2.934
Epoch 165 Batch  145/215   train_loss = 2.912
Epoch 165 Batch  155/215   train_loss = 3.138
Epoch 165 Batch  165/215   train_loss = 2.950
Epoch 165 Batch  175/215   train_loss = 3.104
Epoch 165 Batch  185/215   train_loss = 2.989
Epoch 165 Batch  195/215   train_loss = 2.847
Epoch 165 Batch  205/215   train_loss = 2.830
Epoch 166 Batch    0/215   train_loss = 3.003
Epoch 166 Batch   10/215   train_loss = 3.023
Epoch 166 Batch   20/215   train_loss = 2.922
Epoch 166 Batch   30/215   train_loss = 2.920
Epoch 166 Batch   40/215   train_loss = 2.885
Epoch 166 Batch   50/215   train_loss = 2.980
Epoch 166 Batch   60/215   train_loss = 2.892
Epoch 166 Batch   70/215   train_loss = 2.723
Epoch 166 Batch   80/215   train_loss = 2.921
Epoch 166 Batch   90/215   train_loss = 3.079
Epoch 166 Batch  100/215   train_loss = 3.035
Epoch 166 Batch  110/215   train_loss = 3.024
Epoch 166 Batch  120/215   train_loss = 3.096
Epoch 166 Batch  130/215   train_loss = 3.082
Epoch 166 Batch  140/215   train_loss = 2.930
Epoch 166 Batch  150/215   train_loss = 2.978
Epoch 166 Batch  160/215   train_loss = 2.965
Epoch 166 Batch  170/215   train_loss = 3.001
Epoch 166 Batch  180/215   train_loss = 2.943
Epoch 166 Batch  190/215   train_loss = 2.885
Epoch 166 Batch  200/215   train_loss = 2.757
Epoch 166 Batch  210/215   train_loss = 2.741
Epoch 167 Batch    5/215   train_loss = 2.969
Epoch 167 Batch   15/215   train_loss = 2.821
Epoch 167 Batch   25/215   train_loss = 2.833
Epoch 167 Batch   35/215   train_loss = 3.025
Epoch 167 Batch   45/215   train_loss = 2.842
Epoch 167 Batch   55/215   train_loss = 2.885
Epoch 167 Batch   65/215   train_loss = 2.958
Epoch 167 Batch   75/215   train_loss = 2.829
Epoch 167 Batch   85/215   train_loss = 2.986
Epoch 167 Batch   95/215   train_loss = 2.829
Epoch 167 Batch  105/215   train_loss = 2.734
Epoch 167 Batch  115/215   train_loss = 2.619
Epoch 167 Batch  125/215   train_loss = 2.676
Epoch 167 Batch  135/215   train_loss = 3.081
Epoch 167 Batch  145/215   train_loss = 2.926
Epoch 167 Batch  155/215   train_loss = 3.034
Epoch 167 Batch  165/215   train_loss = 2.779
Epoch 167 Batch  175/215   train_loss = 3.126
Epoch 167 Batch  185/215   train_loss = 3.169
Epoch 167 Batch  195/215   train_loss = 2.737
Epoch 167 Batch  205/215   train_loss = 2.922
Epoch 168 Batch    0/215   train_loss = 2.936
Epoch 168 Batch   10/215   train_loss = 3.019
Epoch 168 Batch   20/215   train_loss = 2.921
Epoch 168 Batch   30/215   train_loss = 2.800
Epoch 168 Batch   40/215   train_loss = 2.999
Epoch 168 Batch   50/215   train_loss = 2.913
Epoch 168 Batch   60/215   train_loss = 2.918
Epoch 168 Batch   70/215   train_loss = 3.116
Epoch 168 Batch   80/215   train_loss = 2.871
Epoch 168 Batch   90/215   train_loss = 3.208
Epoch 168 Batch  100/215   train_loss = 3.042
Epoch 168 Batch  110/215   train_loss = 2.846
Epoch 168 Batch  120/215   train_loss = 2.837
Epoch 168 Batch  130/215   train_loss = 3.081
Epoch 168 Batch  140/215   train_loss = 3.090
Epoch 168 Batch  150/215   train_loss = 3.004
Epoch 168 Batch  160/215   train_loss = 2.878
Epoch 168 Batch  170/215   train_loss = 2.876
Epoch 168 Batch  180/215   train_loss = 2.981
Epoch 168 Batch  190/215   train_loss = 2.931
Epoch 168 Batch  200/215   train_loss = 2.702
Epoch 168 Batch  210/215   train_loss = 2.920
Epoch 169 Batch    5/215   train_loss = 3.207
Epoch 169 Batch   15/215   train_loss = 2.916
Epoch 169 Batch   25/215   train_loss = 2.850
Epoch 169 Batch   35/215   train_loss = 2.906
Epoch 169 Batch   45/215   train_loss = 2.999
Epoch 169 Batch   55/215   train_loss = 2.955
Epoch 169 Batch   65/215   train_loss = 2.902
Epoch 169 Batch   75/215   train_loss = 2.877
Epoch 169 Batch   85/215   train_loss = 2.926
Epoch 169 Batch   95/215   train_loss = 2.999
Epoch 169 Batch  105/215   train_loss = 2.725
Epoch 169 Batch  115/215   train_loss = 2.690
Epoch 169 Batch  125/215   train_loss = 2.917
Epoch 169 Batch  135/215   train_loss = 2.905
Epoch 169 Batch  145/215   train_loss = 2.902
Epoch 169 Batch  155/215   train_loss = 3.018
Epoch 169 Batch  165/215   train_loss = 3.109
Epoch 169 Batch  175/215   train_loss = 3.113
Epoch 169 Batch  185/215   train_loss = 3.122
Epoch 169 Batch  195/215   train_loss = 2.849
Epoch 169 Batch  205/215   train_loss = 2.982
Epoch 170 Batch    0/215   train_loss = 2.993
Epoch 170 Batch   10/215   train_loss = 3.056
Epoch 170 Batch   20/215   train_loss = 2.846
Epoch 170 Batch   30/215   train_loss = 2.687
Epoch 170 Batch   40/215   train_loss = 2.813
Epoch 170 Batch   50/215   train_loss = 2.713
Epoch 170 Batch   60/215   train_loss = 3.154
Epoch 170 Batch   70/215   train_loss = 2.776
Epoch 170 Batch   80/215   train_loss = 2.934
Epoch 170 Batch   90/215   train_loss = 3.141
Epoch 170 Batch  100/215   train_loss = 2.828
Epoch 170 Batch  110/215   train_loss = 2.733
Epoch 170 Batch  120/215   train_loss = 2.934
Epoch 170 Batch  130/215   train_loss = 3.144
Epoch 170 Batch  140/215   train_loss = 3.146
Epoch 170 Batch  150/215   train_loss = 3.034
Epoch 170 Batch  160/215   train_loss = 2.928
Epoch 170 Batch  170/215   train_loss = 2.896
Epoch 170 Batch  180/215   train_loss = 2.903
Epoch 170 Batch  190/215   train_loss = 3.044
Epoch 170 Batch  200/215   train_loss = 3.057
Epoch 170 Batch  210/215   train_loss = 2.826
Epoch 171 Batch    5/215   train_loss = 2.991
Epoch 171 Batch   15/215   train_loss = 2.702
Epoch 171 Batch   25/215   train_loss = 2.865
Epoch 171 Batch   35/215   train_loss = 3.000
Epoch 171 Batch   45/215   train_loss = 3.136
Epoch 171 Batch   55/215   train_loss = 2.890
Epoch 171 Batch   65/215   train_loss = 3.000
Epoch 171 Batch   75/215   train_loss = 2.796
Epoch 171 Batch   85/215   train_loss = 2.820
Epoch 171 Batch   95/215   train_loss = 2.802
Epoch 171 Batch  105/215   train_loss = 2.749
Epoch 171 Batch  115/215   train_loss = 2.723
Epoch 171 Batch  125/215   train_loss = 2.895
Epoch 171 Batch  135/215   train_loss = 2.730
Epoch 171 Batch  145/215   train_loss = 2.943
Epoch 171 Batch  155/215   train_loss = 2.981
Epoch 171 Batch  165/215   train_loss = 2.682
Epoch 171 Batch  175/215   train_loss = 3.078
Epoch 171 Batch  185/215   train_loss = 2.977
Epoch 171 Batch  195/215   train_loss = 2.763
Epoch 171 Batch  205/215   train_loss = 2.766
Epoch 172 Batch    0/215   train_loss = 3.071
Epoch 172 Batch   10/215   train_loss = 3.015
Epoch 172 Batch   20/215   train_loss = 2.902
Epoch 172 Batch   30/215   train_loss = 2.722
Epoch 172 Batch   40/215   train_loss = 2.883
Epoch 172 Batch   50/215   train_loss = 2.900
Epoch 172 Batch   60/215   train_loss = 2.960
Epoch 172 Batch   70/215   train_loss = 2.715
Epoch 172 Batch   80/215   train_loss = 2.886
Epoch 172 Batch   90/215   train_loss = 3.190
Epoch 172 Batch  100/215   train_loss = 3.065
Epoch 172 Batch  110/215   train_loss = 2.817
Epoch 172 Batch  120/215   train_loss = 2.952
Epoch 172 Batch  130/215   train_loss = 3.036
Epoch 172 Batch  140/215   train_loss = 3.015
Epoch 172 Batch  150/215   train_loss = 2.959
Epoch 172 Batch  160/215   train_loss = 2.843
Epoch 172 Batch  170/215   train_loss = 2.870
Epoch 172 Batch  180/215   train_loss = 2.793
Epoch 172 Batch  190/215   train_loss = 3.022
Epoch 172 Batch  200/215   train_loss = 2.748
Epoch 172 Batch  210/215   train_loss = 2.712
Epoch 173 Batch    5/215   train_loss = 3.095
Epoch 173 Batch   15/215   train_loss = 2.715
Epoch 173 Batch   25/215   train_loss = 2.810
Epoch 173 Batch   35/215   train_loss = 3.014
Epoch 173 Batch   45/215   train_loss = 2.881
Epoch 173 Batch   55/215   train_loss = 2.811
Epoch 173 Batch   65/215   train_loss = 3.024
Epoch 173 Batch   75/215   train_loss = 2.899
Epoch 173 Batch   85/215   train_loss = 2.833
Epoch 173 Batch   95/215   train_loss = 2.887
Epoch 173 Batch  105/215   train_loss = 2.914
Epoch 173 Batch  115/215   train_loss = 2.821
Epoch 173 Batch  125/215   train_loss = 2.916
Epoch 173 Batch  135/215   train_loss = 2.774
Epoch 173 Batch  145/215   train_loss = 2.839
Epoch 173 Batch  155/215   train_loss = 2.836
Epoch 173 Batch  165/215   train_loss = 3.126
Epoch 173 Batch  175/215   train_loss = 3.175
Epoch 173 Batch  185/215   train_loss = 2.914
Epoch 173 Batch  195/215   train_loss = 2.750
Epoch 173 Batch  205/215   train_loss = 2.859
Epoch 174 Batch    0/215   train_loss = 2.948
Epoch 174 Batch   10/215   train_loss = 3.065
Epoch 174 Batch   20/215   train_loss = 3.043
Epoch 174 Batch   30/215   train_loss = 2.726
Epoch 174 Batch   40/215   train_loss = 3.023
Epoch 174 Batch   50/215   train_loss = 2.968
Epoch 174 Batch   60/215   train_loss = 2.994
Epoch 174 Batch   70/215   train_loss = 3.004
Epoch 174 Batch   80/215   train_loss = 2.905
Epoch 174 Batch   90/215   train_loss = 3.048
Epoch 174 Batch  100/215   train_loss = 3.021
Epoch 174 Batch  110/215   train_loss = 2.917
Epoch 174 Batch  120/215   train_loss = 3.010
Epoch 174 Batch  130/215   train_loss = 3.051
Epoch 174 Batch  140/215   train_loss = 2.874
Epoch 174 Batch  150/215   train_loss = 2.987
Epoch 174 Batch  160/215   train_loss = 2.842
Epoch 174 Batch  170/215   train_loss = 2.785
Epoch 174 Batch  180/215   train_loss = 3.101
Epoch 174 Batch  190/215   train_loss = 3.033
Epoch 174 Batch  200/215   train_loss = 2.929
Epoch 174 Batch  210/215   train_loss = 2.727
Epoch 175 Batch    5/215   train_loss = 2.956
Epoch 175 Batch   15/215   train_loss = 3.094
Epoch 175 Batch   25/215   train_loss = 2.890
Epoch 175 Batch   35/215   train_loss = 2.851
Epoch 175 Batch   45/215   train_loss = 3.141
Epoch 175 Batch   55/215   train_loss = 2.936
Epoch 175 Batch   65/215   train_loss = 3.173
Epoch 175 Batch   75/215   train_loss = 2.919
Epoch 175 Batch   85/215   train_loss = 2.833
Epoch 175 Batch   95/215   train_loss = 2.849
Epoch 175 Batch  105/215   train_loss = 2.833
Epoch 175 Batch  115/215   train_loss = 2.602
Epoch 175 Batch  125/215   train_loss = 2.874
Epoch 175 Batch  135/215   train_loss = 2.707
Epoch 175 Batch  145/215   train_loss = 2.885
Epoch 175 Batch  155/215   train_loss = 2.901
Epoch 175 Batch  165/215   train_loss = 2.759
Epoch 175 Batch  175/215   train_loss = 3.050
Epoch 175 Batch  185/215   train_loss = 2.955
Epoch 175 Batch  195/215   train_loss = 2.834
Epoch 175 Batch  205/215   train_loss = 2.953
Epoch 176 Batch    0/215   train_loss = 2.980
Epoch 176 Batch   10/215   train_loss = 2.986
Epoch 176 Batch   20/215   train_loss = 2.878
Epoch 176 Batch   30/215   train_loss = 2.797
Epoch 176 Batch   40/215   train_loss = 3.033
Epoch 176 Batch   50/215   train_loss = 2.874
Epoch 176 Batch   60/215   train_loss = 3.038
Epoch 176 Batch   70/215   train_loss = 2.788
Epoch 176 Batch   80/215   train_loss = 2.915
Epoch 176 Batch   90/215   train_loss = 2.985
Epoch 176 Batch  100/215   train_loss = 2.927
Epoch 176 Batch  110/215   train_loss = 2.801
Epoch 176 Batch  120/215   train_loss = 2.925
Epoch 176 Batch  130/215   train_loss = 3.119
Epoch 176 Batch  140/215   train_loss = 2.841
Epoch 176 Batch  150/215   train_loss = 2.952
Epoch 176 Batch  160/215   train_loss = 2.827
Epoch 176 Batch  170/215   train_loss = 2.860
Epoch 176 Batch  180/215   train_loss = 2.923
Epoch 176 Batch  190/215   train_loss = 3.002
Epoch 176 Batch  200/215   train_loss = 2.854
Epoch 176 Batch  210/215   train_loss = 2.749
Epoch 177 Batch    5/215   train_loss = 3.127
Epoch 177 Batch   15/215   train_loss = 2.756
Epoch 177 Batch   25/215   train_loss = 2.791
Epoch 177 Batch   35/215   train_loss = 3.062
Epoch 177 Batch   45/215   train_loss = 2.849
Epoch 177 Batch   55/215   train_loss = 2.828
Epoch 177 Batch   65/215   train_loss = 2.967
Epoch 177 Batch   75/215   train_loss = 2.813
Epoch 177 Batch   85/215   train_loss = 2.816
Epoch 177 Batch   95/215   train_loss = 2.865
Epoch 177 Batch  105/215   train_loss = 2.836
Epoch 177 Batch  115/215   train_loss = 2.708
Epoch 177 Batch  125/215   train_loss = 2.797
Epoch 177 Batch  135/215   train_loss = 2.769
Epoch 177 Batch  145/215   train_loss = 2.917
Epoch 177 Batch  155/215   train_loss = 2.886
Epoch 177 Batch  165/215   train_loss = 2.941
Epoch 177 Batch  175/215   train_loss = 3.011
Epoch 177 Batch  185/215   train_loss = 2.891
Epoch 177 Batch  195/215   train_loss = 2.767
Epoch 177 Batch  205/215   train_loss = 3.001
Epoch 178 Batch    0/215   train_loss = 2.970
Epoch 178 Batch   10/215   train_loss = 3.171
Epoch 178 Batch   20/215   train_loss = 2.932
Epoch 178 Batch   30/215   train_loss = 2.705
Epoch 178 Batch   40/215   train_loss = 2.916
Epoch 178 Batch   50/215   train_loss = 2.943
Epoch 178 Batch   60/215   train_loss = 3.010
Epoch 178 Batch   70/215   train_loss = 2.644
Epoch 178 Batch   80/215   train_loss = 2.835
Epoch 178 Batch   90/215   train_loss = 2.950
Epoch 178 Batch  100/215   train_loss = 2.919
Epoch 178 Batch  110/215   train_loss = 2.922
Epoch 178 Batch  120/215   train_loss = 2.886
Epoch 178 Batch  130/215   train_loss = 3.069
Epoch 178 Batch  140/215   train_loss = 3.065
Epoch 178 Batch  150/215   train_loss = 2.883
Epoch 178 Batch  160/215   train_loss = 2.907
Epoch 178 Batch  170/215   train_loss = 2.857
Epoch 178 Batch  180/215   train_loss = 3.040
Epoch 178 Batch  190/215   train_loss = 3.169
Epoch 178 Batch  200/215   train_loss = 2.891
Epoch 178 Batch  210/215   train_loss = 2.604
Epoch 179 Batch    5/215   train_loss = 3.006
Epoch 179 Batch   15/215   train_loss = 2.807
Epoch 179 Batch   25/215   train_loss = 2.736
Epoch 179 Batch   35/215   train_loss = 3.027
Epoch 179 Batch   45/215   train_loss = 2.822
Epoch 179 Batch   55/215   train_loss = 2.750
Epoch 179 Batch   65/215   train_loss = 3.019
Epoch 179 Batch   75/215   train_loss = 3.280
Epoch 179 Batch   85/215   train_loss = 2.828
Epoch 179 Batch   95/215   train_loss = 2.948
Epoch 179 Batch  105/215   train_loss = 2.789
Epoch 179 Batch  115/215   train_loss = 2.768
Epoch 179 Batch  125/215   train_loss = 2.783
Epoch 179 Batch  135/215   train_loss = 2.924
Epoch 179 Batch  145/215   train_loss = 2.920
Epoch 179 Batch  155/215   train_loss = 2.891
Epoch 179 Batch  165/215   train_loss = 2.948
Epoch 179 Batch  175/215   train_loss = 3.163
Epoch 179 Batch  185/215   train_loss = 3.065
Epoch 179 Batch  195/215   train_loss = 2.723
Epoch 179 Batch  205/215   train_loss = 2.875
Epoch 180 Batch    0/215   train_loss = 2.910
Epoch 180 Batch   10/215   train_loss = 2.868
Epoch 180 Batch   20/215   train_loss = 2.865
Epoch 180 Batch   30/215   train_loss = 2.887
Epoch 180 Batch   40/215   train_loss = 2.943
Epoch 180 Batch   50/215   train_loss = 2.743
Epoch 180 Batch   60/215   train_loss = 3.090
Epoch 180 Batch   70/215   train_loss = 2.890
Epoch 180 Batch   80/215   train_loss = 3.008
Epoch 180 Batch   90/215   train_loss = 2.957
Epoch 180 Batch  100/215   train_loss = 2.927
Epoch 180 Batch  110/215   train_loss = 2.830
Epoch 180 Batch  120/215   train_loss = 2.992
Epoch 180 Batch  130/215   train_loss = 2.932
Epoch 180 Batch  140/215   train_loss = 2.885
Epoch 180 Batch  150/215   train_loss = 3.013
Epoch 180 Batch  160/215   train_loss = 3.012
Epoch 180 Batch  170/215   train_loss = 2.999
Epoch 180 Batch  180/215   train_loss = 2.879
Epoch 180 Batch  190/215   train_loss = 3.051
Epoch 180 Batch  200/215   train_loss = 2.809
Epoch 180 Batch  210/215   train_loss = 2.676
Epoch 181 Batch    5/215   train_loss = 2.840
Epoch 181 Batch   15/215   train_loss = 2.777
Epoch 181 Batch   25/215   train_loss = 2.763
Epoch 181 Batch   35/215   train_loss = 2.914
Epoch 181 Batch   45/215   train_loss = 2.914
Epoch 181 Batch   55/215   train_loss = 2.710
Epoch 181 Batch   65/215   train_loss = 2.923
Epoch 181 Batch   75/215   train_loss = 2.734
Epoch 181 Batch   85/215   train_loss = 2.718
Epoch 181 Batch   95/215   train_loss = 2.957
Epoch 181 Batch  105/215   train_loss = 2.747
Epoch 181 Batch  115/215   train_loss = 2.843
Epoch 181 Batch  125/215   train_loss = 2.754
Epoch 181 Batch  135/215   train_loss = 2.752
Epoch 181 Batch  145/215   train_loss = 2.915
Epoch 181 Batch  155/215   train_loss = 2.951
Epoch 181 Batch  165/215   train_loss = 3.031
Epoch 181 Batch  175/215   train_loss = 2.982
Epoch 181 Batch  185/215   train_loss = 2.955
Epoch 181 Batch  195/215   train_loss = 2.705
Epoch 181 Batch  205/215   train_loss = 2.900
Epoch 182 Batch    0/215   train_loss = 3.130
Epoch 182 Batch   10/215   train_loss = 3.052
Epoch 182 Batch   20/215   train_loss = 2.878
Epoch 182 Batch   30/215   train_loss = 2.845
Epoch 182 Batch   40/215   train_loss = 2.771
Epoch 182 Batch   50/215   train_loss = 2.753
Epoch 182 Batch   60/215   train_loss = 2.870
Epoch 182 Batch   70/215   train_loss = 2.879
Epoch 182 Batch   80/215   train_loss = 2.868
Epoch 182 Batch   90/215   train_loss = 2.855
Epoch 182 Batch  100/215   train_loss = 3.013
Epoch 182 Batch  110/215   train_loss = 2.801
Epoch 182 Batch  120/215   train_loss = 2.731
Epoch 182 Batch  130/215   train_loss = 3.151
Epoch 182 Batch  140/215   train_loss = 2.853
Epoch 182 Batch  150/215   train_loss = 3.014
Epoch 182 Batch  160/215   train_loss = 3.076
Epoch 182 Batch  170/215   train_loss = 2.868
Epoch 182 Batch  180/215   train_loss = 2.914
Epoch 182 Batch  190/215   train_loss = 2.976
Epoch 182 Batch  200/215   train_loss = 2.859
Epoch 182 Batch  210/215   train_loss = 2.749
Epoch 183 Batch    5/215   train_loss = 2.833
Epoch 183 Batch   15/215   train_loss = 2.790
Epoch 183 Batch   25/215   train_loss = 2.806
Epoch 183 Batch   35/215   train_loss = 2.904
Epoch 183 Batch   45/215   train_loss = 2.890
Epoch 183 Batch   55/215   train_loss = 2.896
Epoch 183 Batch   65/215   train_loss = 3.086
Epoch 183 Batch   75/215   train_loss = 2.883
Epoch 183 Batch   85/215   train_loss = 2.973
Epoch 183 Batch   95/215   train_loss = 2.885
Epoch 183 Batch  105/215   train_loss = 2.700
Epoch 183 Batch  115/215   train_loss = 2.774
Epoch 183 Batch  125/215   train_loss = 2.693
Epoch 183 Batch  135/215   train_loss = 2.951
Epoch 183 Batch  145/215   train_loss = 2.870
Epoch 183 Batch  155/215   train_loss = 2.895
Epoch 183 Batch  165/215   train_loss = 2.929
Epoch 183 Batch  175/215   train_loss = 3.077
Epoch 183 Batch  185/215   train_loss = 2.850
Epoch 183 Batch  195/215   train_loss = 2.778
Epoch 183 Batch  205/215   train_loss = 2.801
Epoch 184 Batch    0/215   train_loss = 3.043
Epoch 184 Batch   10/215   train_loss = 2.998
Epoch 184 Batch   20/215   train_loss = 2.979
Epoch 184 Batch   30/215   train_loss = 2.919
Epoch 184 Batch   40/215   train_loss = 3.022
Epoch 184 Batch   50/215   train_loss = 2.748
Epoch 184 Batch   60/215   train_loss = 3.071
Epoch 184 Batch   70/215   train_loss = 2.770
Epoch 184 Batch   80/215   train_loss = 2.918
Epoch 184 Batch   90/215   train_loss = 2.953
Epoch 184 Batch  100/215   train_loss = 2.757
Epoch 184 Batch  110/215   train_loss = 2.872
Epoch 184 Batch  120/215   train_loss = 3.120
Epoch 184 Batch  130/215   train_loss = 3.241
Epoch 184 Batch  140/215   train_loss = 2.810
Epoch 184 Batch  150/215   train_loss = 2.817
Epoch 184 Batch  160/215   train_loss = 2.872
Epoch 184 Batch  170/215   train_loss = 3.104
Epoch 184 Batch  180/215   train_loss = 2.794
Epoch 184 Batch  190/215   train_loss = 3.022
Epoch 184 Batch  200/215   train_loss = 2.876
Epoch 184 Batch  210/215   train_loss = 2.763
Epoch 185 Batch    5/215   train_loss = 2.971
Epoch 185 Batch   15/215   train_loss = 2.893
Epoch 185 Batch   25/215   train_loss = 2.783
Epoch 185 Batch   35/215   train_loss = 2.963
Epoch 185 Batch   45/215   train_loss = 2.765
Epoch 185 Batch   55/215   train_loss = 2.787
Epoch 185 Batch   65/215   train_loss = 2.931
Epoch 185 Batch   75/215   train_loss = 2.872
Epoch 185 Batch   85/215   train_loss = 2.803
Epoch 185 Batch   95/215   train_loss = 2.821
Epoch 185 Batch  105/215   train_loss = 2.694
Epoch 185 Batch  115/215   train_loss = 2.735
Epoch 185 Batch  125/215   train_loss = 2.766
Epoch 185 Batch  135/215   train_loss = 2.900
Epoch 185 Batch  145/215   train_loss = 3.003
Epoch 185 Batch  155/215   train_loss = 2.861
Epoch 185 Batch  165/215   train_loss = 3.054
Epoch 185 Batch  175/215   train_loss = 3.181
Epoch 185 Batch  185/215   train_loss = 2.947
Epoch 185 Batch  195/215   train_loss = 2.683
Epoch 185 Batch  205/215   train_loss = 2.875
Epoch 186 Batch    0/215   train_loss = 2.955
Epoch 186 Batch   10/215   train_loss = 3.115
Epoch 186 Batch   20/215   train_loss = 3.020
Epoch 186 Batch   30/215   train_loss = 2.849
Epoch 186 Batch   40/215   train_loss = 2.713
Epoch 186 Batch   50/215   train_loss = 3.029
Epoch 186 Batch   60/215   train_loss = 2.912
Epoch 186 Batch   70/215   train_loss = 2.728
Epoch 186 Batch   80/215   train_loss = 2.764
Epoch 186 Batch   90/215   train_loss = 2.903
Epoch 186 Batch  100/215   train_loss = 3.107
Epoch 186 Batch  110/215   train_loss = 2.818
Epoch 186 Batch  120/215   train_loss = 3.027
Epoch 186 Batch  130/215   train_loss = 3.052
Epoch 186 Batch  140/215   train_loss = 2.764
Epoch 186 Batch  150/215   train_loss = 3.049
Epoch 186 Batch  160/215   train_loss = 2.805
Epoch 186 Batch  170/215   train_loss = 2.926
Epoch 186 Batch  180/215   train_loss = 3.001
Epoch 186 Batch  190/215   train_loss = 3.039
Epoch 186 Batch  200/215   train_loss = 2.848
Epoch 186 Batch  210/215   train_loss = 2.788
Epoch 187 Batch    5/215   train_loss = 3.079
Epoch 187 Batch   15/215   train_loss = 2.855
Epoch 187 Batch   25/215   train_loss = 2.845
Epoch 187 Batch   35/215   train_loss = 2.926
Epoch 187 Batch   45/215   train_loss = 2.811
Epoch 187 Batch   55/215   train_loss = 2.696
Epoch 187 Batch   65/215   train_loss = 3.067
Epoch 187 Batch   75/215   train_loss = 2.740
Epoch 187 Batch   85/215   train_loss = 2.666
Epoch 187 Batch   95/215   train_loss = 2.695
Epoch 187 Batch  105/215   train_loss = 2.783
Epoch 187 Batch  115/215   train_loss = 2.795
Epoch 187 Batch  125/215   train_loss = 2.879
Epoch 187 Batch  135/215   train_loss = 2.900
Epoch 187 Batch  145/215   train_loss = 2.904
Epoch 187 Batch  155/215   train_loss = 3.040
Epoch 187 Batch  165/215   train_loss = 2.793
Epoch 187 Batch  175/215   train_loss = 2.940
Epoch 187 Batch  185/215   train_loss = 2.924
Epoch 187 Batch  195/215   train_loss = 2.768
Epoch 187 Batch  205/215   train_loss = 2.735
Epoch 188 Batch    0/215   train_loss = 2.979
Epoch 188 Batch   10/215   train_loss = 2.904
Epoch 188 Batch   20/215   train_loss = 2.912
Epoch 188 Batch   30/215   train_loss = 2.749
Epoch 188 Batch   40/215   train_loss = 2.938
Epoch 188 Batch   50/215   train_loss = 2.975
Epoch 188 Batch   60/215   train_loss = 3.035
Epoch 188 Batch   70/215   train_loss = 2.896
Epoch 188 Batch   80/215   train_loss = 2.710
Epoch 188 Batch   90/215   train_loss = 2.828
Epoch 188 Batch  100/215   train_loss = 2.969
Epoch 188 Batch  110/215   train_loss = 3.156
Epoch 188 Batch  120/215   train_loss = 2.914
Epoch 188 Batch  130/215   train_loss = 3.187
Epoch 188 Batch  140/215   train_loss = 2.981
Epoch 188 Batch  150/215   train_loss = 2.750
Epoch 188 Batch  160/215   train_loss = 2.966
Epoch 188 Batch  170/215   train_loss = 2.867
Epoch 188 Batch  180/215   train_loss = 3.032
Epoch 188 Batch  190/215   train_loss = 2.925
Epoch 188 Batch  200/215   train_loss = 2.791
Epoch 188 Batch  210/215   train_loss = 2.600
Epoch 189 Batch    5/215   train_loss = 2.979
Epoch 189 Batch   15/215   train_loss = 2.663
Epoch 189 Batch   25/215   train_loss = 2.834
Epoch 189 Batch   35/215   train_loss = 2.814
Epoch 189 Batch   45/215   train_loss = 2.838
Epoch 189 Batch   55/215   train_loss = 2.793
Epoch 189 Batch   65/215   train_loss = 2.881
Epoch 189 Batch   75/215   train_loss = 2.780
Epoch 189 Batch   85/215   train_loss = 2.732
Epoch 189 Batch   95/215   train_loss = 2.806
Epoch 189 Batch  105/215   train_loss = 2.679
Epoch 189 Batch  115/215   train_loss = 2.781
Epoch 189 Batch  125/215   train_loss = 2.867
Epoch 189 Batch  135/215   train_loss = 2.798
Epoch 189 Batch  145/215   train_loss = 2.919
Epoch 189 Batch  155/215   train_loss = 2.896
Epoch 189 Batch  165/215   train_loss = 2.723
Epoch 189 Batch  175/215   train_loss = 3.087
Epoch 189 Batch  185/215   train_loss = 2.914
Epoch 189 Batch  195/215   train_loss = 2.882
Epoch 189 Batch  205/215   train_loss = 2.847
Epoch 190 Batch    0/215   train_loss = 3.022
Epoch 190 Batch   10/215   train_loss = 2.993
Epoch 190 Batch   20/215   train_loss = 2.957
Epoch 190 Batch   30/215   train_loss = 2.772
Epoch 190 Batch   40/215   train_loss = 2.761
Epoch 190 Batch   50/215   train_loss = 2.872
Epoch 190 Batch   60/215   train_loss = 2.926
Epoch 190 Batch   70/215   train_loss = 2.720
Epoch 190 Batch   80/215   train_loss = 2.883
Epoch 190 Batch   90/215   train_loss = 2.961
Epoch 190 Batch  100/215   train_loss = 2.875
Epoch 190 Batch  110/215   train_loss = 2.784
Epoch 190 Batch  120/215   train_loss = 2.837
Epoch 190 Batch  130/215   train_loss = 2.924
Epoch 190 Batch  140/215   train_loss = 3.060
Epoch 190 Batch  150/215   train_loss = 2.876
Epoch 190 Batch  160/215   train_loss = 2.935
Epoch 190 Batch  170/215   train_loss = 2.776
Epoch 190 Batch  180/215   train_loss = 2.871
Epoch 190 Batch  190/215   train_loss = 2.867
Epoch 190 Batch  200/215   train_loss = 2.847
Epoch 190 Batch  210/215   train_loss = 2.640
Epoch 191 Batch    5/215   train_loss = 3.007
Epoch 191 Batch   15/215   train_loss = 2.778
Epoch 191 Batch   25/215   train_loss = 2.707
Epoch 191 Batch   35/215   train_loss = 2.934
Epoch 191 Batch   45/215   train_loss = 3.022
Epoch 191 Batch   55/215   train_loss = 2.839
Epoch 191 Batch   65/215   train_loss = 3.008
Epoch 191 Batch   75/215   train_loss = 2.823
Epoch 191 Batch   85/215   train_loss = 2.788
Epoch 191 Batch   95/215   train_loss = 2.774
Epoch 191 Batch  105/215   train_loss = 2.784
Epoch 191 Batch  115/215   train_loss = 2.751
Epoch 191 Batch  125/215   train_loss = 3.001
Epoch 191 Batch  135/215   train_loss = 2.931
Epoch 191 Batch  145/215   train_loss = 2.927
Epoch 191 Batch  155/215   train_loss = 3.077
Epoch 191 Batch  165/215   train_loss = 2.812
Epoch 191 Batch  175/215   train_loss = 3.207
Epoch 191 Batch  185/215   train_loss = 2.999
Epoch 191 Batch  195/215   train_loss = 2.839
Epoch 191 Batch  205/215   train_loss = 2.920
Epoch 192 Batch    0/215   train_loss = 2.885
Epoch 192 Batch   10/215   train_loss = 3.041
Epoch 192 Batch   20/215   train_loss = 2.805
Epoch 192 Batch   30/215   train_loss = 2.708
Epoch 192 Batch   40/215   train_loss = 2.878
Epoch 192 Batch   50/215   train_loss = 2.654
Epoch 192 Batch   60/215   train_loss = 3.051
Epoch 192 Batch   70/215   train_loss = 2.651
Epoch 192 Batch   80/215   train_loss = 2.792
Epoch 192 Batch   90/215   train_loss = 2.863
Epoch 192 Batch  100/215   train_loss = 3.015
Epoch 192 Batch  110/215   train_loss = 2.827
Epoch 192 Batch  120/215   train_loss = 2.833
Epoch 192 Batch  130/215   train_loss = 2.908
Epoch 192 Batch  140/215   train_loss = 2.862
Epoch 192 Batch  150/215   train_loss = 2.998
Epoch 192 Batch  160/215   train_loss = 2.903
Epoch 192 Batch  170/215   train_loss = 2.698
Epoch 192 Batch  180/215   train_loss = 2.889
Epoch 192 Batch  190/215   train_loss = 2.823
Epoch 192 Batch  200/215   train_loss = 2.586
Epoch 192 Batch  210/215   train_loss = 2.730
Epoch 193 Batch    5/215   train_loss = 3.055
Epoch 193 Batch   15/215   train_loss = 2.791
Epoch 193 Batch   25/215   train_loss = 2.964
Epoch 193 Batch   35/215   train_loss = 3.043
Epoch 193 Batch   45/215   train_loss = 2.798
Epoch 193 Batch   55/215   train_loss = 2.780
Epoch 193 Batch   65/215   train_loss = 3.002
Epoch 193 Batch   75/215   train_loss = 2.812
Epoch 193 Batch   85/215   train_loss = 2.735
Epoch 193 Batch   95/215   train_loss = 2.796
Epoch 193 Batch  105/215   train_loss = 2.851
Epoch 193 Batch  115/215   train_loss = 2.867
Epoch 193 Batch  125/215   train_loss = 2.809
Epoch 193 Batch  135/215   train_loss = 2.946
Epoch 193 Batch  145/215   train_loss = 2.930
Epoch 193 Batch  155/215   train_loss = 2.947
Epoch 193 Batch  165/215   train_loss = 2.725
Epoch 193 Batch  175/215   train_loss = 3.046
Epoch 193 Batch  185/215   train_loss = 2.884
Epoch 193 Batch  195/215   train_loss = 2.798
Epoch 193 Batch  205/215   train_loss = 2.719
Epoch 194 Batch    0/215   train_loss = 3.095
Epoch 194 Batch   10/215   train_loss = 2.966
Epoch 194 Batch   20/215   train_loss = 2.778
Epoch 194 Batch   30/215   train_loss = 2.765
Epoch 194 Batch   40/215   train_loss = 2.830
Epoch 194 Batch   50/215   train_loss = 2.701
Epoch 194 Batch   60/215   train_loss = 3.024
Epoch 194 Batch   70/215   train_loss = 2.710
Epoch 194 Batch   80/215   train_loss = 3.048
Epoch 194 Batch   90/215   train_loss = 3.009
Epoch 194 Batch  100/215   train_loss = 2.977
Epoch 194 Batch  110/215   train_loss = 2.786
Epoch 194 Batch  120/215   train_loss = 2.918
Epoch 194 Batch  130/215   train_loss = 3.128
Epoch 194 Batch  140/215   train_loss = 2.772
Epoch 194 Batch  150/215   train_loss = 2.898
Epoch 194 Batch  160/215   train_loss = 2.909
Epoch 194 Batch  170/215   train_loss = 2.847
Epoch 194 Batch  180/215   train_loss = 2.897
Epoch 194 Batch  190/215   train_loss = 2.967
Epoch 194 Batch  200/215   train_loss = 2.788
Epoch 194 Batch  210/215   train_loss = 2.703
Epoch 195 Batch    5/215   train_loss = 3.042
Epoch 195 Batch   15/215   train_loss = 2.721
Epoch 195 Batch   25/215   train_loss = 2.860
Epoch 195 Batch   35/215   train_loss = 3.119
Epoch 195 Batch   45/215   train_loss = 2.905
Epoch 195 Batch   55/215   train_loss = 2.862
Epoch 195 Batch   65/215   train_loss = 2.894
Epoch 195 Batch   75/215   train_loss = 2.824
Epoch 195 Batch   85/215   train_loss = 2.747
Epoch 195 Batch   95/215   train_loss = 2.829
Epoch 195 Batch  105/215   train_loss = 2.647
Epoch 195 Batch  115/215   train_loss = 2.670
Epoch 195 Batch  125/215   train_loss = 2.799
Epoch 195 Batch  135/215   train_loss = 2.799
Epoch 195 Batch  145/215   train_loss = 2.850
Epoch 195 Batch  155/215   train_loss = 2.848
Epoch 195 Batch  165/215   train_loss = 2.783
Epoch 195 Batch  175/215   train_loss = 3.108
Epoch 195 Batch  185/215   train_loss = 2.854
Epoch 195 Batch  195/215   train_loss = 2.832
Epoch 195 Batch  205/215   train_loss = 3.005
Epoch 196 Batch    0/215   train_loss = 2.903
Epoch 196 Batch   10/215   train_loss = 3.061
Epoch 196 Batch   20/215   train_loss = 2.955
Epoch 196 Batch   30/215   train_loss = 2.788
Epoch 196 Batch   40/215   train_loss = 2.916
Epoch 196 Batch   50/215   train_loss = 2.716
Epoch 196 Batch   60/215   train_loss = 2.892
Epoch 196 Batch   70/215   train_loss = 2.926
Epoch 196 Batch   80/215   train_loss = 3.164
Epoch 196 Batch   90/215   train_loss = 2.808
Epoch 196 Batch  100/215   train_loss = 2.829
Epoch 196 Batch  110/215   train_loss = 2.885
Epoch 196 Batch  120/215   train_loss = 3.035
Epoch 196 Batch  130/215   train_loss = 3.066
Epoch 196 Batch  140/215   train_loss = 2.813
Epoch 196 Batch  150/215   train_loss = 2.852
Epoch 196 Batch  160/215   train_loss = 2.917
Epoch 196 Batch  170/215   train_loss = 2.767
Epoch 196 Batch  180/215   train_loss = 2.953
Epoch 196 Batch  190/215   train_loss = 2.979
Epoch 196 Batch  200/215   train_loss = 2.644
Epoch 196 Batch  210/215   train_loss = 2.715
Epoch 197 Batch    5/215   train_loss = 3.018
Epoch 197 Batch   15/215   train_loss = 2.698
Epoch 197 Batch   25/215   train_loss = 2.985
Epoch 197 Batch   35/215   train_loss = 2.992
Epoch 197 Batch   45/215   train_loss = 2.887
Epoch 197 Batch   55/215   train_loss = 3.072
Epoch 197 Batch   65/215   train_loss = 2.985
Epoch 197 Batch   75/215   train_loss = 2.745
Epoch 197 Batch   85/215   train_loss = 2.889
Epoch 197 Batch   95/215   train_loss = 2.669
Epoch 197 Batch  105/215   train_loss = 2.716
Epoch 197 Batch  115/215   train_loss = 2.817
Epoch 197 Batch  125/215   train_loss = 2.827
Epoch 197 Batch  135/215   train_loss = 2.820
Epoch 197 Batch  145/215   train_loss = 3.158
Epoch 197 Batch  155/215   train_loss = 2.823
Epoch 197 Batch  165/215   train_loss = 3.014
Epoch 197 Batch  175/215   train_loss = 3.104
Epoch 197 Batch  185/215   train_loss = 2.919
Epoch 197 Batch  195/215   train_loss = 2.863
Epoch 197 Batch  205/215   train_loss = 2.862
Epoch 198 Batch    0/215   train_loss = 2.951
Epoch 198 Batch   10/215   train_loss = 2.901
Epoch 198 Batch   20/215   train_loss = 2.977
Epoch 198 Batch   30/215   train_loss = 2.719
Epoch 198 Batch   40/215   train_loss = 2.879
Epoch 198 Batch   50/215   train_loss = 2.776
Epoch 198 Batch   60/215   train_loss = 2.944
Epoch 198 Batch   70/215   train_loss = 3.120
Epoch 198 Batch   80/215   train_loss = 2.869
Epoch 198 Batch   90/215   train_loss = 3.040
Epoch 198 Batch  100/215   train_loss = 3.145
Epoch 198 Batch  110/215   train_loss = 2.841
Epoch 198 Batch  120/215   train_loss = 2.901
Epoch 198 Batch  130/215   train_loss = 3.123
Epoch 198 Batch  140/215   train_loss = 2.903
Epoch 198 Batch  150/215   train_loss = 2.879
Epoch 198 Batch  160/215   train_loss = 2.927
Epoch 198 Batch  170/215   train_loss = 2.723
Epoch 198 Batch  180/215   train_loss = 2.976
Epoch 198 Batch  190/215   train_loss = 2.971
Epoch 198 Batch  200/215   train_loss = 2.692
Epoch 198 Batch  210/215   train_loss = 2.904
Epoch 199 Batch    5/215   train_loss = 2.922
Epoch 199 Batch   15/215   train_loss = 2.727
Epoch 199 Batch   25/215   train_loss = 2.793
Epoch 199 Batch   35/215   train_loss = 2.969
Epoch 199 Batch   45/215   train_loss = 2.808
Epoch 199 Batch   55/215   train_loss = 2.778
Epoch 199 Batch   65/215   train_loss = 3.290
Epoch 199 Batch   75/215   train_loss = 2.842
Epoch 199 Batch   85/215   train_loss = 2.904
Epoch 199 Batch   95/215   train_loss = 2.976
Epoch 199 Batch  105/215   train_loss = 2.807
Epoch 199 Batch  115/215   train_loss = 2.707
Epoch 199 Batch  125/215   train_loss = 2.814
Epoch 199 Batch  135/215   train_loss = 2.868
Epoch 199 Batch  145/215   train_loss = 2.915
Epoch 199 Batch  155/215   train_loss = 2.964
Epoch 199 Batch  165/215   train_loss = 2.918
Epoch 199 Batch  175/215   train_loss = 2.898
Epoch 199 Batch  185/215   train_loss = 2.905
Epoch 199 Batch  195/215   train_loss = 2.922
Epoch 199 Batch  205/215   train_loss = 2.766
Epoch 200 Batch    0/215   train_loss = 3.088
Epoch 200 Batch   10/215   train_loss = 2.903
Epoch 200 Batch   20/215   train_loss = 2.808
Epoch 200 Batch   30/215   train_loss = 2.805
Epoch 200 Batch   40/215   train_loss = 2.824
Epoch 200 Batch   50/215   train_loss = 2.864
Epoch 200 Batch   60/215   train_loss = 2.950
Epoch 200 Batch   70/215   train_loss = 2.749
Epoch 200 Batch   80/215   train_loss = 2.940
Epoch 200 Batch   90/215   train_loss = 2.811
Epoch 200 Batch  100/215   train_loss = 3.175
Epoch 200 Batch  110/215   train_loss = 2.829
Epoch 200 Batch  120/215   train_loss = 2.924
Epoch 200 Batch  130/215   train_loss = 3.081
Epoch 200 Batch  140/215   train_loss = 2.761
Epoch 200 Batch  150/215   train_loss = 2.985
Epoch 200 Batch  160/215   train_loss = 2.846
Epoch 200 Batch  170/215   train_loss = 2.893
Epoch 200 Batch  180/215   train_loss = 2.811
Epoch 200 Batch  190/215   train_loss = 2.763
Epoch 200 Batch  200/215   train_loss = 2.736
Epoch 200 Batch  210/215   train_loss = 2.778
Epoch 201 Batch    5/215   train_loss = 2.900
Epoch 201 Batch   15/215   train_loss = 2.836
Epoch 201 Batch   25/215   train_loss = 2.907
Epoch 201 Batch   35/215   train_loss = 2.905
Epoch 201 Batch   45/215   train_loss = 2.857
Epoch 201 Batch   55/215   train_loss = 2.949
Epoch 201 Batch   65/215   train_loss = 3.064
Epoch 201 Batch   75/215   train_loss = 2.830
Epoch 201 Batch   85/215   train_loss = 2.870
Epoch 201 Batch   95/215   train_loss = 2.786
Epoch 201 Batch  105/215   train_loss = 2.856
Epoch 201 Batch  115/215   train_loss = 2.610
Epoch 201 Batch  125/215   train_loss = 2.821
Epoch 201 Batch  135/215   train_loss = 2.883
Epoch 201 Batch  145/215   train_loss = 2.954
Epoch 201 Batch  155/215   train_loss = 2.905
Epoch 201 Batch  165/215   train_loss = 2.854
Epoch 201 Batch  175/215   train_loss = 3.062
Epoch 201 Batch  185/215   train_loss = 3.004
Epoch 201 Batch  195/215   train_loss = 2.680
Epoch 201 Batch  205/215   train_loss = 3.155
Epoch 202 Batch    0/215   train_loss = 3.061
Epoch 202 Batch   10/215   train_loss = 2.961
Epoch 202 Batch   20/215   train_loss = 2.815
Epoch 202 Batch   30/215   train_loss = 2.628
Epoch 202 Batch   40/215   train_loss = 2.798
Epoch 202 Batch   50/215   train_loss = 2.838
Epoch 202 Batch   60/215   train_loss = 2.944
Epoch 202 Batch   70/215   train_loss = 2.999
Epoch 202 Batch   80/215   train_loss = 2.987
Epoch 202 Batch   90/215   train_loss = 2.786
Epoch 202 Batch  100/215   train_loss = 2.823
Epoch 202 Batch  110/215   train_loss = 2.921
Epoch 202 Batch  120/215   train_loss = 2.966
Epoch 202 Batch  130/215   train_loss = 2.989
Epoch 202 Batch  140/215   train_loss = 3.026
Epoch 202 Batch  150/215   train_loss = 2.893
Epoch 202 Batch  160/215   train_loss = 2.926
Epoch 202 Batch  170/215   train_loss = 2.664
Epoch 202 Batch  180/215   train_loss = 3.092
Epoch 202 Batch  190/215   train_loss = 2.864
Epoch 202 Batch  200/215   train_loss = 2.861
Epoch 202 Batch  210/215   train_loss = 2.657
Epoch 203 Batch    5/215   train_loss = 3.061
Epoch 203 Batch   15/215   train_loss = 2.720
Epoch 203 Batch   25/215   train_loss = 2.748
Epoch 203 Batch   35/215   train_loss = 2.961
Epoch 203 Batch   45/215   train_loss = 3.030
Epoch 203 Batch   55/215   train_loss = 2.816
Epoch 203 Batch   65/215   train_loss = 2.986
Epoch 203 Batch   75/215   train_loss = 2.949
Epoch 203 Batch   85/215   train_loss = 2.604
Epoch 203 Batch   95/215   train_loss = 2.810
Epoch 203 Batch  105/215   train_loss = 2.808
Epoch 203 Batch  115/215   train_loss = 2.683
Epoch 203 Batch  125/215   train_loss = 2.770
Epoch 203 Batch  135/215   train_loss = 2.817
Epoch 203 Batch  145/215   train_loss = 2.757
Epoch 203 Batch  155/215   train_loss = 2.842
Epoch 203 Batch  165/215   train_loss = 2.907
Epoch 203 Batch  175/215   train_loss = 3.054
Epoch 203 Batch  185/215   train_loss = 3.154
Epoch 203 Batch  195/215   train_loss = 2.784
Epoch 203 Batch  205/215   train_loss = 2.738
Epoch 204 Batch    0/215   train_loss = 3.068
Epoch 204 Batch   10/215   train_loss = 2.808
Epoch 204 Batch   20/215   train_loss = 2.907
Epoch 204 Batch   30/215   train_loss = 2.746
Epoch 204 Batch   40/215   train_loss = 2.866
Epoch 204 Batch   50/215   train_loss = 3.091
Epoch 204 Batch   60/215   train_loss = 2.863
Epoch 204 Batch   70/215   train_loss = 2.773
Epoch 204 Batch   80/215   train_loss = 2.783
Epoch 204 Batch   90/215   train_loss = 3.014
Epoch 204 Batch  100/215   train_loss = 3.127
Epoch 204 Batch  110/215   train_loss = 2.805
Epoch 204 Batch  120/215   train_loss = 2.890
Epoch 204 Batch  130/215   train_loss = 3.176
Epoch 204 Batch  140/215   train_loss = 2.864
Epoch 204 Batch  150/215   train_loss = 2.829
Epoch 204 Batch  160/215   train_loss = 2.788
Epoch 204 Batch  170/215   train_loss = 2.887
Epoch 204 Batch  180/215   train_loss = 2.986
Epoch 204 Batch  190/215   train_loss = 2.901
Epoch 204 Batch  200/215   train_loss = 2.838
Epoch 204 Batch  210/215   train_loss = 2.662
Epoch 205 Batch    5/215   train_loss = 3.027
Epoch 205 Batch   15/215   train_loss = 3.005
Epoch 205 Batch   25/215   train_loss = 2.614
Epoch 205 Batch   35/215   train_loss = 3.030
Epoch 205 Batch   45/215   train_loss = 2.863
Epoch 205 Batch   55/215   train_loss = 2.899
Epoch 205 Batch   65/215   train_loss = 2.699
Epoch 205 Batch   75/215   train_loss = 2.946
Epoch 205 Batch   85/215   train_loss = 2.796
Epoch 205 Batch   95/215   train_loss = 2.830
Epoch 205 Batch  105/215   train_loss = 2.807
Epoch 205 Batch  115/215   train_loss = 2.511
Epoch 205 Batch  125/215   train_loss = 2.776
Epoch 205 Batch  135/215   train_loss = 2.994
Epoch 205 Batch  145/215   train_loss = 2.692
Epoch 205 Batch  155/215   train_loss = 2.823
Epoch 205 Batch  165/215   train_loss = 2.803
Epoch 205 Batch  175/215   train_loss = 2.950
Epoch 205 Batch  185/215   train_loss = 2.868
Epoch 205 Batch  195/215   train_loss = 2.933
Epoch 205 Batch  205/215   train_loss = 2.846
Epoch 206 Batch    0/215   train_loss = 2.877
Epoch 206 Batch   10/215   train_loss = 2.848
Epoch 206 Batch   20/215   train_loss = 2.951
Epoch 206 Batch   30/215   train_loss = 2.624
Epoch 206 Batch   40/215   train_loss = 2.645
Epoch 206 Batch   50/215   train_loss = 2.678
Epoch 206 Batch   60/215   train_loss = 2.785
Epoch 206 Batch   70/215   train_loss = 2.848
Epoch 206 Batch   80/215   train_loss = 2.826
Epoch 206 Batch   90/215   train_loss = 2.848
Epoch 206 Batch  100/215   train_loss = 2.882
Epoch 206 Batch  110/215   train_loss = 2.863
Epoch 206 Batch  120/215   train_loss = 2.855
Epoch 206 Batch  130/215   train_loss = 3.123
Epoch 206 Batch  140/215   train_loss = 2.810
Epoch 206 Batch  150/215   train_loss = 2.933
Epoch 206 Batch  160/215   train_loss = 2.936
Epoch 206 Batch  170/215   train_loss = 2.844
Epoch 206 Batch  180/215   train_loss = 2.885
Epoch 206 Batch  190/215   train_loss = 2.859
Epoch 206 Batch  200/215   train_loss = 2.847
Epoch 206 Batch  210/215   train_loss = 2.774
Epoch 207 Batch    5/215   train_loss = 3.200
Epoch 207 Batch   15/215   train_loss = 2.923
Epoch 207 Batch   25/215   train_loss = 2.824
Epoch 207 Batch   35/215   train_loss = 2.952
Epoch 207 Batch   45/215   train_loss = 2.874
Epoch 207 Batch   55/215   train_loss = 2.802
Epoch 207 Batch   65/215   train_loss = 2.892
Epoch 207 Batch   75/215   train_loss = 2.659
Epoch 207 Batch   85/215   train_loss = 2.658
Epoch 207 Batch   95/215   train_loss = 2.952
Epoch 207 Batch  105/215   train_loss = 2.663
Epoch 207 Batch  115/215   train_loss = 2.675
Epoch 207 Batch  125/215   train_loss = 2.880
Epoch 207 Batch  135/215   train_loss = 2.723
Epoch 207 Batch  145/215   train_loss = 2.924
Epoch 207 Batch  155/215   train_loss = 2.927
Epoch 207 Batch  165/215   train_loss = 2.729
Epoch 207 Batch  175/215   train_loss = 2.924
Epoch 207 Batch  185/215   train_loss = 3.094
Epoch 207 Batch  195/215   train_loss = 2.838
Epoch 207 Batch  205/215   train_loss = 2.882
Epoch 208 Batch    0/215   train_loss = 2.881
Epoch 208 Batch   10/215   train_loss = 2.926
Epoch 208 Batch   20/215   train_loss = 2.934
Epoch 208 Batch   30/215   train_loss = 2.795
Epoch 208 Batch   40/215   train_loss = 2.847
Epoch 208 Batch   50/215   train_loss = 2.687
Epoch 208 Batch   60/215   train_loss = 2.997
Epoch 208 Batch   70/215   train_loss = 2.811
Epoch 208 Batch   80/215   train_loss = 2.839
Epoch 208 Batch   90/215   train_loss = 2.806
Epoch 208 Batch  100/215   train_loss = 2.902
Epoch 208 Batch  110/215   train_loss = 2.921
Epoch 208 Batch  120/215   train_loss = 2.894
Epoch 208 Batch  130/215   train_loss = 2.950
Epoch 208 Batch  140/215   train_loss = 3.052
Epoch 208 Batch  150/215   train_loss = 2.815
Epoch 208 Batch  160/215   train_loss = 2.736
Epoch 208 Batch  170/215   train_loss = 2.697
Epoch 208 Batch  180/215   train_loss = 2.876
Epoch 208 Batch  190/215   train_loss = 2.896
Epoch 208 Batch  200/215   train_loss = 2.685
Epoch 208 Batch  210/215   train_loss = 2.801
Epoch 209 Batch    5/215   train_loss = 3.198
Epoch 209 Batch   15/215   train_loss = 2.925
Epoch 209 Batch   25/215   train_loss = 2.879
Epoch 209 Batch   35/215   train_loss = 2.777
Epoch 209 Batch   45/215   train_loss = 2.781
Epoch 209 Batch   55/215   train_loss = 2.868
Epoch 209 Batch   65/215   train_loss = 2.918
Epoch 209 Batch   75/215   train_loss = 2.802
Epoch 209 Batch   85/215   train_loss = 2.765
Epoch 209 Batch   95/215   train_loss = 2.932
Epoch 209 Batch  105/215   train_loss = 2.732
Epoch 209 Batch  115/215   train_loss = 2.646
Epoch 209 Batch  125/215   train_loss = 2.755
Epoch 209 Batch  135/215   train_loss = 2.792
Epoch 209 Batch  145/215   train_loss = 2.894
Epoch 209 Batch  155/215   train_loss = 2.864
Epoch 209 Batch  165/215   train_loss = 3.115
Epoch 209 Batch  175/215   train_loss = 2.931
Epoch 209 Batch  185/215   train_loss = 2.685
Epoch 209 Batch  195/215   train_loss = 2.949
Epoch 209 Batch  205/215   train_loss = 2.900
Epoch 210 Batch    0/215   train_loss = 3.035
Epoch 210 Batch   10/215   train_loss = 2.915
Epoch 210 Batch   20/215   train_loss = 2.901
Epoch 210 Batch   30/215   train_loss = 2.728
Epoch 210 Batch   40/215   train_loss = 2.727
Epoch 210 Batch   50/215   train_loss = 2.698
Epoch 210 Batch   60/215   train_loss = 2.980
Epoch 210 Batch   70/215   train_loss = 2.566
Epoch 210 Batch   80/215   train_loss = 2.811
Epoch 210 Batch   90/215   train_loss = 2.989
Epoch 210 Batch  100/215   train_loss = 2.884
Epoch 210 Batch  110/215   train_loss = 2.863
Epoch 210 Batch  120/215   train_loss = 2.831
Epoch 210 Batch  130/215   train_loss = 3.109
Epoch 210 Batch  140/215   train_loss = 2.805
Epoch 210 Batch  150/215   train_loss = 3.038
Epoch 210 Batch  160/215   train_loss = 2.890
Epoch 210 Batch  170/215   train_loss = 2.730
Epoch 210 Batch  180/215   train_loss = 2.870
Epoch 210 Batch  190/215   train_loss = 2.917
Epoch 210 Batch  200/215   train_loss = 2.761
Epoch 210 Batch  210/215   train_loss = 2.606
Epoch 211 Batch    5/215   train_loss = 2.988
Epoch 211 Batch   15/215   train_loss = 2.763
Epoch 211 Batch   25/215   train_loss = 2.748
Epoch 211 Batch   35/215   train_loss = 2.958
Epoch 211 Batch   45/215   train_loss = 2.702
Epoch 211 Batch   55/215   train_loss = 3.201
Epoch 211 Batch   65/215   train_loss = 2.907
Epoch 211 Batch   75/215   train_loss = 2.817
Epoch 211 Batch   85/215   train_loss = 2.822
Epoch 211 Batch   95/215   train_loss = 2.856
Epoch 211 Batch  105/215   train_loss = 2.741
Epoch 211 Batch  115/215   train_loss = 2.765
Epoch 211 Batch  125/215   train_loss = 2.974
Epoch 211 Batch  135/215   train_loss = 2.908
Epoch 211 Batch  145/215   train_loss = 2.767
Epoch 211 Batch  155/215   train_loss = 2.814
Epoch 211 Batch  165/215   train_loss = 2.793
Epoch 211 Batch  175/215   train_loss = 3.128
Epoch 211 Batch  185/215   train_loss = 2.987
Epoch 211 Batch  195/215   train_loss = 2.796
Epoch 211 Batch  205/215   train_loss = 2.784
Epoch 212 Batch    0/215   train_loss = 2.976
Epoch 212 Batch   10/215   train_loss = 3.040
Epoch 212 Batch   20/215   train_loss = 2.883
Epoch 212 Batch   30/215   train_loss = 2.599
Epoch 212 Batch   40/215   train_loss = 2.840
Epoch 212 Batch   50/215   train_loss = 2.786
Epoch 212 Batch   60/215   train_loss = 2.873
Epoch 212 Batch   70/215   train_loss = 2.823
Epoch 212 Batch   80/215   train_loss = 2.774
Epoch 212 Batch   90/215   train_loss = 2.790
Epoch 212 Batch  100/215   train_loss = 2.931
Epoch 212 Batch  110/215   train_loss = 2.985
Epoch 212 Batch  120/215   train_loss = 2.867
Epoch 212 Batch  130/215   train_loss = 3.013
Epoch 212 Batch  140/215   train_loss = 2.930
Epoch 212 Batch  150/215   train_loss = 2.770
Epoch 212 Batch  160/215   train_loss = 2.902
Epoch 212 Batch  170/215   train_loss = 2.855
Epoch 212 Batch  180/215   train_loss = 3.026
Epoch 212 Batch  190/215   train_loss = 2.979
Epoch 212 Batch  200/215   train_loss = 2.770
Epoch 212 Batch  210/215   train_loss = 2.778
Epoch 213 Batch    5/215   train_loss = 3.007
Epoch 213 Batch   15/215   train_loss = 2.765
Epoch 213 Batch   25/215   train_loss = 2.816
Epoch 213 Batch   35/215   train_loss = 2.858
Epoch 213 Batch   45/215   train_loss = 2.860
Epoch 213 Batch   55/215   train_loss = 2.791
Epoch 213 Batch   65/215   train_loss = 2.828
Epoch 213 Batch   75/215   train_loss = 2.839
Epoch 213 Batch   85/215   train_loss = 2.745
Epoch 213 Batch   95/215   train_loss = 2.796
Epoch 213 Batch  105/215   train_loss = 2.666
Epoch 213 Batch  115/215   train_loss = 2.881
Epoch 213 Batch  125/215   train_loss = 2.791
Epoch 213 Batch  135/215   train_loss = 2.663
Epoch 213 Batch  145/215   train_loss = 3.075
Epoch 213 Batch  155/215   train_loss = 2.966
Epoch 213 Batch  165/215   train_loss = 2.783
Epoch 213 Batch  175/215   train_loss = 3.036
Epoch 213 Batch  185/215   train_loss = 2.907
Epoch 213 Batch  195/215   train_loss = 2.761
Epoch 213 Batch  205/215   train_loss = 2.758
Epoch 214 Batch    0/215   train_loss = 2.741
Epoch 214 Batch   10/215   train_loss = 3.125
Epoch 214 Batch   20/215   train_loss = 2.913
Epoch 214 Batch   30/215   train_loss = 2.575
Epoch 214 Batch   40/215   train_loss = 3.008
Epoch 214 Batch   50/215   train_loss = 2.766
Epoch 214 Batch   60/215   train_loss = 3.223
Epoch 214 Batch   70/215   train_loss = 2.793
Epoch 214 Batch   80/215   train_loss = 2.978
Epoch 214 Batch   90/215   train_loss = 3.036
Epoch 214 Batch  100/215   train_loss = 2.934
Epoch 214 Batch  110/215   train_loss = 3.013
Epoch 214 Batch  120/215   train_loss = 2.879
Epoch 214 Batch  130/215   train_loss = 3.024
Epoch 214 Batch  140/215   train_loss = 2.727
Epoch 214 Batch  150/215   train_loss = 2.835
Epoch 214 Batch  160/215   train_loss = 2.889
Epoch 214 Batch  170/215   train_loss = 2.816
Epoch 214 Batch  180/215   train_loss = 2.846
Epoch 214 Batch  190/215   train_loss = 2.858
Epoch 214 Batch  200/215   train_loss = 2.965
Epoch 214 Batch  210/215   train_loss = 2.687
Epoch 215 Batch    5/215   train_loss = 2.961
Epoch 215 Batch   15/215   train_loss = 2.881
Epoch 215 Batch   25/215   train_loss = 2.839
Epoch 215 Batch   35/215   train_loss = 3.012
Epoch 215 Batch   45/215   train_loss = 2.985
Epoch 215 Batch   55/215   train_loss = 2.750
Epoch 215 Batch   65/215   train_loss = 3.052
Epoch 215 Batch   75/215   train_loss = 2.826
Epoch 215 Batch   85/215   train_loss = 2.795
Epoch 215 Batch   95/215   train_loss = 2.837
Epoch 215 Batch  105/215   train_loss = 2.560
Epoch 215 Batch  115/215   train_loss = 2.684
Epoch 215 Batch  125/215   train_loss = 2.686
Epoch 215 Batch  135/215   train_loss = 2.921
Epoch 215 Batch  145/215   train_loss = 2.964
Epoch 215 Batch  155/215   train_loss = 2.723
Epoch 215 Batch  165/215   train_loss = 2.777
Epoch 215 Batch  175/215   train_loss = 2.866
Epoch 215 Batch  185/215   train_loss = 3.081
Epoch 215 Batch  195/215   train_loss = 2.942
Epoch 215 Batch  205/215   train_loss = 2.668
Epoch 216 Batch    0/215   train_loss = 2.865
Epoch 216 Batch   10/215   train_loss = 2.971
Epoch 216 Batch   20/215   train_loss = 2.929
Epoch 216 Batch   30/215   train_loss = 2.775
Epoch 216 Batch   40/215   train_loss = 2.994
Epoch 216 Batch   50/215   train_loss = 2.771
Epoch 216 Batch   60/215   train_loss = 3.006
Epoch 216 Batch   70/215   train_loss = 2.786
Epoch 216 Batch   80/215   train_loss = 2.892
Epoch 216 Batch   90/215   train_loss = 2.871
Epoch 216 Batch  100/215   train_loss = 2.834
Epoch 216 Batch  110/215   train_loss = 2.780
Epoch 216 Batch  120/215   train_loss = 3.145
Epoch 216 Batch  130/215   train_loss = 3.073
Epoch 216 Batch  140/215   train_loss = 2.674
Epoch 216 Batch  150/215   train_loss = 2.745
Epoch 216 Batch  160/215   train_loss = 2.955
Epoch 216 Batch  170/215   train_loss = 2.796
Epoch 216 Batch  180/215   train_loss = 2.817
Epoch 216 Batch  190/215   train_loss = 2.853
Epoch 216 Batch  200/215   train_loss = 2.926
Epoch 216 Batch  210/215   train_loss = 2.655
Epoch 217 Batch    5/215   train_loss = 2.862
Epoch 217 Batch   15/215   train_loss = 2.738
Epoch 217 Batch   25/215   train_loss = 2.804
Epoch 217 Batch   35/215   train_loss = 2.958
Epoch 217 Batch   45/215   train_loss = 3.040
Epoch 217 Batch   55/215   train_loss = 2.703
Epoch 217 Batch   65/215   train_loss = 2.789
Epoch 217 Batch   75/215   train_loss = 2.834
Epoch 217 Batch   85/215   train_loss = 2.621
Epoch 217 Batch   95/215   train_loss = 2.641
Epoch 217 Batch  105/215   train_loss = 2.782
Epoch 217 Batch  115/215   train_loss = 2.746
Epoch 217 Batch  125/215   train_loss = 2.782
Epoch 217 Batch  135/215   train_loss = 2.955
Epoch 217 Batch  145/215   train_loss = 2.830
Epoch 217 Batch  155/215   train_loss = 2.969
Epoch 217 Batch  165/215   train_loss = 2.749
Epoch 217 Batch  175/215   train_loss = 3.022
Epoch 217 Batch  185/215   train_loss = 2.900
Epoch 217 Batch  195/215   train_loss = 2.882
Epoch 217 Batch  205/215   train_loss = 2.818
Epoch 218 Batch    0/215   train_loss = 2.918
Epoch 218 Batch   10/215   train_loss = 3.008
Epoch 218 Batch   20/215   train_loss = 2.834
Epoch 218 Batch   30/215   train_loss = 2.771
Epoch 218 Batch   40/215   train_loss = 2.803
Epoch 218 Batch   50/215   train_loss = 2.752
Epoch 218 Batch   60/215   train_loss = 2.813
Epoch 218 Batch   70/215   train_loss = 2.744
Epoch 218 Batch   80/215   train_loss = 2.879
Epoch 218 Batch   90/215   train_loss = 2.881
Epoch 218 Batch  100/215   train_loss = 2.830
Epoch 218 Batch  110/215   train_loss = 2.951
Epoch 218 Batch  120/215   train_loss = 2.754
Epoch 218 Batch  130/215   train_loss = 3.008
Epoch 218 Batch  140/215   train_loss = 2.969
Epoch 218 Batch  150/215   train_loss = 2.805
Epoch 218 Batch  160/215   train_loss = 2.885
Epoch 218 Batch  170/215   train_loss = 2.774
Epoch 218 Batch  180/215   train_loss = 2.867
Epoch 218 Batch  190/215   train_loss = 2.837
Epoch 218 Batch  200/215   train_loss = 2.787
Epoch 218 Batch  210/215   train_loss = 2.828
Epoch 219 Batch    5/215   train_loss = 2.871
Epoch 219 Batch   15/215   train_loss = 2.744
Epoch 219 Batch   25/215   train_loss = 2.794
Epoch 219 Batch   35/215   train_loss = 2.997
Epoch 219 Batch   45/215   train_loss = 3.078
Epoch 219 Batch   55/215   train_loss = 2.835
Epoch 219 Batch   65/215   train_loss = 2.863
Epoch 219 Batch   75/215   train_loss = 2.832
Epoch 219 Batch   85/215   train_loss = 2.735
Epoch 219 Batch   95/215   train_loss = 2.806
Epoch 219 Batch  105/215   train_loss = 2.910
Epoch 219 Batch  115/215   train_loss = 2.696
Epoch 219 Batch  125/215   train_loss = 2.582
Epoch 219 Batch  135/215   train_loss = 2.972
Epoch 219 Batch  145/215   train_loss = 2.957
Epoch 219 Batch  155/215   train_loss = 2.960
Epoch 219 Batch  165/215   train_loss = 2.715
Epoch 219 Batch  175/215   train_loss = 3.052
Epoch 219 Batch  185/215   train_loss = 2.878
Epoch 219 Batch  195/215   train_loss = 2.796
Epoch 219 Batch  205/215   train_loss = 2.827
Epoch 220 Batch    0/215   train_loss = 2.844
Epoch 220 Batch   10/215   train_loss = 3.295
Epoch 220 Batch   20/215   train_loss = 2.729
Epoch 220 Batch   30/215   train_loss = 2.740
Epoch 220 Batch   40/215   train_loss = 2.784
Epoch 220 Batch   50/215   train_loss = 2.754
Epoch 220 Batch   60/215   train_loss = 2.866
Epoch 220 Batch   70/215   train_loss = 2.666
Epoch 220 Batch   80/215   train_loss = 2.845
Epoch 220 Batch   90/215   train_loss = 3.072
Epoch 220 Batch  100/215   train_loss = 2.771
Epoch 220 Batch  110/215   train_loss = 2.781
Epoch 220 Batch  120/215   train_loss = 2.778
Epoch 220 Batch  130/215   train_loss = 3.058
Epoch 220 Batch  140/215   train_loss = 2.793
Epoch 220 Batch  150/215   train_loss = 2.871
Epoch 220 Batch  160/215   train_loss = 2.905
Epoch 220 Batch  170/215   train_loss = 2.733
Epoch 220 Batch  180/215   train_loss = 2.798
Epoch 220 Batch  190/215   train_loss = 2.721
Epoch 220 Batch  200/215   train_loss = 2.976
Epoch 220 Batch  210/215   train_loss = 2.622
Epoch 221 Batch    5/215   train_loss = 2.912
Epoch 221 Batch   15/215   train_loss = 2.726
Epoch 221 Batch   25/215   train_loss = 2.889
Epoch 221 Batch   35/215   train_loss = 2.957
Epoch 221 Batch   45/215   train_loss = 3.019
Epoch 221 Batch   55/215   train_loss = 2.804
Epoch 221 Batch   65/215   train_loss = 2.882
Epoch 221 Batch   75/215   train_loss = 2.770
Epoch 221 Batch   85/215   train_loss = 2.555
Epoch 221 Batch   95/215   train_loss = 2.824
Epoch 221 Batch  105/215   train_loss = 2.629
Epoch 221 Batch  115/215   train_loss = 2.762
Epoch 221 Batch  125/215   train_loss = 2.839
Epoch 221 Batch  135/215   train_loss = 2.822
Epoch 221 Batch  145/215   train_loss = 2.970
Epoch 221 Batch  155/215   train_loss = 2.895
Epoch 221 Batch  165/215   train_loss = 2.853
Epoch 221 Batch  175/215   train_loss = 3.141
Epoch 221 Batch  185/215   train_loss = 2.862
Epoch 221 Batch  195/215   train_loss = 2.738
Epoch 221 Batch  205/215   train_loss = 2.741
Epoch 222 Batch    0/215   train_loss = 2.909
Epoch 222 Batch   10/215   train_loss = 2.963
Epoch 222 Batch   20/215   train_loss = 2.968
Epoch 222 Batch   30/215   train_loss = 2.691
Epoch 222 Batch   40/215   train_loss = 2.772
Epoch 222 Batch   50/215   train_loss = 2.631
Epoch 222 Batch   60/215   train_loss = 3.119
Epoch 222 Batch   70/215   train_loss = 2.846
Epoch 222 Batch   80/215   train_loss = 2.887
Epoch 222 Batch   90/215   train_loss = 2.890
Epoch 222 Batch  100/215   train_loss = 2.841
Epoch 222 Batch  110/215   train_loss = 2.703
Epoch 222 Batch  120/215   train_loss = 2.941
Epoch 222 Batch  130/215   train_loss = 3.016
Epoch 222 Batch  140/215   train_loss = 2.789
Epoch 222 Batch  150/215   train_loss = 2.881
Epoch 222 Batch  160/215   train_loss = 2.860
Epoch 222 Batch  170/215   train_loss = 2.767
Epoch 222 Batch  180/215   train_loss = 3.017
Epoch 222 Batch  190/215   train_loss = 2.930
Epoch 222 Batch  200/215   train_loss = 2.775
Epoch 222 Batch  210/215   train_loss = 2.683
Epoch 223 Batch    5/215   train_loss = 2.912
Epoch 223 Batch   15/215   train_loss = 2.802
Epoch 223 Batch   25/215   train_loss = 2.767
Epoch 223 Batch   35/215   train_loss = 2.948
Epoch 223 Batch   45/215   train_loss = 2.693
Epoch 223 Batch   55/215   train_loss = 2.837
Epoch 223 Batch   65/215   train_loss = 2.905
Epoch 223 Batch   75/215   train_loss = 2.766
Epoch 223 Batch   85/215   train_loss = 2.674
Epoch 223 Batch   95/215   train_loss = 2.701
Epoch 223 Batch  105/215   train_loss = 2.700
Epoch 223 Batch  115/215   train_loss = 2.635
Epoch 223 Batch  125/215   train_loss = 2.618
Epoch 223 Batch  135/215   train_loss = 2.564
Epoch 223 Batch  145/215   train_loss = 3.002
Epoch 223 Batch  155/215   train_loss = 2.921
Epoch 223 Batch  165/215   train_loss = 2.877
Epoch 223 Batch  175/215   train_loss = 3.138
Epoch 223 Batch  185/215   train_loss = 2.812
Epoch 223 Batch  195/215   train_loss = 2.772
Epoch 223 Batch  205/215   train_loss = 2.979
Epoch 224 Batch    0/215   train_loss = 2.954
Epoch 224 Batch   10/215   train_loss = 2.961
Epoch 224 Batch   20/215   train_loss = 2.900
Epoch 224 Batch   30/215   train_loss = 2.659
Epoch 224 Batch   40/215   train_loss = 2.799
Epoch 224 Batch   50/215   train_loss = 2.676
Epoch 224 Batch   60/215   train_loss = 3.128
Epoch 224 Batch   70/215   train_loss = 2.782
Epoch 224 Batch   80/215   train_loss = 2.939
Epoch 224 Batch   90/215   train_loss = 2.766
Epoch 224 Batch  100/215   train_loss = 2.728
Epoch 224 Batch  110/215   train_loss = 2.785
Epoch 224 Batch  120/215   train_loss = 2.748
Epoch 224 Batch  130/215   train_loss = 3.005
Epoch 224 Batch  140/215   train_loss = 2.781
Epoch 224 Batch  150/215   train_loss = 2.814
Epoch 224 Batch  160/215   train_loss = 2.975
Epoch 224 Batch  170/215   train_loss = 2.740
Epoch 224 Batch  180/215   train_loss = 2.924
Epoch 224 Batch  190/215   train_loss = 2.771
Epoch 224 Batch  200/215   train_loss = 2.804
Epoch 224 Batch  210/215   train_loss = 2.640
Epoch 225 Batch    5/215   train_loss = 2.999
Epoch 225 Batch   15/215   train_loss = 2.900
Epoch 225 Batch   25/215   train_loss = 2.640
Epoch 225 Batch   35/215   train_loss = 2.878
Epoch 225 Batch   45/215   train_loss = 2.878
Epoch 225 Batch   55/215   train_loss = 2.810
Epoch 225 Batch   65/215   train_loss = 2.706
Epoch 225 Batch   75/215   train_loss = 2.742
Epoch 225 Batch   85/215   train_loss = 2.648
Epoch 225 Batch   95/215   train_loss = 2.757
Epoch 225 Batch  105/215   train_loss = 2.679
Epoch 225 Batch  115/215   train_loss = 2.767
Epoch 225 Batch  125/215   train_loss = 2.854
Epoch 225 Batch  135/215   train_loss = 2.868
Epoch 225 Batch  145/215   train_loss = 2.828
Epoch 225 Batch  155/215   train_loss = 2.907
Epoch 225 Batch  165/215   train_loss = 2.790
Epoch 225 Batch  175/215   train_loss = 2.842
Epoch 225 Batch  185/215   train_loss = 2.809
Epoch 225 Batch  195/215   train_loss = 2.851
Epoch 225 Batch  205/215   train_loss = 2.847
Epoch 226 Batch    0/215   train_loss = 2.935
Epoch 226 Batch   10/215   train_loss = 2.963
Epoch 226 Batch   20/215   train_loss = 2.708
Epoch 226 Batch   30/215   train_loss = 2.663
Epoch 226 Batch   40/215   train_loss = 2.846
Epoch 226 Batch   50/215   train_loss = 2.889
Epoch 226 Batch   60/215   train_loss = 2.931
Epoch 226 Batch   70/215   train_loss = 2.667
Epoch 226 Batch   80/215   train_loss = 2.747
Epoch 226 Batch   90/215   train_loss = 3.005
Epoch 226 Batch  100/215   train_loss = 2.904
Epoch 226 Batch  110/215   train_loss = 2.845
Epoch 226 Batch  120/215   train_loss = 2.913
Epoch 226 Batch  130/215   train_loss = 2.984
Epoch 226 Batch  140/215   train_loss = 3.172
Epoch 226 Batch  150/215   train_loss = 2.952
Epoch 226 Batch  160/215   train_loss = 2.962
Epoch 226 Batch  170/215   train_loss = 2.645
Epoch 226 Batch  180/215   train_loss = 2.976
Epoch 226 Batch  190/215   train_loss = 2.978
Epoch 226 Batch  200/215   train_loss = 2.681
Epoch 226 Batch  210/215   train_loss = 2.654
Epoch 227 Batch    5/215   train_loss = 2.946
Epoch 227 Batch   15/215   train_loss = 2.908
Epoch 227 Batch   25/215   train_loss = 2.838
Epoch 227 Batch   35/215   train_loss = 2.967
Epoch 227 Batch   45/215   train_loss = 2.816
Epoch 227 Batch   55/215   train_loss = 2.754
Epoch 227 Batch   65/215   train_loss = 2.849
Epoch 227 Batch   75/215   train_loss = 2.845
Epoch 227 Batch   85/215   train_loss = 2.776
Epoch 227 Batch   95/215   train_loss = 2.748
Epoch 227 Batch  105/215   train_loss = 2.809
Epoch 227 Batch  115/215   train_loss = 2.747
Epoch 227 Batch  125/215   train_loss = 2.642
Epoch 227 Batch  135/215   train_loss = 2.784
Epoch 227 Batch  145/215   train_loss = 2.741
Epoch 227 Batch  155/215   train_loss = 2.698
Epoch 227 Batch  165/215   train_loss = 2.924
Epoch 227 Batch  175/215   train_loss = 2.983
Epoch 227 Batch  185/215   train_loss = 2.854
Epoch 227 Batch  195/215   train_loss = 2.672
Epoch 227 Batch  205/215   train_loss = 2.722
Epoch 228 Batch    0/215   train_loss = 3.011
Epoch 228 Batch   10/215   train_loss = 2.754
Epoch 228 Batch   20/215   train_loss = 2.812
Epoch 228 Batch   30/215   train_loss = 2.658
Epoch 228 Batch   40/215   train_loss = 2.982
Epoch 228 Batch   50/215   train_loss = 2.746
Epoch 228 Batch   60/215   train_loss = 2.986
Epoch 228 Batch   70/215   train_loss = 2.744
Epoch 228 Batch   80/215   train_loss = 2.815
Epoch 228 Batch   90/215   train_loss = 3.087
Epoch 228 Batch  100/215   train_loss = 2.955
Epoch 228 Batch  110/215   train_loss = 2.855
Epoch 228 Batch  120/215   train_loss = 2.796
Epoch 228 Batch  130/215   train_loss = 3.108
Epoch 228 Batch  140/215   train_loss = 2.902
Epoch 228 Batch  150/215   train_loss = 2.721
Epoch 228 Batch  160/215   train_loss = 2.817
Epoch 228 Batch  170/215   train_loss = 2.806
Epoch 228 Batch  180/215   train_loss = 2.795
Epoch 228 Batch  190/215   train_loss = 2.926
Epoch 228 Batch  200/215   train_loss = 2.655
Epoch 228 Batch  210/215   train_loss = 2.575
Epoch 229 Batch    5/215   train_loss = 3.240
Epoch 229 Batch   15/215   train_loss = 2.489
Epoch 229 Batch   25/215   train_loss = 2.775
Epoch 229 Batch   35/215   train_loss = 2.882
Epoch 229 Batch   45/215   train_loss = 2.743
Epoch 229 Batch   55/215   train_loss = 2.712
Epoch 229 Batch   65/215   train_loss = 2.875
Epoch 229 Batch   75/215   train_loss = 2.734
Epoch 229 Batch   85/215   train_loss = 2.919
Epoch 229 Batch   95/215   train_loss = 2.833
Epoch 229 Batch  105/215   train_loss = 2.585
Epoch 229 Batch  115/215   train_loss = 2.606
Epoch 229 Batch  125/215   train_loss = 2.637
Epoch 229 Batch  135/215   train_loss = 2.886
Epoch 229 Batch  145/215   train_loss = 2.794
Epoch 229 Batch  155/215   train_loss = 2.938
Epoch 229 Batch  165/215   train_loss = 2.777
Epoch 229 Batch  175/215   train_loss = 3.006
Epoch 229 Batch  185/215   train_loss = 2.949
Epoch 229 Batch  195/215   train_loss = 2.606
Epoch 229 Batch  205/215   train_loss = 2.740
Epoch 230 Batch    0/215   train_loss = 2.958
Epoch 230 Batch   10/215   train_loss = 2.898
Epoch 230 Batch   20/215   train_loss = 2.737
Epoch 230 Batch   30/215   train_loss = 2.684
Epoch 230 Batch   40/215   train_loss = 2.770
Epoch 230 Batch   50/215   train_loss = 2.681
Epoch 230 Batch   60/215   train_loss = 3.165
Epoch 230 Batch   70/215   train_loss = 2.646
Epoch 230 Batch   80/215   train_loss = 2.905
Epoch 230 Batch   90/215   train_loss = 2.864
Epoch 230 Batch  100/215   train_loss = 2.849
Epoch 230 Batch  110/215   train_loss = 2.722
Epoch 230 Batch  120/215   train_loss = 2.796
Epoch 230 Batch  130/215   train_loss = 2.860
Epoch 230 Batch  140/215   train_loss = 2.820
Epoch 230 Batch  150/215   train_loss = 2.804
Epoch 230 Batch  160/215   train_loss = 2.853
Epoch 230 Batch  170/215   train_loss = 2.827
Epoch 230 Batch  180/215   train_loss = 2.854
Epoch 230 Batch  190/215   train_loss = 3.024
Epoch 230 Batch  200/215   train_loss = 2.892
Epoch 230 Batch  210/215   train_loss = 2.730
Epoch 231 Batch    5/215   train_loss = 3.061
Epoch 231 Batch   15/215   train_loss = 2.897
Epoch 231 Batch   25/215   train_loss = 2.808
Epoch 231 Batch   35/215   train_loss = 2.916
Epoch 231 Batch   45/215   train_loss = 2.762
Epoch 231 Batch   55/215   train_loss = 2.897
Epoch 231 Batch   65/215   train_loss = 2.667
Epoch 231 Batch   75/215   train_loss = 2.699
Epoch 231 Batch   85/215   train_loss = 2.650
Epoch 231 Batch   95/215   train_loss = 2.980
Epoch 231 Batch  105/215   train_loss = 2.799
Epoch 231 Batch  115/215   train_loss = 2.525
Epoch 231 Batch  125/215   train_loss = 2.737
Epoch 231 Batch  135/215   train_loss = 2.830
Epoch 231 Batch  145/215   train_loss = 2.925
Epoch 231 Batch  155/215   train_loss = 2.923
Epoch 231 Batch  165/215   train_loss = 2.811
Epoch 231 Batch  175/215   train_loss = 3.089
Epoch 231 Batch  185/215   train_loss = 2.984
Epoch 231 Batch  195/215   train_loss = 2.793
Epoch 231 Batch  205/215   train_loss = 2.796
Epoch 232 Batch    0/215   train_loss = 2.947
Epoch 232 Batch   10/215   train_loss = 2.915
Epoch 232 Batch   20/215   train_loss = 2.797
Epoch 232 Batch   30/215   train_loss = 2.650
Epoch 232 Batch   40/215   train_loss = 2.784
Epoch 232 Batch   50/215   train_loss = 2.772
Epoch 232 Batch   60/215   train_loss = 3.029
Epoch 232 Batch   70/215   train_loss = 2.671
Epoch 232 Batch   80/215   train_loss = 2.645
Epoch 232 Batch   90/215   train_loss = 3.093
Epoch 232 Batch  100/215   train_loss = 2.986
Epoch 232 Batch  110/215   train_loss = 2.825
Epoch 232 Batch  120/215   train_loss = 2.932
Epoch 232 Batch  130/215   train_loss = 3.112
Epoch 232 Batch  140/215   train_loss = 2.778
Epoch 232 Batch  150/215   train_loss = 2.797
Epoch 232 Batch  160/215   train_loss = 3.050
Epoch 232 Batch  170/215   train_loss = 2.790
Epoch 232 Batch  180/215   train_loss = 2.964
Epoch 232 Batch  190/215   train_loss = 2.732
Epoch 232 Batch  200/215   train_loss = 2.744
Epoch 232 Batch  210/215   train_loss = 2.834
Epoch 233 Batch    5/215   train_loss = 2.867
Epoch 233 Batch   15/215   train_loss = 2.881
Epoch 233 Batch   25/215   train_loss = 2.586
Epoch 233 Batch   35/215   train_loss = 2.750
Epoch 233 Batch   45/215   train_loss = 2.881
Epoch 233 Batch   55/215   train_loss = 2.759
Epoch 233 Batch   65/215   train_loss = 2.890
Epoch 233 Batch   75/215   train_loss = 2.923
Epoch 233 Batch   85/215   train_loss = 2.721
Epoch 233 Batch   95/215   train_loss = 2.900
Epoch 233 Batch  105/215   train_loss = 2.926
Epoch 233 Batch  115/215   train_loss = 2.699
Epoch 233 Batch  125/215   train_loss = 2.836
Epoch 233 Batch  135/215   train_loss = 2.845
Epoch 233 Batch  145/215   train_loss = 2.811
Epoch 233 Batch  155/215   train_loss = 2.939
Epoch 233 Batch  165/215   train_loss = 2.945
Epoch 233 Batch  175/215   train_loss = 3.033
Epoch 233 Batch  185/215   train_loss = 2.794
Epoch 233 Batch  195/215   train_loss = 2.847
Epoch 233 Batch  205/215   train_loss = 2.909
Epoch 234 Batch    0/215   train_loss = 2.842
Epoch 234 Batch   10/215   train_loss = 2.963
Epoch 234 Batch   20/215   train_loss = 2.875
Epoch 234 Batch   30/215   train_loss = 2.659
Epoch 234 Batch   40/215   train_loss = 2.663
Epoch 234 Batch   50/215   train_loss = 2.939
Epoch 234 Batch   60/215   train_loss = 2.774
Epoch 234 Batch   70/215   train_loss = 2.619
Epoch 234 Batch   80/215   train_loss = 2.951
Epoch 234 Batch   90/215   train_loss = 3.068
Epoch 234 Batch  100/215   train_loss = 2.845
Epoch 234 Batch  110/215   train_loss = 2.671
Epoch 234 Batch  120/215   train_loss = 2.857
Epoch 234 Batch  130/215   train_loss = 2.965
Epoch 234 Batch  140/215   train_loss = 2.772
Epoch 234 Batch  150/215   train_loss = 2.816
Epoch 234 Batch  160/215   train_loss = 2.807
Epoch 234 Batch  170/215   train_loss = 2.805
Epoch 234 Batch  180/215   train_loss = 2.877
Epoch 234 Batch  190/215   train_loss = 2.861
Epoch 234 Batch  200/215   train_loss = 2.955
Epoch 234 Batch  210/215   train_loss = 2.822
Epoch 235 Batch    5/215   train_loss = 2.980
Epoch 235 Batch   15/215   train_loss = 2.761
Epoch 235 Batch   25/215   train_loss = 2.634
Epoch 235 Batch   35/215   train_loss = 2.829
Epoch 235 Batch   45/215   train_loss = 2.778
Epoch 235 Batch   55/215   train_loss = 2.836
Epoch 235 Batch   65/215   train_loss = 3.049
Epoch 235 Batch   75/215   train_loss = 2.707
Epoch 235 Batch   85/215   train_loss = 2.862
Epoch 235 Batch   95/215   train_loss = 2.715
Epoch 235 Batch  105/215   train_loss = 2.702
Epoch 235 Batch  115/215   train_loss = 2.625
Epoch 235 Batch  125/215   train_loss = 2.702
Epoch 235 Batch  135/215   train_loss = 2.688
Epoch 235 Batch  145/215   train_loss = 2.932
Epoch 235 Batch  155/215   train_loss = 2.864
Epoch 235 Batch  165/215   train_loss = 2.626
Epoch 235 Batch  175/215   train_loss = 2.995
Epoch 235 Batch  185/215   train_loss = 3.034
Epoch 235 Batch  195/215   train_loss = 2.751
Epoch 235 Batch  205/215   train_loss = 2.826
Epoch 236 Batch    0/215   train_loss = 2.772
Epoch 236 Batch   10/215   train_loss = 2.947
Epoch 236 Batch   20/215   train_loss = 2.972
Epoch 236 Batch   30/215   train_loss = 2.670
Epoch 236 Batch   40/215   train_loss = 2.989
Epoch 236 Batch   50/215   train_loss = 2.621
Epoch 236 Batch   60/215   train_loss = 2.958
Epoch 236 Batch   70/215   train_loss = 2.669
Epoch 236 Batch   80/215   train_loss = 2.938
Epoch 236 Batch   90/215   train_loss = 2.878
Epoch 236 Batch  100/215   train_loss = 2.982
Epoch 236 Batch  110/215   train_loss = 2.791
Epoch 236 Batch  120/215   train_loss = 3.139
Epoch 236 Batch  130/215   train_loss = 3.057
Epoch 236 Batch  140/215   train_loss = 2.863
Epoch 236 Batch  150/215   train_loss = 2.855
Epoch 236 Batch  160/215   train_loss = 2.914
Epoch 236 Batch  170/215   train_loss = 2.626
Epoch 236 Batch  180/215   train_loss = 2.905
Epoch 236 Batch  190/215   train_loss = 2.772
Epoch 236 Batch  200/215   train_loss = 2.717
Epoch 236 Batch  210/215   train_loss = 2.653
Epoch 237 Batch    5/215   train_loss = 2.931
Epoch 237 Batch   15/215   train_loss = 2.992
Epoch 237 Batch   25/215   train_loss = 2.728
Epoch 237 Batch   35/215   train_loss = 2.962
Epoch 237 Batch   45/215   train_loss = 2.878
Epoch 237 Batch   55/215   train_loss = 2.654
Epoch 237 Batch   65/215   train_loss = 2.665
Epoch 237 Batch   75/215   train_loss = 2.842
Epoch 237 Batch   85/215   train_loss = 2.532
Epoch 237 Batch   95/215   train_loss = 2.940
Epoch 237 Batch  105/215   train_loss = 2.690
Epoch 237 Batch  115/215   train_loss = 2.610
Epoch 237 Batch  125/215   train_loss = 2.684
Epoch 237 Batch  135/215   train_loss = 2.704
Epoch 237 Batch  145/215   train_loss = 3.102
Epoch 237 Batch  155/215   train_loss = 2.809
Epoch 237 Batch  165/215   train_loss = 2.932
Epoch 237 Batch  175/215   train_loss = 2.994
Epoch 237 Batch  185/215   train_loss = 2.931
Epoch 237 Batch  195/215   train_loss = 2.691
Epoch 237 Batch  205/215   train_loss = 2.715
Epoch 238 Batch    0/215   train_loss = 2.945
Epoch 238 Batch   10/215   train_loss = 2.814
Epoch 238 Batch   20/215   train_loss = 3.055
Epoch 238 Batch   30/215   train_loss = 2.733
Epoch 238 Batch   40/215   train_loss = 2.820
Epoch 238 Batch   50/215   train_loss = 2.733
Epoch 238 Batch   60/215   train_loss = 2.946
Epoch 238 Batch   70/215   train_loss = 2.632
Epoch 238 Batch   80/215   train_loss = 2.786
Epoch 238 Batch   90/215   train_loss = 3.044
Epoch 238 Batch  100/215   train_loss = 2.747
Epoch 238 Batch  110/215   train_loss = 2.684
Epoch 238 Batch  120/215   train_loss = 2.780
Epoch 238 Batch  130/215   train_loss = 3.113
Epoch 238 Batch  140/215   train_loss = 2.784
Epoch 238 Batch  150/215   train_loss = 2.874
Epoch 238 Batch  160/215   train_loss = 2.982
Epoch 238 Batch  170/215   train_loss = 2.702
Epoch 238 Batch  180/215   train_loss = 2.722
Epoch 238 Batch  190/215   train_loss = 2.869
Epoch 238 Batch  200/215   train_loss = 2.591
Epoch 238 Batch  210/215   train_loss = 2.715
Epoch 239 Batch    5/215   train_loss = 2.968
Epoch 239 Batch   15/215   train_loss = 2.816
Epoch 239 Batch   25/215   train_loss = 2.753
Epoch 239 Batch   35/215   train_loss = 3.119
Epoch 239 Batch   45/215   train_loss = 2.914
Epoch 239 Batch   55/215   train_loss = 2.973
Epoch 239 Batch   65/215   train_loss = 2.873
Epoch 239 Batch   75/215   train_loss = 2.699
Epoch 239 Batch   85/215   train_loss = 2.668
Epoch 239 Batch   95/215   train_loss = 2.871
Epoch 239 Batch  105/215   train_loss = 2.870
Epoch 239 Batch  115/215   train_loss = 2.762
Epoch 239 Batch  125/215   train_loss = 2.686
Epoch 239 Batch  135/215   train_loss = 2.787
Epoch 239 Batch  145/215   train_loss = 2.809
Epoch 239 Batch  155/215   train_loss = 2.815
Epoch 239 Batch  165/215   train_loss = 2.757
Epoch 239 Batch  175/215   train_loss = 3.054
Epoch 239 Batch  185/215   train_loss = 2.942
Epoch 239 Batch  195/215   train_loss = 2.760
Epoch 239 Batch  205/215   train_loss = 2.596
Epoch 240 Batch    0/215   train_loss = 2.862
Epoch 240 Batch   10/215   train_loss = 3.015
Epoch 240 Batch   20/215   train_loss = 2.716
Epoch 240 Batch   30/215   train_loss = 2.777
Epoch 240 Batch   40/215   train_loss = 2.805
Epoch 240 Batch   50/215   train_loss = 2.776
Epoch 240 Batch   60/215   train_loss = 2.742
Epoch 240 Batch   70/215   train_loss = 2.755
Epoch 240 Batch   80/215   train_loss = 2.891
Epoch 240 Batch   90/215   train_loss = 3.117
Epoch 240 Batch  100/215   train_loss = 2.802
Epoch 240 Batch  110/215   train_loss = 2.796
Epoch 240 Batch  120/215   train_loss = 2.714
Epoch 240 Batch  130/215   train_loss = 2.876
Epoch 240 Batch  140/215   train_loss = 2.676
Epoch 240 Batch  150/215   train_loss = 2.727
Epoch 240 Batch  160/215   train_loss = 2.859
Epoch 240 Batch  170/215   train_loss = 2.749
Epoch 240 Batch  180/215   train_loss = 2.889
Epoch 240 Batch  190/215   train_loss = 2.837
Epoch 240 Batch  200/215   train_loss = 2.729
Epoch 240 Batch  210/215   train_loss = 2.725
Epoch 241 Batch    5/215   train_loss = 3.159
Epoch 241 Batch   15/215   train_loss = 2.742
Epoch 241 Batch   25/215   train_loss = 2.768
Epoch 241 Batch   35/215   train_loss = 2.910
Epoch 241 Batch   45/215   train_loss = 2.777
Epoch 241 Batch   55/215   train_loss = 2.588
Epoch 241 Batch   65/215   train_loss = 2.987
Epoch 241 Batch   75/215   train_loss = 2.586
Epoch 241 Batch   85/215   train_loss = 2.825
Epoch 241 Batch   95/215   train_loss = 2.848
Epoch 241 Batch  105/215   train_loss = 2.672
Epoch 241 Batch  115/215   train_loss = 2.557
Epoch 241 Batch  125/215   train_loss = 2.700
Epoch 241 Batch  135/215   train_loss = 2.902
Epoch 241 Batch  145/215   train_loss = 2.944
Epoch 241 Batch  155/215   train_loss = 2.817
Epoch 241 Batch  165/215   train_loss = 2.831
Epoch 241 Batch  175/215   train_loss = 2.935
Epoch 241 Batch  185/215   train_loss = 3.058
Epoch 241 Batch  195/215   train_loss = 2.739
Epoch 241 Batch  205/215   train_loss = 2.647
Epoch 242 Batch    0/215   train_loss = 2.891
Epoch 242 Batch   10/215   train_loss = 2.908
Epoch 242 Batch   20/215   train_loss = 2.851
Epoch 242 Batch   30/215   train_loss = 2.547
Epoch 242 Batch   40/215   train_loss = 2.887
Epoch 242 Batch   50/215   train_loss = 2.788
Epoch 242 Batch   60/215   train_loss = 2.795
Epoch 242 Batch   70/215   train_loss = 2.664
Epoch 242 Batch   80/215   train_loss = 2.794
Epoch 242 Batch   90/215   train_loss = 2.806
Epoch 242 Batch  100/215   train_loss = 2.936
Epoch 242 Batch  110/215   train_loss = 2.616
Epoch 242 Batch  120/215   train_loss = 2.821
Epoch 242 Batch  130/215   train_loss = 2.817
Epoch 242 Batch  140/215   train_loss = 2.686
Epoch 242 Batch  150/215   train_loss = 2.600
Epoch 242 Batch  160/215   train_loss = 3.028
Epoch 242 Batch  170/215   train_loss = 2.752
Epoch 242 Batch  180/215   train_loss = 2.914
Epoch 242 Batch  190/215   train_loss = 2.838
Epoch 242 Batch  200/215   train_loss = 2.681
Epoch 242 Batch  210/215   train_loss = 2.772
Epoch 243 Batch    5/215   train_loss = 3.041
Epoch 243 Batch   15/215   train_loss = 2.584
Epoch 243 Batch   25/215   train_loss = 2.770
Epoch 243 Batch   35/215   train_loss = 2.861
Epoch 243 Batch   45/215   train_loss = 2.804
Epoch 243 Batch   55/215   train_loss = 2.751
Epoch 243 Batch   65/215   train_loss = 2.871
Epoch 243 Batch   75/215   train_loss = 3.103
Epoch 243 Batch   85/215   train_loss = 2.934
Epoch 243 Batch   95/215   train_loss = 2.819
Epoch 243 Batch  105/215   train_loss = 2.770
Epoch 243 Batch  115/215   train_loss = 2.655
Epoch 243 Batch  125/215   train_loss = 2.632
Epoch 243 Batch  135/215   train_loss = 2.861
Epoch 243 Batch  145/215   train_loss = 2.779
Epoch 243 Batch  155/215   train_loss = 2.851
Epoch 243 Batch  165/215   train_loss = 2.819
Epoch 243 Batch  175/215   train_loss = 3.074
Epoch 243 Batch  185/215   train_loss = 2.909
Epoch 243 Batch  195/215   train_loss = 2.614
Epoch 243 Batch  205/215   train_loss = 2.723
Epoch 244 Batch    0/215   train_loss = 2.901
Epoch 244 Batch   10/215   train_loss = 3.013
Epoch 244 Batch   20/215   train_loss = 2.750
Epoch 244 Batch   30/215   train_loss = 2.688
Epoch 244 Batch   40/215   train_loss = 2.730
Epoch 244 Batch   50/215   train_loss = 2.688
Epoch 244 Batch   60/215   train_loss = 2.903
Epoch 244 Batch   70/215   train_loss = 2.591
Epoch 244 Batch   80/215   train_loss = 2.921
Epoch 244 Batch   90/215   train_loss = 2.805
Epoch 244 Batch  100/215   train_loss = 2.861
Epoch 244 Batch  110/215   train_loss = 2.816
Epoch 244 Batch  120/215   train_loss = 2.814
Epoch 244 Batch  130/215   train_loss = 2.878
Epoch 244 Batch  140/215   train_loss = 2.766
Epoch 244 Batch  150/215   train_loss = 3.023
Epoch 244 Batch  160/215   train_loss = 2.874
Epoch 244 Batch  170/215   train_loss = 2.765
Epoch 244 Batch  180/215   train_loss = 3.002
Epoch 244 Batch  190/215   train_loss = 2.794
Epoch 244 Batch  200/215   train_loss = 2.693
Epoch 244 Batch  210/215   train_loss = 2.664
Epoch 245 Batch    5/215   train_loss = 2.922
Epoch 245 Batch   15/215   train_loss = 2.836
Epoch 245 Batch   25/215   train_loss = 2.839
Epoch 245 Batch   35/215   train_loss = 2.834
Epoch 245 Batch   45/215   train_loss = 2.866
Epoch 245 Batch   55/215   train_loss = 2.737
Epoch 245 Batch   65/215   train_loss = 3.060
Epoch 245 Batch   75/215   train_loss = 2.749
Epoch 245 Batch   85/215   train_loss = 2.698
Epoch 245 Batch   95/215   train_loss = 2.727
Epoch 245 Batch  105/215   train_loss = 2.581
Epoch 245 Batch  115/215   train_loss = 2.715
Epoch 245 Batch  125/215   train_loss = 2.800
Epoch 245 Batch  135/215   train_loss = 2.723
Epoch 245 Batch  145/215   train_loss = 2.803
Epoch 245 Batch  155/215   train_loss = 2.914
Epoch 245 Batch  165/215   train_loss = 2.869
Epoch 245 Batch  175/215   train_loss = 3.161
Epoch 245 Batch  185/215   train_loss = 2.934
Epoch 245 Batch  195/215   train_loss = 2.769
Epoch 245 Batch  205/215   train_loss = 2.884
Epoch 246 Batch    0/215   train_loss = 2.970
Epoch 246 Batch   10/215   train_loss = 2.816
Epoch 246 Batch   20/215   train_loss = 2.802
Epoch 246 Batch   30/215   train_loss = 2.757
Epoch 246 Batch   40/215   train_loss = 2.741
Epoch 246 Batch   50/215   train_loss = 2.625
Epoch 246 Batch   60/215   train_loss = 2.721
Epoch 246 Batch   70/215   train_loss = 2.628
Epoch 246 Batch   80/215   train_loss = 2.979
Epoch 246 Batch   90/215   train_loss = 2.799
Epoch 246 Batch  100/215   train_loss = 2.947
Epoch 246 Batch  110/215   train_loss = 2.782
Epoch 246 Batch  120/215   train_loss = 2.708
Epoch 246 Batch  130/215   train_loss = 2.942
Epoch 246 Batch  140/215   train_loss = 2.964
Epoch 246 Batch  150/215   train_loss = 2.968
Epoch 246 Batch  160/215   train_loss = 2.963
Epoch 246 Batch  170/215   train_loss = 2.851
Epoch 246 Batch  180/215   train_loss = 3.011
Epoch 246 Batch  190/215   train_loss = 3.025
Epoch 246 Batch  200/215   train_loss = 2.827
Epoch 246 Batch  210/215   train_loss = 2.726
Epoch 247 Batch    5/215   train_loss = 2.795
Epoch 247 Batch   15/215   train_loss = 2.846
Epoch 247 Batch   25/215   train_loss = 2.777
Epoch 247 Batch   35/215   train_loss = 2.879
Epoch 247 Batch   45/215   train_loss = 2.923
Epoch 247 Batch   55/215   train_loss = 2.852
Epoch 247 Batch   65/215   train_loss = 2.819
Epoch 247 Batch   75/215   train_loss = 2.785
Epoch 247 Batch   85/215   train_loss = 2.745
Epoch 247 Batch   95/215   train_loss = 2.735
Epoch 247 Batch  105/215   train_loss = 2.729
Epoch 247 Batch  115/215   train_loss = 2.704
Epoch 247 Batch  125/215   train_loss = 2.647
Epoch 247 Batch  135/215   train_loss = 2.725
Epoch 247 Batch  145/215   train_loss = 2.712
Epoch 247 Batch  155/215   train_loss = 3.042
Epoch 247 Batch  165/215   train_loss = 2.712
Epoch 247 Batch  175/215   train_loss = 3.055
Epoch 247 Batch  185/215   train_loss = 2.813
Epoch 247 Batch  195/215   train_loss = 2.771
Epoch 247 Batch  205/215   train_loss = 2.897
Epoch 248 Batch    0/215   train_loss = 2.837
Epoch 248 Batch   10/215   train_loss = 3.079
Epoch 248 Batch   20/215   train_loss = 2.844
Epoch 248 Batch   30/215   train_loss = 2.619
Epoch 248 Batch   40/215   train_loss = 2.629
Epoch 248 Batch   50/215   train_loss = 2.703
Epoch 248 Batch   60/215   train_loss = 2.934
Epoch 248 Batch   70/215   train_loss = 2.692
Epoch 248 Batch   80/215   train_loss = 2.616
Epoch 248 Batch   90/215   train_loss = 2.815
Epoch 248 Batch  100/215   train_loss = 2.964
Epoch 248 Batch  110/215   train_loss = 2.798
Epoch 248 Batch  120/215   train_loss = 2.814
Epoch 248 Batch  130/215   train_loss = 3.103
Epoch 248 Batch  140/215   train_loss = 2.677
Epoch 248 Batch  150/215   train_loss = 2.832
Epoch 248 Batch  160/215   train_loss = 2.839
Epoch 248 Batch  170/215   train_loss = 2.768
Epoch 248 Batch  180/215   train_loss = 2.956
Epoch 248 Batch  190/215   train_loss = 2.841
Epoch 248 Batch  200/215   train_loss = 2.819
Epoch 248 Batch  210/215   train_loss = 2.741
Epoch 249 Batch    5/215   train_loss = 3.180
Epoch 249 Batch   15/215   train_loss = 2.747
Epoch 249 Batch   25/215   train_loss = 2.928
Epoch 249 Batch   35/215   train_loss = 2.941
Epoch 249 Batch   45/215   train_loss = 2.890
Epoch 249 Batch   55/215   train_loss = 2.648
Epoch 249 Batch   65/215   train_loss = 2.757
Epoch 249 Batch   75/215   train_loss = 2.480
Epoch 249 Batch   85/215   train_loss = 2.587
Epoch 249 Batch   95/215   train_loss = 2.668
Epoch 249 Batch  105/215   train_loss = 2.619
Epoch 249 Batch  115/215   train_loss = 2.550
Epoch 249 Batch  125/215   train_loss = 2.633
Epoch 249 Batch  135/215   train_loss = 2.880
Epoch 249 Batch  145/215   train_loss = 2.852
Epoch 249 Batch  155/215   train_loss = 2.820
Epoch 249 Batch  165/215   train_loss = 2.662
Epoch 249 Batch  175/215   train_loss = 3.128
Epoch 249 Batch  185/215   train_loss = 2.808
Epoch 249 Batch  195/215   train_loss = 2.781
Epoch 249 Batch  205/215   train_loss = 2.773
Epoch 250 Batch    0/215   train_loss = 2.958
Epoch 250 Batch   10/215   train_loss = 2.974
Epoch 250 Batch   20/215   train_loss = 2.952
Epoch 250 Batch   30/215   train_loss = 2.524
Epoch 250 Batch   40/215   train_loss = 2.667
Epoch 250 Batch   50/215   train_loss = 2.877
Epoch 250 Batch   60/215   train_loss = 2.864
Epoch 250 Batch   70/215   train_loss = 2.598
Epoch 250 Batch   80/215   train_loss = 2.708
Epoch 250 Batch   90/215   train_loss = 2.977
Epoch 250 Batch  100/215   train_loss = 2.922
Epoch 250 Batch  110/215   train_loss = 2.957
Epoch 250 Batch  120/215   train_loss = 2.733
Epoch 250 Batch  130/215   train_loss = 2.968
Epoch 250 Batch  140/215   train_loss = 2.907
Epoch 250 Batch  150/215   train_loss = 2.842
Epoch 250 Batch  160/215   train_loss = 2.831
Epoch 250 Batch  170/215   train_loss = 2.772
Epoch 250 Batch  180/215   train_loss = 2.916
Epoch 250 Batch  190/215   train_loss = 2.858
Epoch 250 Batch  200/215   train_loss = 2.954
Epoch 250 Batch  210/215   train_loss = 3.015
Epoch 251 Batch    5/215   train_loss = 3.015
Epoch 251 Batch   15/215   train_loss = 2.652
Epoch 251 Batch   25/215   train_loss = 2.679
Epoch 251 Batch   35/215   train_loss = 2.644
Epoch 251 Batch   45/215   train_loss = 2.841
Epoch 251 Batch   55/215   train_loss = 2.813
Epoch 251 Batch   65/215   train_loss = 2.777
Epoch 251 Batch   75/215   train_loss = 2.879
Epoch 251 Batch   85/215   train_loss = 2.917
Epoch 251 Batch   95/215   train_loss = 2.721
Epoch 251 Batch  105/215   train_loss = 2.863
Epoch 251 Batch  115/215   train_loss = 2.784
Epoch 251 Batch  125/215   train_loss = 2.814
Epoch 251 Batch  135/215   train_loss = 2.708
Epoch 251 Batch  145/215   train_loss = 2.687
Epoch 251 Batch  155/215   train_loss = 2.848
Epoch 251 Batch  165/215   train_loss = 2.907
Epoch 251 Batch  175/215   train_loss = 3.095
Epoch 251 Batch  185/215   train_loss = 2.815
Epoch 251 Batch  195/215   train_loss = 2.832
Epoch 251 Batch  205/215   train_loss = 2.914
Epoch 252 Batch    0/215   train_loss = 2.918
Epoch 252 Batch   10/215   train_loss = 2.996
Epoch 252 Batch   20/215   train_loss = 2.758
Epoch 252 Batch   30/215   train_loss = 2.773
Epoch 252 Batch   40/215   train_loss = 2.699
Epoch 252 Batch   50/215   train_loss = 2.780
Epoch 252 Batch   60/215   train_loss = 2.864
Epoch 252 Batch   70/215   train_loss = 2.747
Epoch 252 Batch   80/215   train_loss = 2.696
Epoch 252 Batch   90/215   train_loss = 2.820
Epoch 252 Batch  100/215   train_loss = 2.805
Epoch 252 Batch  110/215   train_loss = 2.947
Epoch 252 Batch  120/215   train_loss = 2.826
Epoch 252 Batch  130/215   train_loss = 3.005
Epoch 252 Batch  140/215   train_loss = 2.699
Epoch 252 Batch  150/215   train_loss = 2.787
Epoch 252 Batch  160/215   train_loss = 2.815
Epoch 252 Batch  170/215   train_loss = 2.756
Epoch 252 Batch  180/215   train_loss = 2.683
Epoch 252 Batch  190/215   train_loss = 2.841
Epoch 252 Batch  200/215   train_loss = 2.858
Epoch 252 Batch  210/215   train_loss = 2.619
Epoch 253 Batch    5/215   train_loss = 2.829
Epoch 253 Batch   15/215   train_loss = 2.829
Epoch 253 Batch   25/215   train_loss = 2.745
Epoch 253 Batch   35/215   train_loss = 2.857
Epoch 253 Batch   45/215   train_loss = 2.816
Epoch 253 Batch   55/215   train_loss = 2.789
Epoch 253 Batch   65/215   train_loss = 2.611
Epoch 253 Batch   75/215   train_loss = 2.624
Epoch 253 Batch   85/215   train_loss = 2.769
Epoch 253 Batch   95/215   train_loss = 2.703
Epoch 253 Batch  105/215   train_loss = 2.632
Epoch 253 Batch  115/215   train_loss = 2.623
Epoch 253 Batch  125/215   train_loss = 2.662
Epoch 253 Batch  135/215   train_loss = 2.662
Epoch 253 Batch  145/215   train_loss = 2.798
Epoch 253 Batch  155/215   train_loss = 2.818
Epoch 253 Batch  165/215   train_loss = 2.691
Epoch 253 Batch  175/215   train_loss = 3.036
Epoch 253 Batch  185/215   train_loss = 2.763
Epoch 253 Batch  195/215   train_loss = 2.772
Epoch 253 Batch  205/215   train_loss = 2.812
Epoch 254 Batch    0/215   train_loss = 2.854
Epoch 254 Batch   10/215   train_loss = 2.937
Epoch 254 Batch   20/215   train_loss = 2.924
Epoch 254 Batch   30/215   train_loss = 2.758
Epoch 254 Batch   40/215   train_loss = 2.764
Epoch 254 Batch   50/215   train_loss = 2.811
Epoch 254 Batch   60/215   train_loss = 2.847
Epoch 254 Batch   70/215   train_loss = 2.710
Epoch 254 Batch   80/215   train_loss = 2.811
Epoch 254 Batch   90/215   train_loss = 2.989
Epoch 254 Batch  100/215   train_loss = 2.809
Epoch 254 Batch  110/215   train_loss = 2.761
Epoch 254 Batch  120/215   train_loss = 2.827
Epoch 254 Batch  130/215   train_loss = 3.104
Epoch 254 Batch  140/215   train_loss = 2.838
Epoch 254 Batch  150/215   train_loss = 2.705
Epoch 254 Batch  160/215   train_loss = 2.876
Epoch 254 Batch  170/215   train_loss = 2.750
Epoch 254 Batch  180/215   train_loss = 2.812
Epoch 254 Batch  190/215   train_loss = 2.837
Epoch 254 Batch  200/215   train_loss = 2.746
Epoch 254 Batch  210/215   train_loss = 2.559
Epoch 255 Batch    5/215   train_loss = 2.837
Epoch 255 Batch   15/215   train_loss = 2.720
Epoch 255 Batch   25/215   train_loss = 2.763
Epoch 255 Batch   35/215   train_loss = 2.743
Epoch 255 Batch   45/215   train_loss = 2.812
Epoch 255 Batch   55/215   train_loss = 2.824
Epoch 255 Batch   65/215   train_loss = 2.767
Epoch 255 Batch   75/215   train_loss = 2.665
Epoch 255 Batch   85/215   train_loss = 2.592
Epoch 255 Batch   95/215   train_loss = 2.631
Epoch 255 Batch  105/215   train_loss = 2.815
Epoch 255 Batch  115/215   train_loss = 2.689
Epoch 255 Batch  125/215   train_loss = 2.704
Epoch 255 Batch  135/215   train_loss = 2.721
Epoch 255 Batch  145/215   train_loss = 2.760
Epoch 255 Batch  155/215   train_loss = 2.864
Epoch 255 Batch  165/215   train_loss = 2.906
Epoch 255 Batch  175/215   train_loss = 3.080
Epoch 255 Batch  185/215   train_loss = 2.812
Epoch 255 Batch  195/215   train_loss = 2.674
Epoch 255 Batch  205/215   train_loss = 2.720
Epoch 256 Batch    0/215   train_loss = 2.887
Epoch 256 Batch   10/215   train_loss = 2.820
Epoch 256 Batch   20/215   train_loss = 2.895
Epoch 256 Batch   30/215   train_loss = 2.776
Epoch 256 Batch   40/215   train_loss = 2.873
Epoch 256 Batch   50/215   train_loss = 2.619
Epoch 256 Batch   60/215   train_loss = 2.922
Epoch 256 Batch   70/215   train_loss = 2.541
Epoch 256 Batch   80/215   train_loss = 2.814
Epoch 256 Batch   90/215   train_loss = 3.110
Epoch 256 Batch  100/215   train_loss = 2.711
Epoch 256 Batch  110/215   train_loss = 2.708
Epoch 256 Batch  120/215   train_loss = 2.706
Epoch 256 Batch  130/215   train_loss = 3.049
Epoch 256 Batch  140/215   train_loss = 2.823
Epoch 256 Batch  150/215   train_loss = 3.037
Epoch 256 Batch  160/215   train_loss = 2.781
Epoch 256 Batch  170/215   train_loss = 2.852
Epoch 256 Batch  180/215   train_loss = 2.964
Epoch 256 Batch  190/215   train_loss = 2.624
Epoch 256 Batch  200/215   train_loss = 2.810
Epoch 256 Batch  210/215   train_loss = 2.830
Epoch 257 Batch    5/215   train_loss = 3.021
Epoch 257 Batch   15/215   train_loss = 2.689
Epoch 257 Batch   25/215   train_loss = 2.584
Epoch 257 Batch   35/215   train_loss = 2.945
Epoch 257 Batch   45/215   train_loss = 3.008
Epoch 257 Batch   55/215   train_loss = 2.656
Epoch 257 Batch   65/215   train_loss = 2.851
Epoch 257 Batch   75/215   train_loss = 2.823
Epoch 257 Batch   85/215   train_loss = 2.781
Epoch 257 Batch   95/215   train_loss = 2.786
Epoch 257 Batch  105/215   train_loss = 2.698
Epoch 257 Batch  115/215   train_loss = 2.661
Epoch 257 Batch  125/215   train_loss = 2.631
Epoch 257 Batch  135/215   train_loss = 2.776
Epoch 257 Batch  145/215   train_loss = 2.773
Epoch 257 Batch  155/215   train_loss = 2.855
Epoch 257 Batch  165/215   train_loss = 2.825
Epoch 257 Batch  175/215   train_loss = 2.780
Epoch 257 Batch  185/215   train_loss = 2.673
Epoch 257 Batch  195/215   train_loss = 2.675
Epoch 257 Batch  205/215   train_loss = 2.850
Epoch 258 Batch    0/215   train_loss = 2.723
Epoch 258 Batch   10/215   train_loss = 2.871
Epoch 258 Batch   20/215   train_loss = 2.941
Epoch 258 Batch   30/215   train_loss = 2.584
Epoch 258 Batch   40/215   train_loss = 2.763
Epoch 258 Batch   50/215   train_loss = 2.826
Epoch 258 Batch   60/215   train_loss = 2.915
Epoch 258 Batch   70/215   train_loss = 2.629
Epoch 258 Batch   80/215   train_loss = 2.774
Epoch 258 Batch   90/215   train_loss = 2.928
Epoch 258 Batch  100/215   train_loss = 2.881
Epoch 258 Batch  110/215   train_loss = 2.800
Epoch 258 Batch  120/215   train_loss = 2.853
Epoch 258 Batch  130/215   train_loss = 2.921
Epoch 258 Batch  140/215   train_loss = 2.842
Epoch 258 Batch  150/215   train_loss = 2.874
Epoch 258 Batch  160/215   train_loss = 2.964
Epoch 258 Batch  170/215   train_loss = 2.662
Epoch 258 Batch  180/215   train_loss = 2.944
Epoch 258 Batch  190/215   train_loss = 2.850
Epoch 258 Batch  200/215   train_loss = 2.648
Epoch 258 Batch  210/215   train_loss = 2.707
Epoch 259 Batch    5/215   train_loss = 2.885
Epoch 259 Batch   15/215   train_loss = 2.821
Epoch 259 Batch   25/215   train_loss = 2.766
Epoch 259 Batch   35/215   train_loss = 2.820
Epoch 259 Batch   45/215   train_loss = 2.696
Epoch 259 Batch   55/215   train_loss = 2.665
Epoch 259 Batch   65/215   train_loss = 2.785
Epoch 259 Batch   75/215   train_loss = 2.734
Epoch 259 Batch   85/215   train_loss = 2.601
Epoch 259 Batch   95/215   train_loss = 2.680
Epoch 259 Batch  105/215   train_loss = 2.651
Epoch 259 Batch  115/215   train_loss = 2.699
Epoch 259 Batch  125/215   train_loss = 3.048
Epoch 259 Batch  135/215   train_loss = 2.787
Epoch 259 Batch  145/215   train_loss = 2.799
Epoch 259 Batch  155/215   train_loss = 2.850
Epoch 259 Batch  165/215   train_loss = 2.801
Epoch 259 Batch  175/215   train_loss = 2.881
Epoch 259 Batch  185/215   train_loss = 2.750
Epoch 259 Batch  195/215   train_loss = 2.641
Epoch 259 Batch  205/215   train_loss = 2.783
Epoch 260 Batch    0/215   train_loss = 2.994
Epoch 260 Batch   10/215   train_loss = 2.891
Epoch 260 Batch   20/215   train_loss = 2.778
Epoch 260 Batch   30/215   train_loss = 2.538
Epoch 260 Batch   40/215   train_loss = 2.788
Epoch 260 Batch   50/215   train_loss = 2.575
Epoch 260 Batch   60/215   train_loss = 2.940
Epoch 260 Batch   70/215   train_loss = 2.788
Epoch 260 Batch   80/215   train_loss = 2.767
Epoch 260 Batch   90/215   train_loss = 2.828
Epoch 260 Batch  100/215   train_loss = 3.059
Epoch 260 Batch  110/215   train_loss = 2.787
Epoch 260 Batch  120/215   train_loss = 2.881
Epoch 260 Batch  130/215   train_loss = 3.180
Epoch 260 Batch  140/215   train_loss = 2.771
Epoch 260 Batch  150/215   train_loss = 2.855
Epoch 260 Batch  160/215   train_loss = 2.898
Epoch 260 Batch  170/215   train_loss = 2.919
Epoch 260 Batch  180/215   train_loss = 2.811
Epoch 260 Batch  190/215   train_loss = 2.710
Epoch 260 Batch  200/215   train_loss = 2.557
Epoch 260 Batch  210/215   train_loss = 2.623
Epoch 261 Batch    5/215   train_loss = 2.877
Epoch 261 Batch   15/215   train_loss = 2.657
Epoch 261 Batch   25/215   train_loss = 2.651
Epoch 261 Batch   35/215   train_loss = 2.928
Epoch 261 Batch   45/215   train_loss = 2.838
Epoch 261 Batch   55/215   train_loss = 2.542
Epoch 261 Batch   65/215   train_loss = 2.976
Epoch 261 Batch   75/215   train_loss = 2.749
Epoch 261 Batch   85/215   train_loss = 2.636
Epoch 261 Batch   95/215   train_loss = 2.766
Epoch 261 Batch  105/215   train_loss = 2.606
Epoch 261 Batch  115/215   train_loss = 2.672
Epoch 261 Batch  125/215   train_loss = 2.808
Epoch 261 Batch  135/215   train_loss = 2.913
Epoch 261 Batch  145/215   train_loss = 2.629
Epoch 261 Batch  155/215   train_loss = 3.066
Epoch 261 Batch  165/215   train_loss = 2.705
Epoch 261 Batch  175/215   train_loss = 2.946
Epoch 261 Batch  185/215   train_loss = 2.837
Epoch 261 Batch  195/215   train_loss = 2.714
Epoch 261 Batch  205/215   train_loss = 2.668
Epoch 262 Batch    0/215   train_loss = 2.799
Epoch 262 Batch   10/215   train_loss = 2.911
Epoch 262 Batch   20/215   train_loss = 2.785
Epoch 262 Batch   30/215   train_loss = 2.472
Epoch 262 Batch   40/215   train_loss = 2.774
Epoch 262 Batch   50/215   train_loss = 2.657
Epoch 262 Batch   60/215   train_loss = 2.840
Epoch 262 Batch   70/215   train_loss = 2.611
Epoch 262 Batch   80/215   train_loss = 2.816
Epoch 262 Batch   90/215   train_loss = 2.994
Epoch 262 Batch  100/215   train_loss = 2.708
Epoch 262 Batch  110/215   train_loss = 2.814
Epoch 262 Batch  120/215   train_loss = 3.013
Epoch 262 Batch  130/215   train_loss = 3.061
Epoch 262 Batch  140/215   train_loss = 2.821
Epoch 262 Batch  150/215   train_loss = 2.686
Epoch 262 Batch  160/215   train_loss = 2.912
Epoch 262 Batch  170/215   train_loss = 2.677
Epoch 262 Batch  180/215   train_loss = 2.740
Epoch 262 Batch  190/215   train_loss = 2.917
Epoch 262 Batch  200/215   train_loss = 2.764
Epoch 262 Batch  210/215   train_loss = 2.662
Epoch 263 Batch    5/215   train_loss = 2.904
Epoch 263 Batch   15/215   train_loss = 2.647
Epoch 263 Batch   25/215   train_loss = 2.668
Epoch 263 Batch   35/215   train_loss = 2.858
Epoch 263 Batch   45/215   train_loss = 2.862
Epoch 263 Batch   55/215   train_loss = 2.772
Epoch 263 Batch   65/215   train_loss = 2.882
Epoch 263 Batch   75/215   train_loss = 2.766
Epoch 263 Batch   85/215   train_loss = 2.592
Epoch 263 Batch   95/215   train_loss = 2.654
Epoch 263 Batch  105/215   train_loss = 2.568
Epoch 263 Batch  115/215   train_loss = 2.799
Epoch 263 Batch  125/215   train_loss = 2.749
Epoch 263 Batch  135/215   train_loss = 2.752
Epoch 263 Batch  145/215   train_loss = 2.787
Epoch 263 Batch  155/215   train_loss = 2.765
Epoch 263 Batch  165/215   train_loss = 2.784
Epoch 263 Batch  175/215   train_loss = 2.792
Epoch 263 Batch  185/215   train_loss = 2.834
Epoch 263 Batch  195/215   train_loss = 2.750
Epoch 263 Batch  205/215   train_loss = 2.719
Epoch 264 Batch    0/215   train_loss = 2.771
Epoch 264 Batch   10/215   train_loss = 2.840
Epoch 264 Batch   20/215   train_loss = 2.811
Epoch 264 Batch   30/215   train_loss = 2.779
Epoch 264 Batch   40/215   train_loss = 2.864
Epoch 264 Batch   50/215   train_loss = 2.875
Epoch 264 Batch   60/215   train_loss = 2.884
Epoch 264 Batch   70/215   train_loss = 2.650
Epoch 264 Batch   80/215   train_loss = 2.927
Epoch 264 Batch   90/215   train_loss = 2.829
Epoch 264 Batch  100/215   train_loss = 2.901
Epoch 264 Batch  110/215   train_loss = 2.811
Epoch 264 Batch  120/215   train_loss = 2.797
Epoch 264 Batch  130/215   train_loss = 3.041
Epoch 264 Batch  140/215   train_loss = 2.847
Epoch 264 Batch  150/215   train_loss = 2.823
Epoch 264 Batch  160/215   train_loss = 2.826
Epoch 264 Batch  170/215   train_loss = 2.633
Epoch 264 Batch  180/215   train_loss = 2.947
Epoch 264 Batch  190/215   train_loss = 2.669
Epoch 264 Batch  200/215   train_loss = 2.708
Epoch 264 Batch  210/215   train_loss = 2.724
Epoch 265 Batch    5/215   train_loss = 3.010
Epoch 265 Batch   15/215   train_loss = 2.618
Epoch 265 Batch   25/215   train_loss = 2.746
Epoch 265 Batch   35/215   train_loss = 3.452
Epoch 265 Batch   45/215   train_loss = 2.817
Epoch 265 Batch   55/215   train_loss = 2.623
Epoch 265 Batch   65/215   train_loss = 2.606
Epoch 265 Batch   75/215   train_loss = 2.668
Epoch 265 Batch   85/215   train_loss = 2.671
Epoch 265 Batch   95/215   train_loss = 2.649
Epoch 265 Batch  105/215   train_loss = 2.693
Epoch 265 Batch  115/215   train_loss = 2.734
Epoch 265 Batch  125/215   train_loss = 2.975
Epoch 265 Batch  135/215   train_loss = 2.779
Epoch 265 Batch  145/215   train_loss = 2.672
Epoch 265 Batch  155/215   train_loss = 2.812
Epoch 265 Batch  165/215   train_loss = 2.879
Epoch 265 Batch  175/215   train_loss = 2.914
Epoch 265 Batch  185/215   train_loss = 2.804
Epoch 265 Batch  195/215   train_loss = 2.971
Epoch 265 Batch  205/215   train_loss = 2.715
Epoch 266 Batch    0/215   train_loss = 2.758
Epoch 266 Batch   10/215   train_loss = 2.951
Epoch 266 Batch   20/215   train_loss = 2.835
Epoch 266 Batch   30/215   train_loss = 2.518
Epoch 266 Batch   40/215   train_loss = 2.667
Epoch 266 Batch   50/215   train_loss = 2.740
Epoch 266 Batch   60/215   train_loss = 3.019
Epoch 266 Batch   70/215   train_loss = 2.573
Epoch 266 Batch   80/215   train_loss = 2.807
Epoch 266 Batch   90/215   train_loss = 2.838
Epoch 266 Batch  100/215   train_loss = 2.803
Epoch 266 Batch  110/215   train_loss = 2.679
Epoch 266 Batch  120/215   train_loss = 2.937
Epoch 266 Batch  130/215   train_loss = 2.979
Epoch 266 Batch  140/215   train_loss = 2.868
Epoch 266 Batch  150/215   train_loss = 2.765
Epoch 266 Batch  160/215   train_loss = 3.051
Epoch 266 Batch  170/215   train_loss = 2.761
Epoch 266 Batch  180/215   train_loss = 2.978
Epoch 266 Batch  190/215   train_loss = 2.766
Epoch 266 Batch  200/215   train_loss = 2.911
Epoch 266 Batch  210/215   train_loss = 2.524
Epoch 267 Batch    5/215   train_loss = 2.752
Epoch 267 Batch   15/215   train_loss = 2.678
Epoch 267 Batch   25/215   train_loss = 2.636
Epoch 267 Batch   35/215   train_loss = 2.962
Epoch 267 Batch   45/215   train_loss = 2.732
Epoch 267 Batch   55/215   train_loss = 2.641
Epoch 267 Batch   65/215   train_loss = 2.695
Epoch 267 Batch   75/215   train_loss = 2.700
Epoch 267 Batch   85/215   train_loss = 2.898
Epoch 267 Batch   95/215   train_loss = 2.798
Epoch 267 Batch  105/215   train_loss = 2.793
Epoch 267 Batch  115/215   train_loss = 2.964
Epoch 267 Batch  125/215   train_loss = 2.833
Epoch 267 Batch  135/215   train_loss = 2.807
Epoch 267 Batch  145/215   train_loss = 2.634
Epoch 267 Batch  155/215   train_loss = 2.799
Epoch 267 Batch  165/215   train_loss = 2.861
Epoch 267 Batch  175/215   train_loss = 2.961
Epoch 267 Batch  185/215   train_loss = 2.772
Epoch 267 Batch  195/215   train_loss = 2.613
Epoch 267 Batch  205/215   train_loss = 2.706
Epoch 268 Batch    0/215   train_loss = 2.899
Epoch 268 Batch   10/215   train_loss = 2.708
Epoch 268 Batch   20/215   train_loss = 2.640
Epoch 268 Batch   30/215   train_loss = 2.594
Epoch 268 Batch   40/215   train_loss = 2.605
Epoch 268 Batch   50/215   train_loss = 2.704
Epoch 268 Batch   60/215   train_loss = 2.812
Epoch 268 Batch   70/215   train_loss = 2.894
Epoch 268 Batch   80/215   train_loss = 2.715
Epoch 268 Batch   90/215   train_loss = 2.859
Epoch 268 Batch  100/215   train_loss = 2.680
Epoch 268 Batch  110/215   train_loss = 2.751
Epoch 268 Batch  120/215   train_loss = 2.771
Epoch 268 Batch  130/215   train_loss = 2.969
Epoch 268 Batch  140/215   train_loss = 2.739
Epoch 268 Batch  150/215   train_loss = 2.832
Epoch 268 Batch  160/215   train_loss = 2.868
Epoch 268 Batch  170/215   train_loss = 2.725
Epoch 268 Batch  180/215   train_loss = 2.884
Epoch 268 Batch  190/215   train_loss = 2.708
Epoch 268 Batch  200/215   train_loss = 2.592
Epoch 268 Batch  210/215   train_loss = 2.653
Epoch 269 Batch    5/215   train_loss = 3.038
Epoch 269 Batch   15/215   train_loss = 2.595
Epoch 269 Batch   25/215   train_loss = 2.867
Epoch 269 Batch   35/215   train_loss = 2.868
Epoch 269 Batch   45/215   train_loss = 2.768
Epoch 269 Batch   55/215   train_loss = 2.716
Epoch 269 Batch   65/215   train_loss = 2.751
Epoch 269 Batch   75/215   train_loss = 2.637
Epoch 269 Batch   85/215   train_loss = 2.570
Epoch 269 Batch   95/215   train_loss = 2.621
Epoch 269 Batch  105/215   train_loss = 2.736
Epoch 269 Batch  115/215   train_loss = 2.782
Epoch 269 Batch  125/215   train_loss = 2.862
Epoch 269 Batch  135/215   train_loss = 2.873
Epoch 269 Batch  145/215   train_loss = 2.699
Epoch 269 Batch  155/215   train_loss = 3.127
Epoch 269 Batch  165/215   train_loss = 2.831
Epoch 269 Batch  175/215   train_loss = 2.997
Epoch 269 Batch  185/215   train_loss = 2.975
Epoch 269 Batch  195/215   train_loss = 2.934
Epoch 269 Batch  205/215   train_loss = 3.012
Epoch 270 Batch    0/215   train_loss = 2.791
Epoch 270 Batch   10/215   train_loss = 2.842
Epoch 270 Batch   20/215   train_loss = 2.858
Epoch 270 Batch   30/215   train_loss = 2.746
Epoch 270 Batch   40/215   train_loss = 2.716
Epoch 270 Batch   50/215   train_loss = 2.667
Epoch 270 Batch   60/215   train_loss = 2.764
Epoch 270 Batch   70/215   train_loss = 2.728
Epoch 270 Batch   80/215   train_loss = 2.999
Epoch 270 Batch   90/215   train_loss = 2.841
Epoch 270 Batch  100/215   train_loss = 2.925
Epoch 270 Batch  110/215   train_loss = 2.929
Epoch 270 Batch  120/215   train_loss = 2.637
Epoch 270 Batch  130/215   train_loss = 3.063
Epoch 270 Batch  140/215   train_loss = 2.905
Epoch 270 Batch  150/215   train_loss = 2.703
Epoch 270 Batch  160/215   train_loss = 2.815
Epoch 270 Batch  170/215   train_loss = 2.666
Epoch 270 Batch  180/215   train_loss = 2.835
Epoch 270 Batch  190/215   train_loss = 2.760
Epoch 270 Batch  200/215   train_loss = 2.817
Epoch 270 Batch  210/215   train_loss = 2.595
Epoch 271 Batch    5/215   train_loss = 2.785
Epoch 271 Batch   15/215   train_loss = 2.733
Epoch 271 Batch   25/215   train_loss = 2.891
Epoch 271 Batch   35/215   train_loss = 2.814
Epoch 271 Batch   45/215   train_loss = 2.813
Epoch 271 Batch   55/215   train_loss = 2.711
Epoch 271 Batch   65/215   train_loss = 2.723
Epoch 271 Batch   75/215   train_loss = 2.782
Epoch 271 Batch   85/215   train_loss = 2.641
Epoch 271 Batch   95/215   train_loss = 2.750
Epoch 271 Batch  105/215   train_loss = 2.573
Epoch 271 Batch  115/215   train_loss = 2.809
Epoch 271 Batch  125/215   train_loss = 2.705
Epoch 271 Batch  135/215   train_loss = 2.881
Epoch 271 Batch  145/215   train_loss = 2.727
Epoch 271 Batch  155/215   train_loss = 2.871
Epoch 271 Batch  165/215   train_loss = 3.029
Epoch 271 Batch  175/215   train_loss = 3.075
Epoch 271 Batch  185/215   train_loss = 2.827
Epoch 271 Batch  195/215   train_loss = 2.719
Epoch 271 Batch  205/215   train_loss = 2.762
Epoch 272 Batch    0/215   train_loss = 2.998
Epoch 272 Batch   10/215   train_loss = 3.114
Epoch 272 Batch   20/215   train_loss = 2.772
Epoch 272 Batch   30/215   train_loss = 2.642
Epoch 272 Batch   40/215   train_loss = 2.845
Epoch 272 Batch   50/215   train_loss = 2.807
Epoch 272 Batch   60/215   train_loss = 3.007
Epoch 272 Batch   70/215   train_loss = 2.803
Epoch 272 Batch   80/215   train_loss = 2.697
Epoch 272 Batch   90/215   train_loss = 2.696
Epoch 272 Batch  100/215   train_loss = 2.760
Epoch 272 Batch  110/215   train_loss = 2.842
Epoch 272 Batch  120/215   train_loss = 2.787
Epoch 272 Batch  130/215   train_loss = 2.950
Epoch 272 Batch  140/215   train_loss = 2.783
Epoch 272 Batch  150/215   train_loss = 2.716
Epoch 272 Batch  160/215   train_loss = 2.763
Epoch 272 Batch  170/215   train_loss = 2.992
Epoch 272 Batch  180/215   train_loss = 2.854
Epoch 272 Batch  190/215   train_loss = 2.898
Epoch 272 Batch  200/215   train_loss = 2.970
Epoch 272 Batch  210/215   train_loss = 2.625
Epoch 273 Batch    5/215   train_loss = 3.112
Epoch 273 Batch   15/215   train_loss = 2.810
Epoch 273 Batch   25/215   train_loss = 2.728
Epoch 273 Batch   35/215   train_loss = 2.961
Epoch 273 Batch   45/215   train_loss = 2.894
Epoch 273 Batch   55/215   train_loss = 2.779
Epoch 273 Batch   65/215   train_loss = 2.905
Epoch 273 Batch   75/215   train_loss = 2.736
Epoch 273 Batch   85/215   train_loss = 2.580
Epoch 273 Batch   95/215   train_loss = 2.777
Epoch 273 Batch  105/215   train_loss = 2.569
Epoch 273 Batch  115/215   train_loss = 2.658
Epoch 273 Batch  125/215   train_loss = 2.969
Epoch 273 Batch  135/215   train_loss = 2.706
Epoch 273 Batch  145/215   train_loss = 2.722
Epoch 273 Batch  155/215   train_loss = 3.003
Epoch 273 Batch  165/215   train_loss = 2.686
Epoch 273 Batch  175/215   train_loss = 3.094
Epoch 273 Batch  185/215   train_loss = 2.772
Epoch 273 Batch  195/215   train_loss = 2.802
Epoch 273 Batch  205/215   train_loss = 2.552
Epoch 274 Batch    0/215   train_loss = 2.973
Epoch 274 Batch   10/215   train_loss = 2.723
Epoch 274 Batch   20/215   train_loss = 2.578
Epoch 274 Batch   30/215   train_loss = 2.655
Epoch 274 Batch   40/215   train_loss = 2.667
Epoch 274 Batch   50/215   train_loss = 2.752
Epoch 274 Batch   60/215   train_loss = 3.154
Epoch 274 Batch   70/215   train_loss = 2.684
Epoch 274 Batch   80/215   train_loss = 3.026
Epoch 274 Batch   90/215   train_loss = 2.829
Epoch 274 Batch  100/215   train_loss = 2.791
Epoch 274 Batch  110/215   train_loss = 2.825
Epoch 274 Batch  120/215   train_loss = 2.797
Epoch 274 Batch  130/215   train_loss = 2.867
Epoch 274 Batch  140/215   train_loss = 2.719
Epoch 274 Batch  150/215   train_loss = 2.928
Epoch 274 Batch  160/215   train_loss = 3.087
Epoch 274 Batch  170/215   train_loss = 3.073
Epoch 274 Batch  180/215   train_loss = 2.844
Epoch 274 Batch  190/215   train_loss = 2.758
Epoch 274 Batch  200/215   train_loss = 2.647
Epoch 274 Batch  210/215   train_loss = 2.655
Epoch 275 Batch    5/215   train_loss = 2.923
Epoch 275 Batch   15/215   train_loss = 3.081
Epoch 275 Batch   25/215   train_loss = 2.664
Epoch 275 Batch   35/215   train_loss = 2.929
Epoch 275 Batch   45/215   train_loss = 2.833
Epoch 275 Batch   55/215   train_loss = 2.872
Epoch 275 Batch   65/215   train_loss = 2.616
Epoch 275 Batch   75/215   train_loss = 2.709
Epoch 275 Batch   85/215   train_loss = 2.568
Epoch 275 Batch   95/215   train_loss = 2.827
Epoch 275 Batch  105/215   train_loss = 2.610
Epoch 275 Batch  115/215   train_loss = 2.609
Epoch 275 Batch  125/215   train_loss = 2.800
Epoch 275 Batch  135/215   train_loss = 2.773
Epoch 275 Batch  145/215   train_loss = 2.649
Epoch 275 Batch  155/215   train_loss = 2.857
Epoch 275 Batch  165/215   train_loss = 2.663
Epoch 275 Batch  175/215   train_loss = 2.912
Epoch 275 Batch  185/215   train_loss = 2.919
Epoch 275 Batch  195/215   train_loss = 2.740
Epoch 275 Batch  205/215   train_loss = 2.860
Epoch 276 Batch    0/215   train_loss = 2.855
Epoch 276 Batch   10/215   train_loss = 2.779
Epoch 276 Batch   20/215   train_loss = 2.954
Epoch 276 Batch   30/215   train_loss = 2.665
Epoch 276 Batch   40/215   train_loss = 2.645
Epoch 276 Batch   50/215   train_loss = 2.658
Epoch 276 Batch   60/215   train_loss = 2.993
Epoch 276 Batch   70/215   train_loss = 2.799
Epoch 276 Batch   80/215   train_loss = 2.933
Epoch 276 Batch   90/215   train_loss = 3.088
Epoch 276 Batch  100/215   train_loss = 2.702
Epoch 276 Batch  110/215   train_loss = 2.905
Epoch 276 Batch  120/215   train_loss = 2.754
Epoch 276 Batch  130/215   train_loss = 3.095
Epoch 276 Batch  140/215   train_loss = 2.609
Epoch 276 Batch  150/215   train_loss = 2.985
Epoch 276 Batch  160/215   train_loss = 2.828
Epoch 276 Batch  170/215   train_loss = 2.756
Epoch 276 Batch  180/215   train_loss = 2.858
Epoch 276 Batch  190/215   train_loss = 2.673
Epoch 276 Batch  200/215   train_loss = 2.624
Epoch 276 Batch  210/215   train_loss = 2.620
Epoch 277 Batch    5/215   train_loss = 2.804
Epoch 277 Batch   15/215   train_loss = 2.795
Epoch 277 Batch   25/215   train_loss = 2.816
Epoch 277 Batch   35/215   train_loss = 2.807
Epoch 277 Batch   45/215   train_loss = 2.725
Epoch 277 Batch   55/215   train_loss = 2.707
Epoch 277 Batch   65/215   train_loss = 2.591
Epoch 277 Batch   75/215   train_loss = 2.807
Epoch 277 Batch   85/215   train_loss = 2.741
Epoch 277 Batch   95/215   train_loss = 2.773
Epoch 277 Batch  105/215   train_loss = 2.759
Epoch 277 Batch  115/215   train_loss = 2.564
Epoch 277 Batch  125/215   train_loss = 2.585
Epoch 277 Batch  135/215   train_loss = 2.864
Epoch 277 Batch  145/215   train_loss = 2.608
Epoch 277 Batch  155/215   train_loss = 2.993
Epoch 277 Batch  165/215   train_loss = 2.884
Epoch 277 Batch  175/215   train_loss = 3.150
Epoch 277 Batch  185/215   train_loss = 2.992
Epoch 277 Batch  195/215   train_loss = 2.616
Epoch 277 Batch  205/215   train_loss = 2.803
Epoch 278 Batch    0/215   train_loss = 2.859
Epoch 278 Batch   10/215   train_loss = 2.904
Epoch 278 Batch   20/215   train_loss = 2.754
Epoch 278 Batch   30/215   train_loss = 2.673
Epoch 278 Batch   40/215   train_loss = 2.897
Epoch 278 Batch   50/215   train_loss = 2.801
Epoch 278 Batch   60/215   train_loss = 2.819
Epoch 278 Batch   70/215   train_loss = 2.752
Epoch 278 Batch   80/215   train_loss = 2.833
Epoch 278 Batch   90/215   train_loss = 2.683
Epoch 278 Batch  100/215   train_loss = 2.708
Epoch 278 Batch  110/215   train_loss = 2.718
Epoch 278 Batch  120/215   train_loss = 2.827
Epoch 278 Batch  130/215   train_loss = 2.890
Epoch 278 Batch  140/215   train_loss = 2.790
Epoch 278 Batch  150/215   train_loss = 2.862
Epoch 278 Batch  160/215   train_loss = 2.820
Epoch 278 Batch  170/215   train_loss = 2.672
Epoch 278 Batch  180/215   train_loss = 2.777
Epoch 278 Batch  190/215   train_loss = 2.759
Epoch 278 Batch  200/215   train_loss = 2.864
Epoch 278 Batch  210/215   train_loss = 2.670
Epoch 279 Batch    5/215   train_loss = 2.766
Epoch 279 Batch   15/215   train_loss = 2.764
Epoch 279 Batch   25/215   train_loss = 2.610
Epoch 279 Batch   35/215   train_loss = 2.715
Epoch 279 Batch   45/215   train_loss = 2.787
Epoch 279 Batch   55/215   train_loss = 2.823
Epoch 279 Batch   65/215   train_loss = 2.686
Epoch 279 Batch   75/215   train_loss = 2.641
Epoch 279 Batch   85/215   train_loss = 2.710
Epoch 279 Batch   95/215   train_loss = 2.691
Epoch 279 Batch  105/215   train_loss = 2.552
Epoch 279 Batch  115/215   train_loss = 2.562
Epoch 279 Batch  125/215   train_loss = 2.700
Epoch 279 Batch  135/215   train_loss = 2.915
Epoch 279 Batch  145/215   train_loss = 2.800
Epoch 279 Batch  155/215   train_loss = 2.869
Epoch 279 Batch  165/215   train_loss = 2.941
Epoch 279 Batch  175/215   train_loss = 2.846
Epoch 279 Batch  185/215   train_loss = 2.651
Epoch 279 Batch  195/215   train_loss = 2.782
Epoch 279 Batch  205/215   train_loss = 2.690
Epoch 280 Batch    0/215   train_loss = 2.857
Epoch 280 Batch   10/215   train_loss = 2.783
Epoch 280 Batch   20/215   train_loss = 2.846
Epoch 280 Batch   30/215   train_loss = 2.688
Epoch 280 Batch   40/215   train_loss = 2.869
Epoch 280 Batch   50/215   train_loss = 2.728
Epoch 280 Batch   60/215   train_loss = 2.820
Epoch 280 Batch   70/215   train_loss = 2.764
Epoch 280 Batch   80/215   train_loss = 2.611
Epoch 280 Batch   90/215   train_loss = 2.803
Epoch 280 Batch  100/215   train_loss = 2.901
Epoch 280 Batch  110/215   train_loss = 2.683
Epoch 280 Batch  120/215   train_loss = 2.771
Epoch 280 Batch  130/215   train_loss = 3.000
Epoch 280 Batch  140/215   train_loss = 2.750
Epoch 280 Batch  150/215   train_loss = 2.792
Epoch 280 Batch  160/215   train_loss = 2.948
Epoch 280 Batch  170/215   train_loss = 2.657
Epoch 280 Batch  180/215   train_loss = 2.885
Epoch 280 Batch  190/215   train_loss = 2.937
Epoch 280 Batch  200/215   train_loss = 2.663
Epoch 280 Batch  210/215   train_loss = 2.728
Epoch 281 Batch    5/215   train_loss = 2.845
Epoch 281 Batch   15/215   train_loss = 2.695
Epoch 281 Batch   25/215   train_loss = 2.767
Epoch 281 Batch   35/215   train_loss = 2.875
Epoch 281 Batch   45/215   train_loss = 2.716
Epoch 281 Batch   55/215   train_loss = 2.780
Epoch 281 Batch   65/215   train_loss = 2.922
Epoch 281 Batch   75/215   train_loss = 2.694
Epoch 281 Batch   85/215   train_loss = 2.778
Epoch 281 Batch   95/215   train_loss = 2.654
Epoch 281 Batch  105/215   train_loss = 2.763
Epoch 281 Batch  115/215   train_loss = 2.511
Epoch 281 Batch  125/215   train_loss = 2.870
Epoch 281 Batch  135/215   train_loss = 2.900
Epoch 281 Batch  145/215   train_loss = 2.722
Epoch 281 Batch  155/215   train_loss = 2.843
Epoch 281 Batch  165/215   train_loss = 2.843
Epoch 281 Batch  175/215   train_loss = 3.054
Epoch 281 Batch  185/215   train_loss = 2.842
Epoch 281 Batch  195/215   train_loss = 2.731
Epoch 281 Batch  205/215   train_loss = 2.654
Epoch 282 Batch    0/215   train_loss = 2.877
Epoch 282 Batch   10/215   train_loss = 2.901
Epoch 282 Batch   20/215   train_loss = 2.915
Epoch 282 Batch   30/215   train_loss = 2.797
Epoch 282 Batch   40/215   train_loss = 2.635
Epoch 282 Batch   50/215   train_loss = 2.773
Epoch 282 Batch   60/215   train_loss = 2.770
Epoch 282 Batch   70/215   train_loss = 2.779
Epoch 282 Batch   80/215   train_loss = 2.661
Epoch 282 Batch   90/215   train_loss = 2.738
Epoch 282 Batch  100/215   train_loss = 2.883
Epoch 282 Batch  110/215   train_loss = 3.036
Epoch 282 Batch  120/215   train_loss = 2.639
Epoch 282 Batch  130/215   train_loss = 2.886
Epoch 282 Batch  140/215   train_loss = 2.776
Epoch 282 Batch  150/215   train_loss = 2.837
Epoch 282 Batch  160/215   train_loss = 2.741
Epoch 282 Batch  170/215   train_loss = 2.747
Epoch 282 Batch  180/215   train_loss = 2.865
Epoch 282 Batch  190/215   train_loss = 2.683
Epoch 282 Batch  200/215   train_loss = 2.607
Epoch 282 Batch  210/215   train_loss = 2.749
Epoch 283 Batch    5/215   train_loss = 2.658
Epoch 283 Batch   15/215   train_loss = 2.656
Epoch 283 Batch   25/215   train_loss = 2.668
Epoch 283 Batch   35/215   train_loss = 3.136
Epoch 283 Batch   45/215   train_loss = 2.660
Epoch 283 Batch   55/215   train_loss = 2.745
Epoch 283 Batch   65/215   train_loss = 2.846
Epoch 283 Batch   75/215   train_loss = 2.558
Epoch 283 Batch   85/215   train_loss = 2.614
Epoch 283 Batch   95/215   train_loss = 2.613
Epoch 283 Batch  105/215   train_loss = 2.541
Epoch 283 Batch  115/215   train_loss = 2.691
Epoch 283 Batch  125/215   train_loss = 2.776
Epoch 283 Batch  135/215   train_loss = 2.584
Epoch 283 Batch  145/215   train_loss = 2.717
Epoch 283 Batch  155/215   train_loss = 2.761
Epoch 283 Batch  165/215   train_loss = 2.725
Epoch 283 Batch  175/215   train_loss = 2.955
Epoch 283 Batch  185/215   train_loss = 2.993
Epoch 283 Batch  195/215   train_loss = 2.665
Epoch 283 Batch  205/215   train_loss = 2.591
Epoch 284 Batch    0/215   train_loss = 2.867
Epoch 284 Batch   10/215   train_loss = 2.891
Epoch 284 Batch   20/215   train_loss = 2.812
Epoch 284 Batch   30/215   train_loss = 2.917
Epoch 284 Batch   40/215   train_loss = 2.629
Epoch 284 Batch   50/215   train_loss = 2.851
Epoch 284 Batch   60/215   train_loss = 2.949
Epoch 284 Batch   70/215   train_loss = 2.594
Epoch 284 Batch   80/215   train_loss = 2.704
Epoch 284 Batch   90/215   train_loss = 2.739
Epoch 284 Batch  100/215   train_loss = 2.814
Epoch 284 Batch  110/215   train_loss = 2.864
Epoch 284 Batch  120/215   train_loss = 2.679
Epoch 284 Batch  130/215   train_loss = 2.993
Epoch 284 Batch  140/215   train_loss = 2.729
Epoch 284 Batch  150/215   train_loss = 2.723
Epoch 284 Batch  160/215   train_loss = 2.950
Epoch 284 Batch  170/215   train_loss = 2.639
Epoch 284 Batch  180/215   train_loss = 2.647
Epoch 284 Batch  190/215   train_loss = 2.732
Epoch 284 Batch  200/215   train_loss = 2.643
Epoch 284 Batch  210/215   train_loss = 2.599
Epoch 285 Batch    5/215   train_loss = 2.841
Epoch 285 Batch   15/215   train_loss = 2.817
Epoch 285 Batch   25/215   train_loss = 2.627
Epoch 285 Batch   35/215   train_loss = 2.898
Epoch 285 Batch   45/215   train_loss = 2.978
Epoch 285 Batch   55/215   train_loss = 2.855
Epoch 285 Batch   65/215   train_loss = 2.841
Epoch 285 Batch   75/215   train_loss = 2.752
Epoch 285 Batch   85/215   train_loss = 2.807
Epoch 285 Batch   95/215   train_loss = 2.756
Epoch 285 Batch  105/215   train_loss = 2.857
Epoch 285 Batch  115/215   train_loss = 2.661
Epoch 285 Batch  125/215   train_loss = 2.568
Epoch 285 Batch  135/215   train_loss = 2.550
Epoch 285 Batch  145/215   train_loss = 3.022
Epoch 285 Batch  155/215   train_loss = 2.938
Epoch 285 Batch  165/215   train_loss = 2.698
Epoch 285 Batch  175/215   train_loss = 3.012
Epoch 285 Batch  185/215   train_loss = 2.847
Epoch 285 Batch  195/215   train_loss = 2.697
Epoch 285 Batch  205/215   train_loss = 2.654
Epoch 286 Batch    0/215   train_loss = 2.741
Epoch 286 Batch   10/215   train_loss = 2.880
Epoch 286 Batch   20/215   train_loss = 2.991
Epoch 286 Batch   30/215   train_loss = 2.604
Epoch 286 Batch   40/215   train_loss = 2.794
Epoch 286 Batch   50/215   train_loss = 2.687
Epoch 286 Batch   60/215   train_loss = 2.930
Epoch 286 Batch   70/215   train_loss = 2.558
Epoch 286 Batch   80/215   train_loss = 2.781
Epoch 286 Batch   90/215   train_loss = 2.964
Epoch 286 Batch  100/215   train_loss = 2.752
Epoch 286 Batch  110/215   train_loss = 2.780
Epoch 286 Batch  120/215   train_loss = 2.898
Epoch 286 Batch  130/215   train_loss = 2.971
Epoch 286 Batch  140/215   train_loss = 2.806
Epoch 286 Batch  150/215   train_loss = 2.849
Epoch 286 Batch  160/215   train_loss = 2.939
Epoch 286 Batch  170/215   train_loss = 2.628
Epoch 286 Batch  180/215   train_loss = 2.848
Epoch 286 Batch  190/215   train_loss = 2.649
Epoch 286 Batch  200/215   train_loss = 2.781
Epoch 286 Batch  210/215   train_loss = 2.706
Epoch 287 Batch    5/215   train_loss = 2.906
Epoch 287 Batch   15/215   train_loss = 2.793
Epoch 287 Batch   25/215   train_loss = 2.668
Epoch 287 Batch   35/215   train_loss = 2.870
Epoch 287 Batch   45/215   train_loss = 2.867
Epoch 287 Batch   55/215   train_loss = 2.684
Epoch 287 Batch   65/215   train_loss = 2.925
Epoch 287 Batch   75/215   train_loss = 2.534
Epoch 287 Batch   85/215   train_loss = 2.658
Epoch 287 Batch   95/215   train_loss = 2.694
Epoch 287 Batch  105/215   train_loss = 2.607
Epoch 287 Batch  115/215   train_loss = 2.686
Epoch 287 Batch  125/215   train_loss = 2.617
Epoch 287 Batch  135/215   train_loss = 2.623
Epoch 287 Batch  145/215   train_loss = 2.763
Epoch 287 Batch  155/215   train_loss = 2.710
Epoch 287 Batch  165/215   train_loss = 2.595
Epoch 287 Batch  175/215   train_loss = 2.917
Epoch 287 Batch  185/215   train_loss = 2.936
Epoch 287 Batch  195/215   train_loss = 2.533
Epoch 287 Batch  205/215   train_loss = 2.729
Epoch 288 Batch    0/215   train_loss = 2.875
Epoch 288 Batch   10/215   train_loss = 3.026
</pre>
</div>
</div>

<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_text output_error">
<pre>
<span class="ansi-red-intense-fg ansi-bold">---------------------------------------------------------------------------</span>
<span class="ansi-red-intense-fg ansi-bold">KeyboardInterrupt</span>                         Traceback (most recent call last)
<span class="ansi-green-intense-fg ansi-bold">&lt;ipython-input-17-396c263e7e73&gt;</span> in <span class="ansi-cyan-fg">&lt;module&gt;</span><span class="ansi-blue-intense-fg ansi-bold">()</span>
<span class="ansi-green-fg">     16</span>                 initial_state<span class="ansi-yellow-intense-fg ansi-bold">:</span> state<span class="ansi-yellow-intense-fg ansi-bold">,</span>
<span class="ansi-green-fg">     17</span>                 lr: learning_rate}
<span class="ansi-green-intense-fg ansi-bold">---&gt; 18</span><span class="ansi-yellow-intense-fg ansi-bold">             </span>train_loss<span class="ansi-yellow-intense-fg ansi-bold">,</span> state<span class="ansi-yellow-intense-fg ansi-bold">,</span> _ <span class="ansi-yellow-intense-fg ansi-bold">=</span> sess<span class="ansi-yellow-intense-fg ansi-bold">.</span>run<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">[</span>cost<span class="ansi-yellow-intense-fg ansi-bold">,</span> final_state<span class="ansi-yellow-intense-fg ansi-bold">,</span> train_op<span class="ansi-yellow-intense-fg ansi-bold">]</span><span class="ansi-yellow-intense-fg ansi-bold">,</span> feed<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">     19</span> 
<span class="ansi-green-fg">     20</span>             <span class="ansi-red-intense-fg ansi-bold"># Show every &lt;show_every_n_batches&gt; batches</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ProgramData\Anaconda3\envs\tvscript\lib\site-packages\tensorflow\python\client\session.py</span> in <span class="ansi-cyan-fg">run</span><span class="ansi-blue-intense-fg ansi-bold">(self, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-fg">    765</span>     <span class="ansi-green-intense-fg ansi-bold">try</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    766</span>       result = self._run(None, fetches, feed_dict, options_ptr,
<span class="ansi-green-intense-fg ansi-bold">--&gt; 767</span><span class="ansi-yellow-intense-fg ansi-bold">                          run_metadata_ptr)
</span><span class="ansi-green-fg">    768</span>       <span class="ansi-green-intense-fg ansi-bold">if</span> run_metadata<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    769</span>         proto_data <span class="ansi-yellow-intense-fg ansi-bold">=</span> tf_session<span class="ansi-yellow-intense-fg ansi-bold">.</span>TF_GetBuffer<span class="ansi-yellow-intense-fg ansi-bold">(</span>run_metadata_ptr<span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ProgramData\Anaconda3\envs\tvscript\lib\site-packages\tensorflow\python\client\session.py</span> in <span class="ansi-cyan-fg">_run</span><span class="ansi-blue-intense-fg ansi-bold">(self, handle, fetches, feed_dict, options, run_metadata)</span>
<span class="ansi-green-fg">    963</span>     <span class="ansi-green-intense-fg ansi-bold">if</span> final_fetches <span class="ansi-green-intense-fg ansi-bold">or</span> final_targets<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    964</span>       results = self._do_run(handle, final_targets, final_fetches,
<span class="ansi-green-intense-fg ansi-bold">--&gt; 965</span><span class="ansi-yellow-intense-fg ansi-bold">                              feed_dict_string, options, run_metadata)
</span><span class="ansi-green-fg">    966</span>     <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">    967</span>       results <span class="ansi-yellow-intense-fg ansi-bold">=</span> <span class="ansi-yellow-intense-fg ansi-bold">[</span><span class="ansi-yellow-intense-fg ansi-bold">]</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ProgramData\Anaconda3\envs\tvscript\lib\site-packages\tensorflow\python\client\session.py</span> in <span class="ansi-cyan-fg">_do_run</span><span class="ansi-blue-intense-fg ansi-bold">(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)</span>
<span class="ansi-green-fg">   1013</span>     <span class="ansi-green-intense-fg ansi-bold">if</span> handle <span class="ansi-green-intense-fg ansi-bold">is</span> <span class="ansi-green-intense-fg ansi-bold">None</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">   1014</span>       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,
<span class="ansi-green-intense-fg ansi-bold">-&gt; 1015</span><span class="ansi-yellow-intense-fg ansi-bold">                            target_list, options, run_metadata)
</span><span class="ansi-green-fg">   1016</span>     <span class="ansi-green-intense-fg ansi-bold">else</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">   1017</span>       return self._do_call(_prun_fn, self._session, handle, feed_dict,

<span class="ansi-green-intense-fg ansi-bold">C:\ProgramData\Anaconda3\envs\tvscript\lib\site-packages\tensorflow\python\client\session.py</span> in <span class="ansi-cyan-fg">_do_call</span><span class="ansi-blue-intense-fg ansi-bold">(self, fn, *args)</span>
<span class="ansi-green-fg">   1020</span>   <span class="ansi-green-intense-fg ansi-bold">def</span> _do_call<span class="ansi-yellow-intense-fg ansi-bold">(</span>self<span class="ansi-yellow-intense-fg ansi-bold">,</span> fn<span class="ansi-yellow-intense-fg ansi-bold">,</span> <span class="ansi-yellow-intense-fg ansi-bold">*</span>args<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">   1021</span>     <span class="ansi-green-intense-fg ansi-bold">try</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-intense-fg ansi-bold">-&gt; 1022</span><span class="ansi-yellow-intense-fg ansi-bold">       </span><span class="ansi-green-intense-fg ansi-bold">return</span> fn<span class="ansi-yellow-intense-fg ansi-bold">(</span><span class="ansi-yellow-intense-fg ansi-bold">*</span>args<span class="ansi-yellow-intense-fg ansi-bold">)</span>
<span class="ansi-green-fg">   1023</span>     <span class="ansi-green-intense-fg ansi-bold">except</span> errors<span class="ansi-yellow-intense-fg ansi-bold">.</span>OpError <span class="ansi-green-intense-fg ansi-bold">as</span> e<span class="ansi-yellow-intense-fg ansi-bold">:</span>
<span class="ansi-green-fg">   1024</span>       message <span class="ansi-yellow-intense-fg ansi-bold">=</span> compat<span class="ansi-yellow-intense-fg ansi-bold">.</span>as_text<span class="ansi-yellow-intense-fg ansi-bold">(</span>e<span class="ansi-yellow-intense-fg ansi-bold">.</span>message<span class="ansi-yellow-intense-fg ansi-bold">)</span>

<span class="ansi-green-intense-fg ansi-bold">C:\ProgramData\Anaconda3\envs\tvscript\lib\site-packages\tensorflow\python\client\session.py</span> in <span class="ansi-cyan-fg">_run_fn</span><span class="ansi-blue-intense-fg ansi-bold">(session, feed_dict, fetch_list, target_list, options, run_metadata)</span>
<span class="ansi-green-fg">   1002</span>         return tf_session.TF_Run(session, options,
<span class="ansi-green-fg">   1003</span>                                  feed_dict<span class="ansi-yellow-intense-fg ansi-bold">,</span> fetch_list<span class="ansi-yellow-intense-fg ansi-bold">,</span> target_list<span class="ansi-yellow-intense-fg ansi-bold">,</span>
<span class="ansi-green-intense-fg ansi-bold">-&gt; 1004</span><span class="ansi-yellow-intense-fg ansi-bold">                                  status, run_metadata)
</span><span class="ansi-green-fg">   1005</span> 
<span class="ansi-green-fg">   1006</span>     <span class="ansi-green-intense-fg ansi-bold">def</span> _prun_fn<span class="ansi-yellow-intense-fg ansi-bold">(</span>session<span class="ansi-yellow-intense-fg ansi-bold">,</span> handle<span class="ansi-yellow-intense-fg ansi-bold">,</span> feed_dict<span class="ansi-yellow-intense-fg ansi-bold">,</span> fetch_list<span class="ansi-yellow-intense-fg ansi-bold">)</span><span class="ansi-yellow-intense-fg ansi-bold">:</span>

<span class="ansi-red-intense-fg ansi-bold">KeyboardInterrupt</span>: </pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="These-are-the-hyperparameter-settings-that-got-the-result-I-was-looking-for,-loss-of-&lt;-1.">These are the hyperparameter settings that got the result I was looking for, loss of &lt; 1.<a class="anchor-link" href="#These-are-the-hyperparameter-settings-that-got-the-result-I-was-looking-for,-loss-of-&lt;-1.">&#182;</a></h3>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">batches</span> <span class="o">=</span> <span class="n">get_batches</span><span class="p">(</span><span class="n">int_text</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">)</span>

<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">train_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">global_variables_initializer</span><span class="p">())</span>

    <span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
        <span class="n">state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="p">{</span><span class="n">input_text</span><span class="p">:</span> <span class="n">batches</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]})</span>

        <span class="k">for</span> <span class="n">batch_i</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">batches</span><span class="p">):</span>
            <span class="n">feed</span> <span class="o">=</span> <span class="p">{</span>
                <span class="n">input_text</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span>
                <span class="n">targets</span><span class="p">:</span> <span class="n">y</span><span class="p">,</span>
                <span class="n">initial_state</span><span class="p">:</span> <span class="n">state</span><span class="p">,</span>
                <span class="n">lr</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">}</span>
            <span class="n">train_loss</span><span class="p">,</span> <span class="n">state</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">([</span><span class="n">cost</span><span class="p">,</span> <span class="n">final_state</span><span class="p">,</span> <span class="n">train_op</span><span class="p">],</span> <span class="n">feed</span><span class="p">)</span>

            <span class="c1"># Show every &lt;show_every_n_batches&gt; batches</span>
            <span class="k">if</span> <span class="p">(</span><span class="n">epoch_i</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">)</span> <span class="o">+</span> <span class="n">batch_i</span><span class="p">)</span> <span class="o">%</span> <span class="n">show_every_n_batches</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Epoch </span><span class="si">{:&gt;3}</span><span class="s1"> Batch </span><span class="si">{:&gt;4}</span><span class="s1">/</span><span class="si">{}</span><span class="s1">   train_loss = </span><span class="si">{:.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="n">epoch_i</span><span class="p">,</span>
                    <span class="n">batch_i</span><span class="p">,</span>
                    <span class="nb">len</span><span class="p">(</span><span class="n">batches</span><span class="p">),</span>
                    <span class="n">train_loss</span><span class="p">))</span>

    <span class="c1"># Save Model</span>
    <span class="n">saver</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">Saver</span><span class="p">()</span>
    <span class="n">saver</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Model Trained and Saved&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Epoch   0 Batch    0/17   train_loss = 8.917
Epoch   1 Batch    3/17   train_loss = 5.055
Epoch   2 Batch    6/17   train_loss = 4.128
Epoch   3 Batch    9/17   train_loss = 3.341
Epoch   4 Batch   12/17   train_loss = 2.658
Epoch   5 Batch   15/17   train_loss = 2.198
Epoch   7 Batch    1/17   train_loss = 1.812
Epoch   8 Batch    4/17   train_loss = 1.556
Epoch   9 Batch    7/17   train_loss = 1.342
Epoch  10 Batch   10/17   train_loss = 1.168
Epoch  11 Batch   13/17   train_loss = 1.069
Epoch  12 Batch   16/17   train_loss = 0.989
Epoch  14 Batch    2/17   train_loss = 0.935
Epoch  15 Batch    5/17   train_loss = 0.852
Epoch  16 Batch    8/17   train_loss = 0.862
Epoch  17 Batch   11/17   train_loss = 0.768
Epoch  18 Batch   14/17   train_loss = 0.772
Epoch  20 Batch    0/17   train_loss = 0.740
Epoch  21 Batch    3/17   train_loss = 0.726
Epoch  22 Batch    6/17   train_loss = 0.688
Epoch  23 Batch    9/17   train_loss = 0.628
Epoch  24 Batch   12/17   train_loss = 0.667
Epoch  25 Batch   15/17   train_loss = 0.643
Epoch  27 Batch    1/17   train_loss = 0.617
Epoch  28 Batch    4/17   train_loss = 0.626
Epoch  29 Batch    7/17   train_loss = 0.591
Epoch  30 Batch   10/17   train_loss = 0.576
Epoch  31 Batch   13/17   train_loss = 0.561
Epoch  32 Batch   16/17   train_loss = 0.571
Epoch  34 Batch    2/17   train_loss = 0.550
Epoch  35 Batch    5/17   train_loss = 0.542
Epoch  36 Batch    8/17   train_loss = 0.548
Epoch  37 Batch   11/17   train_loss = 0.568
Epoch  38 Batch   14/17   train_loss = 0.544
Epoch  40 Batch    0/17   train_loss = 0.560
Epoch  41 Batch    3/17   train_loss = 0.557
Epoch  42 Batch    6/17   train_loss = 0.557
Epoch  43 Batch    9/17   train_loss = 0.556
Epoch  44 Batch   12/17   train_loss = 0.578
Epoch  45 Batch   15/17   train_loss = 0.550
Epoch  47 Batch    1/17   train_loss = 0.554
Epoch  48 Batch    4/17   train_loss = 0.541
Epoch  49 Batch    7/17   train_loss = 0.566
Epoch  50 Batch   10/17   train_loss = 0.550
Epoch  51 Batch   13/17   train_loss = 0.595
Epoch  52 Batch   16/17   train_loss = 0.574
Epoch  54 Batch    2/17   train_loss = 0.559
Epoch  55 Batch    5/17   train_loss = 0.566
Epoch  56 Batch    8/17   train_loss = 0.587
Epoch  57 Batch   11/17   train_loss = 0.605
Epoch  58 Batch   14/17   train_loss = 0.610
Epoch  60 Batch    0/17   train_loss = 0.618
Epoch  61 Batch    3/17   train_loss = 0.655
Epoch  62 Batch    6/17   train_loss = 0.597
Epoch  63 Batch    9/17   train_loss = 0.616
Epoch  64 Batch   12/17   train_loss = 0.632
Epoch  65 Batch   15/17   train_loss = 0.624
Epoch  67 Batch    1/17   train_loss = 0.596
Epoch  68 Batch    4/17   train_loss = 0.624
Epoch  69 Batch    7/17   train_loss = 0.621
Epoch  70 Batch   10/17   train_loss = 0.640
Epoch  71 Batch   13/17   train_loss = 0.656
Epoch  72 Batch   16/17   train_loss = 0.663
Epoch  74 Batch    2/17   train_loss = 0.631
Epoch  75 Batch    5/17   train_loss = 0.638
Epoch  76 Batch    8/17   train_loss = 0.651
Epoch  77 Batch   11/17   train_loss = 0.633
Epoch  78 Batch   14/17   train_loss = 0.692
Epoch  80 Batch    0/17   train_loss = 0.676
Epoch  81 Batch    3/17   train_loss = 0.649
Epoch  82 Batch    6/17   train_loss = 0.640
Epoch  83 Batch    9/17   train_loss = 0.620
Epoch  84 Batch   12/17   train_loss = 0.662
Epoch  85 Batch   15/17   train_loss = 0.635
Epoch  87 Batch    1/17   train_loss = 0.627
Epoch  88 Batch    4/17   train_loss = 0.615
Epoch  89 Batch    7/17   train_loss = 0.649
Epoch  90 Batch   10/17   train_loss = 0.642
Epoch  91 Batch   13/17   train_loss = 0.623
Epoch  92 Batch   16/17   train_loss = 0.623
Epoch  94 Batch    2/17   train_loss = 0.627
Epoch  95 Batch    5/17   train_loss = 0.619
Epoch  96 Batch    8/17   train_loss = 0.634
Epoch  97 Batch   11/17   train_loss = 0.641
Epoch  98 Batch   14/17   train_loss = 0.638
Model Trained and Saved
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Save-Parameters">Save Parameters<a class="anchor-link" href="#Save-Parameters">&#182;</a></h2><p>Save <code>seq_length</code> and <code>save_dir</code> for generating a new TV script.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[23]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="c1"># Save parameters for checkpoint</span>
<span class="n">helper</span><span class="o">.</span><span class="n">save_params</span><span class="p">((</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">save_dir</span><span class="p">))</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Checkpoint">Checkpoint<a class="anchor-link" href="#Checkpoint">&#182;</a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[24]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">helper</span>
<span class="kn">import</span> <span class="nn">problem_unittests</span> <span class="k">as</span> <span class="nn">tests</span>

<span class="n">_</span><span class="p">,</span> <span class="n">vocab_to_int</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">,</span> <span class="n">token_dict</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_preprocess</span><span class="p">()</span>
<span class="n">seq_length</span><span class="p">,</span> <span class="n">load_dir</span> <span class="o">=</span> <span class="n">helper</span><span class="o">.</span><span class="n">load_params</span><span class="p">()</span>
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Implement-Generate-Functions">Implement Generate Functions<a class="anchor-link" href="#Implement-Generate-Functions">&#182;</a></h2><h3 id="Get-Tensors">Get Tensors<a class="anchor-link" href="#Get-Tensors">&#182;</a></h3><p>Get tensors from <code>loaded_graph</code> using the function <a href="https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name"><code>get_tensor_by_name()</code></a>.  Get the tensors using the following names:</p>
<ul>
<li>"input:0"</li>
<li>"initial_state:0"</li>
<li>"final_state:0"</li>
<li>"probs:0"</li>
</ul>
<p>Return the tensors in the following tuple <code>(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[27]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">get_tensors</span><span class="p">(</span><span class="n">loaded_graph</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Get input, initial state, final state, and probabilities tensor from &lt;loaded_graph&gt;</span>
<span class="sd">    :param loaded_graph: TensorFlow graph loaded from file</span>
<span class="sd">    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
    <span class="c1"># Get the tensors by name</span>
    <span class="c1">#</span>
    <span class="c1"># NOTE: It seems like tensorflow enumerates the variable names you give it,</span>
    <span class="c1">#       so I appened :0 to the variable names since there should be only one copy.</span>
    <span class="n">InputTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;input:0&#39;</span><span class="p">)</span>
    <span class="n">InitialStateTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;initial_state:0&#39;</span><span class="p">)</span>
    <span class="n">FinalStateTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;final_state:0&#39;</span><span class="p">)</span>
    <span class="n">ProbsTensor</span> <span class="o">=</span> <span class="n">loaded_graph</span><span class="o">.</span><span class="n">get_tensor_by_name</span><span class="p">(</span><span class="s1">&#39;probs:0&#39;</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">InputTensor</span><span class="p">,</span> <span class="n">InitialStateTensor</span><span class="p">,</span> <span class="n">FinalStateTensor</span><span class="p">,</span> <span class="n">ProbsTensor</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_get_tensors</span><span class="p">(</span><span class="n">get_tensors</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Choose-Word">Choose Word<a class="anchor-link" href="#Choose-Word">&#182;</a></h3><p>Implement the <code>pick_word()</code> function to select the next word using <code>probabilities</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[40]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">def</span> <span class="nf">pick_word</span><span class="p">(</span><span class="n">probabilities</span><span class="p">,</span> <span class="n">int_to_vocab</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Pick the next word in the generated text</span>
<span class="sd">    :param probabilities: Probabilites of the next word</span>
<span class="sd">    :param int_to_vocab: Dictionary of word ids as the keys and words as the values</span>
<span class="sd">    :return: String of the predicted word</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># TODO: Implement Function</span>
    
<span class="c1">#     # DEBUG</span>
<span class="c1">#     #</span>
<span class="c1">#     print(&quot;DEBUG - type(probabilities): {}&quot;.format(type(probabilities)))</span>
<span class="c1">#     print(&quot;DEBUG - type(int_to_vocab): {}&quot;.format(type(int_to_vocab)))</span>
    
    <span class="c1"># Generates a random sample from a given 1-D array</span>
    <span class="c1"># https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html</span>
    <span class="n">pred_word</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span> <span class="nb">list</span><span class="p">(</span><span class="n">int_to_vocab</span><span class="o">.</span><span class="n">values</span><span class="p">()),</span>
                                  <span class="n">p</span><span class="o">=</span><span class="n">probabilities</span> <span class="p">)</span>
    
    <span class="k">return</span> <span class="n">pred_word</span>


<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">tests</span><span class="o">.</span><span class="n">test_pick_word</span><span class="p">(</span><span class="n">pick_word</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>Tests Passed
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Generate-TV-Script">Generate TV Script<a class="anchor-link" href="#Generate-TV-Script">&#182;</a></h2><p>This will generate the TV script for you.  Set <code>gen_length</code> to the length of TV script you want to generate.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[41]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">gen_length</span> <span class="o">=</span> <span class="mi">200</span>
<span class="c1"># homer_simpson, moe_szyslak, or Barney_Gumble</span>
<span class="n">prime_word</span> <span class="o">=</span> <span class="s1">&#39;moe_szyslak&#39;</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">DON&#39;T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE</span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="n">loaded_graph</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">Graph</span><span class="p">()</span>
<span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">graph</span><span class="o">=</span><span class="n">loaded_graph</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># Load saved model</span>
    <span class="n">loader</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">import_meta_graph</span><span class="p">(</span><span class="n">load_dir</span> <span class="o">+</span> <span class="s1">&#39;.meta&#39;</span><span class="p">)</span>
    <span class="n">loader</span><span class="o">.</span><span class="n">restore</span><span class="p">(</span><span class="n">sess</span><span class="p">,</span> <span class="n">load_dir</span><span class="p">)</span>

    <span class="c1"># Get Tensors from loaded model</span>
    <span class="n">input_text</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">final_state</span><span class="p">,</span> <span class="n">probs</span> <span class="o">=</span> <span class="n">get_tensors</span><span class="p">(</span><span class="n">loaded_graph</span><span class="p">)</span>

    <span class="c1"># Sentences generation setup</span>
    <span class="n">gen_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">prime_word</span> <span class="o">+</span> <span class="s1">&#39;:&#39;</span><span class="p">]</span>
    <span class="n">prev_state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">initial_state</span><span class="p">,</span> <span class="p">{</span><span class="n">input_text</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])})</span>

    <span class="c1"># Generate sentences</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">gen_length</span><span class="p">):</span>
        <span class="c1"># Dynamic Input</span>
        <span class="n">dyn_input</span> <span class="o">=</span> <span class="p">[[</span><span class="n">vocab_to_int</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">gen_sentences</span><span class="p">[</span><span class="o">-</span><span class="n">seq_length</span><span class="p">:]]]</span>
        <span class="n">dyn_seq_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dyn_input</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Get Prediction</span>
        <span class="n">probabilities</span><span class="p">,</span> <span class="n">prev_state</span> <span class="o">=</span> <span class="n">sess</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
            <span class="p">[</span><span class="n">probs</span><span class="p">,</span> <span class="n">final_state</span><span class="p">],</span>
            <span class="p">{</span><span class="n">input_text</span><span class="p">:</span> <span class="n">dyn_input</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">:</span> <span class="n">prev_state</span><span class="p">})</span>
        
        <span class="n">pred_word</span> <span class="o">=</span> <span class="n">pick_word</span><span class="p">(</span><span class="n">probabilities</span><span class="p">[</span><span class="n">dyn_seq_length</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">int_to_vocab</span><span class="p">)</span>

        <span class="n">gen_sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pred_word</span><span class="p">)</span>
    
    <span class="c1"># Remove tokens</span>
    <span class="n">tv_script</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">gen_sentences</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">token_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">ending</span> <span class="o">=</span> <span class="s1">&#39; &#39;</span> <span class="k">if</span> <span class="n">key</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">,</span> <span class="s1">&#39;&quot;&#39;</span><span class="p">]</span> <span class="k">else</span> <span class="s1">&#39;&#39;</span>
        <span class="n">tv_script</span> <span class="o">=</span> <span class="n">tv_script</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">token</span><span class="o">.</span><span class="n">lower</span><span class="p">(),</span> <span class="n">key</span><span class="p">)</span>
    <span class="n">tv_script</span> <span class="o">=</span> <span class="n">tv_script</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1"> &#39;</span><span class="p">,</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">tv_script</span> <span class="o">=</span> <span class="n">tv_script</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;( &#39;</span><span class="p">,</span> <span class="s1">&#39;(&#39;</span><span class="p">)</span>
        
    <span class="nb">print</span><span class="p">(</span><span class="n">tv_script</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">
<div class="prompt"></div>

<div class="output_subarea output_stream output_stdout output_text">
<pre>moe_szyslak: oh no!(four hundred nos.
carl_carlson: marge, we gotta celebrate! throw a ragin&#39; on, and then there...
comic_book_guy: my name is walther hotenhoffer and i&#39;m in an here.
marge_simpson:(pained) i guess so one, i&#39;d&#39;a put lenny on the...
homer_simpson:(getting idea) no, the pipes not smells so much about lousy.
moe_szyslak: wow, that i go with that uh, y&#39;know they mr. the private, show me the secret guy i know named to make my rent.
carl_carlson: be for him? every day too?
moe_szyslak: &#39;cause i was inches of you lugs wanna get any specific harm.


moe_szyslak: get down here! the you!
barney_gumble: of course. but didn&#39;t it sounds like one of these&#34; alone during the bonding phase.
moe_szyslak:(desperate) wait, wait, wait, wait, wait... bo&#39;s he still got to say one hundred.
marguerite: is here that the now of you krusty.

</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="The-TV-Script-is-Nonsensical">The TV Script is Nonsensical<a class="anchor-link" href="#The-TV-Script-is-Nonsensical">&#182;</a></h1><p>It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of <a href="https://www.kaggle.com/wcukierski/the-simpsons-by-the-data">another dataset</a>.  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.</p>
<h1 id="Submitting-This-Project">Submitting This Project<a class="anchor-link" href="#Submitting-This-Project">&#182;</a></h1><p>When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as "dlnd_tv_script_generation.ipynb" and save it as a HTML file under "File" -&gt; "Download as". Include the "helper.py" and "problem_unittests.py" files in your submission.</p>

</div>
</div>
</div>
    </div>
  </div>
</body>

 


</html>
