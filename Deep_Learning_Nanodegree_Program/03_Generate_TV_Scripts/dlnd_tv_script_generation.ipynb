{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TV Script Generation\n",
    "In this project, you'll generate your own [Simpsons](https://en.wikipedia.org/wiki/The_Simpsons) TV scripts using RNNs.  You'll be using part of the [Simpsons dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data) of scripts from 27 seasons.  The Neural Network you'll build will generate a new TV script for a scene at [Moe's Tavern](https://simpsonswiki.com/wiki/Moe's_Tavern).\n",
    "## Get the Data\n",
    "The data is already provided for you.  You'll be using a subset of the original dataset.  It consists of only the scenes in Moe's Tavern.  This doesn't include other versions of the tavern, like \"Moe's Cavern\", \"Flaming Moe's\", \"Uncle Moe's Family Feed-Bag\", etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "\n",
    "data_dir = './data/simpsons/moes_tavern_lines.txt'\n",
    "text = helper.load_data(data_dir)\n",
    "# Ignore notice, since we don't use it for analysing the data\n",
    "text = text[81:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 11492\n",
      "Number of scenes: 262\n",
      "Average number of sentences in each scene: 15.251908396946565\n",
      "Number of lines: 4258\n",
      "Average number of words in each line: 11.50164396430249\n",
      "\n",
      "The sentences 0 to 10:\n",
      "\n",
      "Moe_Szyslak: (INTO PHONE) Moe's Tavern. Where the elite meet to drink.\n",
      "Bart_Simpson: Eh, yeah, hello, is Mike there? Last name, Rotch.\n",
      "Moe_Szyslak: (INTO PHONE) Hold on, I'll check. (TO BARFLIES) Mike Rotch. Mike Rotch. Hey, has anybody seen Mike Rotch, lately?\n",
      "Moe_Szyslak: (INTO PHONE) Listen you little puke. One of these days I'm gonna catch you, and I'm gonna carve my name on your back with an ice pick.\n",
      "Moe_Szyslak: What's the matter Homer? You're not your normal effervescent self.\n",
      "Homer_Simpson: I got my problems, Moe. Give me another one.\n",
      "Moe_Szyslak: Homer, hey, you should not drink to forget your problems.\n",
      "Barney_Gumble: Yeah, you should only drink to enhance your social skills.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocessing Functions\n",
    "The first thing to do to any dataset is preprocessing.  Implement the following preprocessing functions below:\n",
    "- Lookup Table\n",
    "- Tokenize Punctuation\n",
    "\n",
    "### Lookup Table\n",
    "To create a word embedding, you first need to transform the words to ids.  In this function, create two dictionaries:\n",
    "- Dictionary to go from the words to an id, we'll call `vocab_to_int`\n",
    "- Dictionary to go from the id to word, we'll call `int_to_vocab`\n",
    "\n",
    "Return these dictionaries in the following tuple `(vocab_to_int, int_to_vocab)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This was my original method. The next cell used dictionary comprehension and it is more concise. I kept this because I did do some research to get it to work and it highlights some differences between python 2 and 3. This can be refered to later in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import problem_unittests as tests\n",
    "# from collections import Counter\n",
    "\n",
    "# # Inverting a Dictionary\n",
    "# # https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch04s15.html\n",
    "# def invert_dict(d):\n",
    "    \n",
    "#     # NOTE: .iteritems is deprecated in python 3, use .items() instead\n",
    "#     # https://stackoverflow.com/questions/30418481/error-dict-object-has-no-attribute-iteritems-when-trying-to-use-networkx\n",
    "#     # https://wiki.python.org/moin/Python3.0\n",
    "#     #return dict([ (v, k) for k, v in d.iteritems( ) ])\n",
    "#     return dict([ (v, k) for k, v in d.items( ) ])\n",
    "\n",
    "# # # Inverting a Dictionary\n",
    "# # # Faster for larger dictionaries\n",
    "# # # https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch04s15.html\n",
    "# # from itertools import izip\n",
    "# # def invert_dict_fast(d):\n",
    "# #     return dict(izip(d.itervalues( ), d.iterkeys( )))\n",
    "\n",
    "# # Inverting a Dictionary\n",
    "# # Faster for larger dictionaries\n",
    "# # https://www.safaribooksonline.com/library/view/python-cookbook-2nd/0596007973/ch04s15.html\n",
    "# #\n",
    "# # NOTE: In python 3, there are some changes.\n",
    "# #       - You don't need itertools izip, you can just use zip \n",
    "# #         http://www.diveintopython3.net/porting-code-to-python-3-with-2to3.html\n",
    "# #       - itervalues and iterkeys are deprecated dictionary methods, use dict.keys() and dict.values()\n",
    "# #         https://wiki.python.org/moin/Python3.0\n",
    "# def invert_dict_fast(d):\n",
    "#     return dict(zip(d.values( ), d.keys( )))\n",
    "\n",
    "\n",
    "# def create_lookup_tables(text):\n",
    "#     \"\"\"\n",
    "#     Create lookup tables for vocabulary\n",
    "#     :param text: The text of tv scripts split into words\n",
    "#     :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "#     \"\"\"\n",
    "#     # TODO: Implement Function\n",
    "#     counts = Counter(text)\n",
    "#     vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    \n",
    "#     # Dictionary comprehension.\n",
    "#     #\n",
    "#     # https://stackoverflow.com/questions/22171558/what-does-enumerate-mean\n",
    "#     # The enumerate() function adds a counter to an iterable.\n",
    "#     # So for each element in cursor, a tuple is produced with (counter, element); \n",
    "#     # the for loop binds that to row_number and row, respectively.\n",
    "#     # By default, enumerate() starts counting at 0 but if you give it a second integer \n",
    "#     # argument, it'll start from that number instead:\n",
    "#     vocab_to_int = {text: ii for ii, text in enumerate(vocab, 1)}\n",
    "    \n",
    "#     # Create the inverted dictionary\n",
    "#     #int_to_vocab = invert_dict(vocab_to_int)\n",
    "#     int_to_vocab = invert_dict_fast(vocab_to_int)\n",
    "\n",
    "    \n",
    "#     return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "# \"\"\"\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "# \"\"\"\n",
    "# tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using dictionary comprehension is more concise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    counts = Counter(text)\n",
    "    vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "    vocab_to_int = {word:i for i, word in enumerate(vocab)}\n",
    "    int_to_vocab = {i:word for i, word in enumerate(vocab)}\n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Tokenize Punctuation\n",
    "We'll be splitting the script into a word array using spaces as delimiters.  However, punctuations like periods and exclamation marks make it hard for the neural network to distinguish between the word \"bye\" and \"bye!\".\n",
    "\n",
    "Implement the function `token_lookup` to return a dict that will be used to tokenize symbols like \"!\" into \"||Exclamation_Mark||\".  Create a dictionary for the following symbols where the symbol is the key and value is the token:\n",
    "- Period ( . )\n",
    "- Comma ( , )\n",
    "- Quotation Mark ( \" )\n",
    "- Semicolon ( ; )\n",
    "- Exclamation mark ( ! )\n",
    "- Question mark ( ? )\n",
    "- Left Parentheses ( ( )\n",
    "- Right Parentheses ( ) )\n",
    "- Dash ( -- )\n",
    "- Return ( \\n )\n",
    "\n",
    "This dictionary will be used to token the symbols and add the delimiter (space) around it.  This separates the symbols as it's own word, making it easier for the neural network to predict on the next word. Make sure you don't use a token that could be confused as a word. Instead of using the token \"dash\", try using something like \"||dash||\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    dict_punctuation_tokens = { '.' : '||Period||',\n",
    "                                ',' : '||Comma||',\n",
    "                                '\"' : '||Quotation_Mark||',\n",
    "                                ';' : '||Semicolon||',\n",
    "                                '!' : '||Exclamation_mark||',\n",
    "                                '?' : '||Question_mark||',\n",
    "                                '(' : '||Left_Parentheses||',\n",
    "                                ')' : '||Right_Parentheses||',\n",
    "                                '--' : '||Dash||',\n",
    "                                '\\n' : '||Return||' }    \n",
    "    return dict_punctuation_tokens\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the data and save it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is your first checkpoint. If you ever decide to come back to this notebook or have to restart the notebook, you can start from here. The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import helper\n",
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the Neural Network\n",
    "You'll build the components necessary to build a RNN by implementing the following functions below:\n",
    "- get_inputs\n",
    "- get_init_cell\n",
    "- get_embed\n",
    "- build_rnn\n",
    "- build_nn\n",
    "- get_batches\n",
    "\n",
    "### Check the Version of TensorFlow and Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.0.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Input\n",
    "Implement the `get_inputs()` function to create TF Placeholders for the Neural Network.  It should create the following placeholders:\n",
    "- Input text placeholder named \"input\" using the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) `name` parameter.\n",
    "- Targets placeholder\n",
    "- Learning Rate placeholder\n",
    "\n",
    "Return the placeholders in the following tuple `(Input, Targets, LearningRate)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_inputs():\n",
    "    \"\"\"\n",
    "    Create TF Placeholders for input, targets, and learning rate.\n",
    "    :return: Tuple (input, targets, learning rate)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    Input = tf.placeholder(tf.int32, shape = [None, None], name='input')\n",
    "    Targets = tf.placeholder(tf.int32,  shape = [None, None], name='Targets')\n",
    "    LearningRate = tf.placeholder(tf.float32, shape = None, name='LearningRate')\n",
    "    \n",
    "    return Input, Targets, LearningRate\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_inputs(get_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN Cell and Initialize\n",
    "Stack one or more [`BasicLSTMCells`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell) in a [`MultiRNNCell`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell).\n",
    "- The Rnn size should be set using `rnn_size`\n",
    "- Initalize Cell State using the MultiRNNCell's [`zero_state()`](https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell#zero_state) function\n",
    "    - Apply the name \"initial_state\" to the initial state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the cell and initial state in the following tuple `(Cell, InitialState)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_init_cell(batch_size, rnn_size):\n",
    "    \"\"\"\n",
    "    Create an RNN Cell and initialize it.\n",
    "    :param batch_size: Size of batches\n",
    "    :param rnn_size: Size of RNNs\n",
    "    :return: Tuple (cell, initialize state)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # NOTE: It seems like setting the number of layers matter when training\n",
    "    #       as well as dropout probability. It would be nice if these \n",
    "    #       \"hyper-parameters\" were consolidated into one place so I don't have to jump around\n",
    "    #        the notebook.\n",
    "    #num_layers = 10\n",
    "    num_layers = 1\n",
    "    keep_prob = 0.5\n",
    "    \n",
    "    # Your basic LSTM cell\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        \n",
    "    # Add dropout\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    # Stack up multiple LSTM layers, for deep learning\n",
    "    #\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/BasicLSTMCell\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/rnn/MultiRNNCell\n",
    "    #\n",
    "    # Note:\n",
    "    # RNN cell composed sequentially of multiple simple cells.\n",
    "    # Even if it is just one BasicLSTMCells you have to make it iterable by\n",
    "    # puttint it in a dictionary.\n",
    "    #\n",
    "    # Note: Put 3 layers because the lecture stated that 3 layers improves 2 but\n",
    "    #       no real difference beyound 3 unless dealing with convolution network.\n",
    "    #       RNN is similar to a traditional feed-forward network, so I will assume\n",
    "    #       3 layers is a good starting point.\n",
    "    Cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "    \n",
    "    # Getting an initial state of all zeros\n",
    "    InitialState = Cell.zero_state(batch_size, tf.float32)\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/identity\n",
    "    InitialState = tf.identity(InitialState, name='initial_state')\n",
    "    \n",
    "    return Cell, InitialState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_init_cell(get_init_cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Word Embedding\n",
    "Apply embedding to `input_data` using TensorFlow.  Return the embedded sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Create embedding for <input_data>.\n",
    "    :param input_data: TF placeholder for text input.\n",
    "    :param vocab_size: Number of words in vocabulary.\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Embedded input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Examples of embedding:\n",
    "    # https://github.com/udacity/deep-learning/blob/master/embeddings/Skip-Grams-Solution.ipynb\n",
    "    # https://github.com/udacity/deep-learning/blob/master/sentiment-rnn/Sentiment_RNN_Solution.ipynb\n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "    return embed\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_embed(get_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build RNN\n",
    "You created a RNN Cell in the `get_init_cell()` function.  Time to use the cell to create a RNN.\n",
    "- Build the RNN using the [`tf.nn.dynamic_rnn()`](https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn)\n",
    " - Apply the name \"final_state\" to the final state using [`tf.identity()`](https://www.tensorflow.org/api_docs/python/tf/identity)\n",
    "\n",
    "Return the outputs and final_state state in the following tuple `(Outputs, FinalState)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_rnn(cell, inputs):\n",
    "    \"\"\"\n",
    "    Create a RNN using a RNN Cell\n",
    "    :param cell: RNN Cell\n",
    "    :param inputs: Input text data\n",
    "    :return: Tuple (Outputs, Final State)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "    Outputs, FinalState = tf.nn.dynamic_rnn( cell,\n",
    "                                             inputs,\n",
    "                                             dtype = tf.float32 )\n",
    "    \n",
    "    # https://www.tensorflow.org/api_docs/python/tf/identity\n",
    "    FinalState = tf.identity(FinalState, name='final_state')\n",
    "    \n",
    "    return Outputs, FinalState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_rnn(build_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Neural Network\n",
    "Apply the functions you implemented above to:\n",
    "- Apply embedding to `input_data` using your `get_embed(input_data, vocab_size, embed_dim)` function.\n",
    "- Build RNN using `cell` and your `build_rnn(cell, inputs)` function.\n",
    "- Apply a fully connected layer with a linear activation and `vocab_size` as the number of outputs.\n",
    "\n",
    "Return the logits and final state in the following tuple (Logits, FinalState) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: rnn_size does not seem to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    "    \"\"\"\n",
    "    Build part of the neural network\n",
    "    :param cell: RNN cell\n",
    "    :param rnn_size: Size of rnns\n",
    "    :param input_data: Input data\n",
    "    :param vocab_size: Vocabulary size\n",
    "    :param embed_dim: Number of embedding dimensions\n",
    "    :return: Tuple (Logits, FinalState)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Apply embedding to input_data using your get_embed(input_data, vocab_size, embed_dim) function.\n",
    "    #embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    embed = get_embed(input_data, vocab_size, embed_dim)\n",
    "    \n",
    "    # Build RNN using cell and your build_rnn(cell, inputs) function.\n",
    "    Outputs, FinalState = build_rnn(cell, embed)\n",
    "\n",
    "    # Apply a fully connected layer with a linear activation and vocab_size as the number of outputs.\n",
    "    # https://www.tensorflow.org/api_docs/python/tf/contrib/layers/fully_connected\n",
    "    #\n",
    "    # Note: To have linear activation, pass None to activation_fn\n",
    "    Logits  = tf.contrib.layers.fully_connected( Outputs,\n",
    "                                                 vocab_size,\n",
    "                                                 activation_fn=None,\n",
    "                                                 weights_initializer = tf.truncated_normal_initializer(stddev=0.1),\n",
    "                                                 biases_initializer=tf.zeros_initializer())\n",
    "    \n",
    "    \n",
    "    return Logits, FinalState\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_build_nn(build_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Batches\n",
    "Implement `get_batches` to create batches of input and targets using `int_text`.  The batches should be a Numpy array with the shape `(number of batches, 2, batch size, sequence length)`. Each batch contains two elements:\n",
    "- The first element is a single batch of **input** with the shape `[batch size, sequence length]`\n",
    "- The second element is a single batch of **targets** with the shape `[batch size, sequence length]`\n",
    "\n",
    "If you can't fill the last batch with enough data, drop the last batch.\n",
    "\n",
    "For exmple, `get_batches([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 3, 2)` would return a Numpy array of the following:\n",
    "```\n",
    "[\n",
    "  # First Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 1  2], [ 7  8], [13 14]]\n",
    "    # Batch of targets\n",
    "    [[ 2  3], [ 8  9], [14 15]]\n",
    "  ]\n",
    "\n",
    "  # Second Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 3  4], [ 9 10], [15 16]]\n",
    "    # Batch of targets\n",
    "    [[ 4  5], [10 11], [16 17]]\n",
    "  ]\n",
    "\n",
    "  # Third Batch\n",
    "  [\n",
    "    # Batch of Input\n",
    "    [[ 5  6], [11 12], [17 18]]\n",
    "    # Batch of targets\n",
    "    [[ 6  7], [12 13], [18  1]]\n",
    "  ]\n",
    "]\n",
    "```\n",
    "\n",
    "Notice that the last target value in the last batch is the first input value of the first batch. In this case, `1`. This is a common technique used when creating sequence batches, although it is rather unintuitive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_batches(int_text, batch_size, seq_length):\n",
    "    \"\"\"\n",
    "    Return batches of input and target\n",
    "    :param int_text: Text with the words replaced by their ids\n",
    "    :param batch_size: The size of batch\n",
    "    :param seq_length: The length of sequence\n",
    "    :return: Batches as a Numpy array\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    chars_in_a_batch = batch_size * seq_length\n",
    "    \n",
    "    # NOTE: In python 3, double slashes // means interger division.\n",
    "    # https://stackoverflow.com/questions/1535596/what-is-the-reason-for-having-in-python\n",
    "    num_batches = len(int_text)//(chars_in_a_batch)\n",
    "    \n",
    "    # Use slicing to select the range.\n",
    "    # NOTE: The target is offset by 1 from the input. That is because it is a recurrent network,\n",
    "    #       the previous input is needed to predict the next output. The example supplied in the \n",
    "    #       markdown cell above demonstrates this phenomenon.\n",
    "    input_data = np.array(int_text[ : (num_batches * chars_in_a_batch)])\n",
    "    target_data = np.array(int_text[1 : (num_batches * chars_in_a_batch)+1])\n",
    "    \n",
    "    # The last target value in the last batch is the first input value of the first batch.\n",
    "    # Note: I don't fully understand why this works. It was mentioned in the lession it is\n",
    "    #       not understood why this works either. I am doing it because it was mentioned in\n",
    "    #       the lecture and in the instruction.\n",
    "    target_data[-1] = input_data[0] \n",
    "    \n",
    "    # Reshape\n",
    "    # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.reshape.html\n",
    "    # \n",
    "    # Note:\n",
    "    # One shape dimension can be -1. In this case, the value is inferred from the length of the array and remaining dimensions.\n",
    "    inputs = input_data.reshape(batch_size, -1)\n",
    "    targets = target_data.reshape(batch_size, -1)\n",
    "\n",
    "    # Numpy Split\n",
    "    # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.split.html\n",
    "    #\n",
    "    # Note: This split an array into multiple sub-arrays.\n",
    "    # NOTE: Use axis 1.\n",
    "    inputs = np.split(inputs, num_batches, 1)\n",
    "    targets = np.split(targets, num_batches, 1)\n",
    "    \n",
    "    # Outputting to format (number of batches, 2, batch size, sequence length)\n",
    "    batches = np.array(list(zip(inputs, targets)))\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_batches(get_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "\n",
    "- Set `num_epochs` to the number of epochs.\n",
    "- Set `batch_size` to the batch size.\n",
    "- Set `rnn_size` to the size of the RNNs.\n",
    "- Set `embed_dim` to the size of the embedding.\n",
    "- Set `seq_length` to the length of sequence.\n",
    "- Set `learning_rate` to the learning rate.\n",
    "- Set `show_every_n_batches` to the number of batches the neural network should print progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: These were my original hyperparameter settings that trained on. I left them because I let it train for about 5 hours on GPU-enabled tensorflow and it hovered around 2.5 and 3.0 loss value before I stopped it. I stopped it around 283 epochs out of 300. It did not look like it was going to get better. I keep it around for reference in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # Number of Epochs\n",
    "# num_epochs = 300\n",
    "# # Batch Size\n",
    "# batch_size = 32\n",
    "# # RNN Size\n",
    "# rnn_size = 300\n",
    "# # Embedding Dimension Size\n",
    "# #embed_dim = 200\n",
    "# embed_dim = 300\n",
    "# # Sequence Length\n",
    "# seq_length = 10\n",
    "# # Learning Rate\n",
    "# learning_rate = 0.01\n",
    "# # Show stats for every n number of batches\n",
    "# show_every_n_batches = 10\n",
    "\n",
    "# \"\"\"\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "# \"\"\"\n",
    "# save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the hyperparameter settings I used that worked. It actually got the desired target loss value of less than 1 in about 10 minutes on a GPU enabled tensorflow (as opposed 5 hours) at around epoch 12. I left it for a little over an hour with 100 epochs. The loss seemed to hover at about 0.6 from epoch 22 and onward. With these settings and the hardward used to train the model, leaving it about 20 to 30 epochs for about 30 min would probably best optimize this configuration. The rest of epochs are most likely a waste of time. Of course, you can't know that when discovering the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 100\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 1024\n",
    "# Embedding Dimension Size\n",
    "#embed_dim = 200\n",
    "embed_dim = 1024\n",
    "# Sequence Length\n",
    "seq_length = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.01\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 20\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Build the Graph\n",
    "Build the graph using the neural network you implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train\n",
    "Train the neural network on the preprocessed data.  If you have a hard time getting a good loss, check the [forums](https://discussions.udacity.com/) to see if anyone is having the same problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As mentioned above, this was my 5 hour GPU-enabled tensorflow settings. I kept it for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/215   train_loss = 8.821\n",
      "Epoch   0 Batch   10/215   train_loss = 6.674\n",
      "Epoch   0 Batch   20/215   train_loss = 6.200\n",
      "Epoch   0 Batch   30/215   train_loss = 5.878\n",
      "Epoch   0 Batch   40/215   train_loss = 6.115\n",
      "Epoch   0 Batch   50/215   train_loss = 5.896\n",
      "Epoch   0 Batch   60/215   train_loss = 5.906\n",
      "Epoch   0 Batch   70/215   train_loss = 5.672\n",
      "Epoch   0 Batch   80/215   train_loss = 5.504\n",
      "Epoch   0 Batch   90/215   train_loss = 5.512\n",
      "Epoch   0 Batch  100/215   train_loss = 5.502\n",
      "Epoch   0 Batch  110/215   train_loss = 5.424\n",
      "Epoch   0 Batch  120/215   train_loss = 5.449\n",
      "Epoch   0 Batch  130/215   train_loss = 5.825\n",
      "Epoch   0 Batch  140/215   train_loss = 5.918\n",
      "Epoch   0 Batch  150/215   train_loss = 5.461\n",
      "Epoch   0 Batch  160/215   train_loss = 5.382\n",
      "Epoch   0 Batch  170/215   train_loss = 5.493\n",
      "Epoch   0 Batch  180/215   train_loss = 5.547\n",
      "Epoch   0 Batch  190/215   train_loss = 5.433\n",
      "Epoch   0 Batch  200/215   train_loss = 5.351\n",
      "Epoch   0 Batch  210/215   train_loss = 4.964\n",
      "Epoch   1 Batch    5/215   train_loss = 5.696\n",
      "Epoch   1 Batch   15/215   train_loss = 5.477\n",
      "Epoch   1 Batch   25/215   train_loss = 5.039\n",
      "Epoch   1 Batch   35/215   train_loss = 5.332\n",
      "Epoch   1 Batch   45/215   train_loss = 5.079\n",
      "Epoch   1 Batch   55/215   train_loss = 5.380\n",
      "Epoch   1 Batch   65/215   train_loss = 5.417\n",
      "Epoch   1 Batch   75/215   train_loss = 5.113\n",
      "Epoch   1 Batch   85/215   train_loss = 5.237\n",
      "Epoch   1 Batch   95/215   train_loss = 5.039\n",
      "Epoch   1 Batch  105/215   train_loss = 5.182\n",
      "Epoch   1 Batch  115/215   train_loss = 5.259\n",
      "Epoch   1 Batch  125/215   train_loss = 5.074\n",
      "Epoch   1 Batch  135/215   train_loss = 5.041\n",
      "Epoch   1 Batch  145/215   train_loss = 4.956\n",
      "Epoch   1 Batch  155/215   train_loss = 5.130\n",
      "Epoch   1 Batch  165/215   train_loss = 5.216\n",
      "Epoch   1 Batch  175/215   train_loss = 5.318\n",
      "Epoch   1 Batch  185/215   train_loss = 5.172\n",
      "Epoch   1 Batch  195/215   train_loss = 5.211\n",
      "Epoch   1 Batch  205/215   train_loss = 5.269\n",
      "Epoch   2 Batch    0/215   train_loss = 5.211\n",
      "Epoch   2 Batch   10/215   train_loss = 5.351\n",
      "Epoch   2 Batch   20/215   train_loss = 5.082\n",
      "Epoch   2 Batch   30/215   train_loss = 4.864\n",
      "Epoch   2 Batch   40/215   train_loss = 5.217\n",
      "Epoch   2 Batch   50/215   train_loss = 5.121\n",
      "Epoch   2 Batch   60/215   train_loss = 5.276\n",
      "Epoch   2 Batch   70/215   train_loss = 5.082\n",
      "Epoch   2 Batch   80/215   train_loss = 4.948\n",
      "Epoch   2 Batch   90/215   train_loss = 5.003\n",
      "Epoch   2 Batch  100/215   train_loss = 4.880\n",
      "Epoch   2 Batch  110/215   train_loss = 5.009\n",
      "Epoch   2 Batch  120/215   train_loss = 5.071\n",
      "Epoch   2 Batch  130/215   train_loss = 5.285\n",
      "Epoch   2 Batch  140/215   train_loss = 5.486\n",
      "Epoch   2 Batch  150/215   train_loss = 4.990\n",
      "Epoch   2 Batch  160/215   train_loss = 4.957\n",
      "Epoch   2 Batch  170/215   train_loss = 4.969\n",
      "Epoch   2 Batch  180/215   train_loss = 5.133\n",
      "Epoch   2 Batch  190/215   train_loss = 4.985\n",
      "Epoch   2 Batch  200/215   train_loss = 4.746\n",
      "Epoch   2 Batch  210/215   train_loss = 4.539\n",
      "Epoch   3 Batch    5/215   train_loss = 5.270\n",
      "Epoch   3 Batch   15/215   train_loss = 5.217\n",
      "Epoch   3 Batch   25/215   train_loss = 4.768\n",
      "Epoch   3 Batch   35/215   train_loss = 5.002\n",
      "Epoch   3 Batch   45/215   train_loss = 4.931\n",
      "Epoch   3 Batch   55/215   train_loss = 5.109\n",
      "Epoch   3 Batch   65/215   train_loss = 5.170\n",
      "Epoch   3 Batch   75/215   train_loss = 4.743\n",
      "Epoch   3 Batch   85/215   train_loss = 4.933\n",
      "Epoch   3 Batch   95/215   train_loss = 4.792\n",
      "Epoch   3 Batch  105/215   train_loss = 4.832\n",
      "Epoch   3 Batch  115/215   train_loss = 4.925\n",
      "Epoch   3 Batch  125/215   train_loss = 4.748\n",
      "Epoch   3 Batch  135/215   train_loss = 4.696\n",
      "Epoch   3 Batch  145/215   train_loss = 4.638\n",
      "Epoch   3 Batch  155/215   train_loss = 4.862\n",
      "Epoch   3 Batch  165/215   train_loss = 4.907\n",
      "Epoch   3 Batch  175/215   train_loss = 4.856\n",
      "Epoch   3 Batch  185/215   train_loss = 4.861\n",
      "Epoch   3 Batch  195/215   train_loss = 4.893\n",
      "Epoch   3 Batch  205/215   train_loss = 4.946\n",
      "Epoch   4 Batch    0/215   train_loss = 4.788\n",
      "Epoch   4 Batch   10/215   train_loss = 5.111\n",
      "Epoch   4 Batch   20/215   train_loss = 4.810\n",
      "Epoch   4 Batch   30/215   train_loss = 4.694\n",
      "Epoch   4 Batch   40/215   train_loss = 5.049\n",
      "Epoch   4 Batch   50/215   train_loss = 5.030\n",
      "Epoch   4 Batch   60/215   train_loss = 5.009\n",
      "Epoch   4 Batch   70/215   train_loss = 4.792\n",
      "Epoch   4 Batch   80/215   train_loss = 4.759\n",
      "Epoch   4 Batch   90/215   train_loss = 4.696\n",
      "Epoch   4 Batch  100/215   train_loss = 4.730\n",
      "Epoch   4 Batch  110/215   train_loss = 4.699\n",
      "Epoch   4 Batch  120/215   train_loss = 4.811\n",
      "Epoch   4 Batch  130/215   train_loss = 5.023\n",
      "Epoch   4 Batch  140/215   train_loss = 5.168\n",
      "Epoch   4 Batch  150/215   train_loss = 4.816\n",
      "Epoch   4 Batch  160/215   train_loss = 4.671\n",
      "Epoch   4 Batch  170/215   train_loss = 4.718\n",
      "Epoch   4 Batch  180/215   train_loss = 4.860\n",
      "Epoch   4 Batch  190/215   train_loss = 4.706\n",
      "Epoch   4 Batch  200/215   train_loss = 4.531\n",
      "Epoch   4 Batch  210/215   train_loss = 4.274\n",
      "Epoch   5 Batch    5/215   train_loss = 5.044\n",
      "Epoch   5 Batch   15/215   train_loss = 5.014\n",
      "Epoch   5 Batch   25/215   train_loss = 4.636\n",
      "Epoch   5 Batch   35/215   train_loss = 4.799\n",
      "Epoch   5 Batch   45/215   train_loss = 4.681\n",
      "Epoch   5 Batch   55/215   train_loss = 4.948\n",
      "Epoch   5 Batch   65/215   train_loss = 4.893\n",
      "Epoch   5 Batch   75/215   train_loss = 4.577\n",
      "Epoch   5 Batch   85/215   train_loss = 4.689\n",
      "Epoch   5 Batch   95/215   train_loss = 4.569\n",
      "Epoch   5 Batch  105/215   train_loss = 4.617\n",
      "Epoch   5 Batch  115/215   train_loss = 4.718\n",
      "Epoch   5 Batch  125/215   train_loss = 4.498\n",
      "Epoch   5 Batch  135/215   train_loss = 4.609\n",
      "Epoch   5 Batch  145/215   train_loss = 4.415\n",
      "Epoch   5 Batch  155/215   train_loss = 4.604\n",
      "Epoch   5 Batch  165/215   train_loss = 4.634\n",
      "Epoch   5 Batch  175/215   train_loss = 4.735\n",
      "Epoch   5 Batch  185/215   train_loss = 4.714\n",
      "Epoch   5 Batch  195/215   train_loss = 4.656\n",
      "Epoch   5 Batch  205/215   train_loss = 4.768\n",
      "Epoch   6 Batch    0/215   train_loss = 4.705\n",
      "Epoch   6 Batch   10/215   train_loss = 4.976\n",
      "Epoch   6 Batch   20/215   train_loss = 4.651\n",
      "Epoch   6 Batch   30/215   train_loss = 4.480\n",
      "Epoch   6 Batch   40/215   train_loss = 4.811\n",
      "Epoch   6 Batch   50/215   train_loss = 4.811\n",
      "Epoch   6 Batch   60/215   train_loss = 4.834\n",
      "Epoch   6 Batch   70/215   train_loss = 4.625\n",
      "Epoch   6 Batch   80/215   train_loss = 4.549\n",
      "Epoch   6 Batch   90/215   train_loss = 4.564\n",
      "Epoch   6 Batch  100/215   train_loss = 4.537\n",
      "Epoch   6 Batch  110/215   train_loss = 4.555\n",
      "Epoch   6 Batch  120/215   train_loss = 4.618\n",
      "Epoch   6 Batch  130/215   train_loss = 4.898\n",
      "Epoch   6 Batch  140/215   train_loss = 4.999\n",
      "Epoch   6 Batch  150/215   train_loss = 4.656\n",
      "Epoch   6 Batch  160/215   train_loss = 4.594\n",
      "Epoch   6 Batch  170/215   train_loss = 4.493\n",
      "Epoch   6 Batch  180/215   train_loss = 4.773\n",
      "Epoch   6 Batch  190/215   train_loss = 4.615\n",
      "Epoch   6 Batch  200/215   train_loss = 4.369\n",
      "Epoch   6 Batch  210/215   train_loss = 4.223\n",
      "Epoch   7 Batch    5/215   train_loss = 4.906\n",
      "Epoch   7 Batch   15/215   train_loss = 4.927\n",
      "Epoch   7 Batch   25/215   train_loss = 4.405\n",
      "Epoch   7 Batch   35/215   train_loss = 4.699\n",
      "Epoch   7 Batch   45/215   train_loss = 4.487\n",
      "Epoch   7 Batch   55/215   train_loss = 4.785\n",
      "Epoch   7 Batch   65/215   train_loss = 4.822\n",
      "Epoch   7 Batch   75/215   train_loss = 4.417\n",
      "Epoch   7 Batch   85/215   train_loss = 4.606\n",
      "Epoch   7 Batch   95/215   train_loss = 4.524\n",
      "Epoch   7 Batch  105/215   train_loss = 4.486\n",
      "Epoch   7 Batch  115/215   train_loss = 4.604\n",
      "Epoch   7 Batch  125/215   train_loss = 4.393\n",
      "Epoch   7 Batch  135/215   train_loss = 4.463\n",
      "Epoch   7 Batch  145/215   train_loss = 4.298\n",
      "Epoch   7 Batch  155/215   train_loss = 4.509\n",
      "Epoch   7 Batch  165/215   train_loss = 4.574\n",
      "Epoch   7 Batch  175/215   train_loss = 4.700\n",
      "Epoch   7 Batch  185/215   train_loss = 4.639\n",
      "Epoch   7 Batch  195/215   train_loss = 4.527\n",
      "Epoch   7 Batch  205/215   train_loss = 4.709\n",
      "Epoch   8 Batch    0/215   train_loss = 4.534\n",
      "Epoch   8 Batch   10/215   train_loss = 4.782\n",
      "Epoch   8 Batch   20/215   train_loss = 4.586\n",
      "Epoch   8 Batch   30/215   train_loss = 4.447\n",
      "Epoch   8 Batch   40/215   train_loss = 4.716\n",
      "Epoch   8 Batch   50/215   train_loss = 4.704\n",
      "Epoch   8 Batch   60/215   train_loss = 4.648\n",
      "Epoch   8 Batch   70/215   train_loss = 4.537\n",
      "Epoch   8 Batch   80/215   train_loss = 4.534\n",
      "Epoch   8 Batch   90/215   train_loss = 4.467\n",
      "Epoch   8 Batch  100/215   train_loss = 4.499\n",
      "Epoch   8 Batch  110/215   train_loss = 4.397\n",
      "Epoch   8 Batch  120/215   train_loss = 4.564\n",
      "Epoch   8 Batch  130/215   train_loss = 4.811\n",
      "Epoch   8 Batch  140/215   train_loss = 4.851\n",
      "Epoch   8 Batch  150/215   train_loss = 4.566\n",
      "Epoch   8 Batch  160/215   train_loss = 4.478\n",
      "Epoch   8 Batch  170/215   train_loss = 4.468\n",
      "Epoch   8 Batch  180/215   train_loss = 4.646\n",
      "Epoch   8 Batch  190/215   train_loss = 4.518\n",
      "Epoch   8 Batch  200/215   train_loss = 4.268\n",
      "Epoch   8 Batch  210/215   train_loss = 4.147\n",
      "Epoch   9 Batch    5/215   train_loss = 4.776\n",
      "Epoch   9 Batch   15/215   train_loss = 4.703\n",
      "Epoch   9 Batch   25/215   train_loss = 4.402\n",
      "Epoch   9 Batch   35/215   train_loss = 4.604\n",
      "Epoch   9 Batch   45/215   train_loss = 4.421\n",
      "Epoch   9 Batch   55/215   train_loss = 4.744\n",
      "Epoch   9 Batch   65/215   train_loss = 4.675\n",
      "Epoch   9 Batch   75/215   train_loss = 4.351\n",
      "Epoch   9 Batch   85/215   train_loss = 4.491\n",
      "Epoch   9 Batch   95/215   train_loss = 4.394\n",
      "Epoch   9 Batch  105/215   train_loss = 4.200\n",
      "Epoch   9 Batch  115/215   train_loss = 4.406\n",
      "Epoch   9 Batch  125/215   train_loss = 4.173\n",
      "Epoch   9 Batch  135/215   train_loss = 4.388\n",
      "Epoch   9 Batch  145/215   train_loss = 4.230\n",
      "Epoch   9 Batch  155/215   train_loss = 4.447\n",
      "Epoch   9 Batch  165/215   train_loss = 4.425\n",
      "Epoch   9 Batch  175/215   train_loss = 4.433\n",
      "Epoch   9 Batch  185/215   train_loss = 4.547\n",
      "Epoch   9 Batch  195/215   train_loss = 4.493\n",
      "Epoch   9 Batch  205/215   train_loss = 4.524\n",
      "Epoch  10 Batch    0/215   train_loss = 4.302\n",
      "Epoch  10 Batch   10/215   train_loss = 4.721\n",
      "Epoch  10 Batch   20/215   train_loss = 4.408\n",
      "Epoch  10 Batch   30/215   train_loss = 4.313\n",
      "Epoch  10 Batch   40/215   train_loss = 4.672\n",
      "Epoch  10 Batch   50/215   train_loss = 4.633\n",
      "Epoch  10 Batch   60/215   train_loss = 4.617\n",
      "Epoch  10 Batch   70/215   train_loss = 4.346\n",
      "Epoch  10 Batch   80/215   train_loss = 4.372\n",
      "Epoch  10 Batch   90/215   train_loss = 4.373\n",
      "Epoch  10 Batch  100/215   train_loss = 4.433\n",
      "Epoch  10 Batch  110/215   train_loss = 4.352\n",
      "Epoch  10 Batch  120/215   train_loss = 4.433\n",
      "Epoch  10 Batch  130/215   train_loss = 4.681\n",
      "Epoch  10 Batch  140/215   train_loss = 4.685\n",
      "Epoch  10 Batch  150/215   train_loss = 4.410\n",
      "Epoch  10 Batch  160/215   train_loss = 4.359\n",
      "Epoch  10 Batch  170/215   train_loss = 4.329\n",
      "Epoch  10 Batch  180/215   train_loss = 4.591\n",
      "Epoch  10 Batch  190/215   train_loss = 4.379\n",
      "Epoch  10 Batch  200/215   train_loss = 4.217\n",
      "Epoch  10 Batch  210/215   train_loss = 3.955\n",
      "Epoch  11 Batch    5/215   train_loss = 4.618\n",
      "Epoch  11 Batch   15/215   train_loss = 4.480\n",
      "Epoch  11 Batch   25/215   train_loss = 4.203\n",
      "Epoch  11 Batch   35/215   train_loss = 4.390\n",
      "Epoch  11 Batch   45/215   train_loss = 4.343\n",
      "Epoch  11 Batch   55/215   train_loss = 4.461\n",
      "Epoch  11 Batch   65/215   train_loss = 4.481\n",
      "Epoch  11 Batch   75/215   train_loss = 4.202\n",
      "Epoch  11 Batch   85/215   train_loss = 4.343\n",
      "Epoch  11 Batch   95/215   train_loss = 4.237\n",
      "Epoch  11 Batch  105/215   train_loss = 4.238\n",
      "Epoch  11 Batch  115/215   train_loss = 4.304\n",
      "Epoch  11 Batch  125/215   train_loss = 4.128\n",
      "Epoch  11 Batch  135/215   train_loss = 4.375\n",
      "Epoch  11 Batch  145/215   train_loss = 4.079\n",
      "Epoch  11 Batch  155/215   train_loss = 4.347\n",
      "Epoch  11 Batch  165/215   train_loss = 4.323\n",
      "Epoch  11 Batch  175/215   train_loss = 4.370\n",
      "Epoch  11 Batch  185/215   train_loss = 4.440\n",
      "Epoch  11 Batch  195/215   train_loss = 4.333\n",
      "Epoch  11 Batch  205/215   train_loss = 4.415\n",
      "Epoch  12 Batch    0/215   train_loss = 4.315\n",
      "Epoch  12 Batch   10/215   train_loss = 4.682\n",
      "Epoch  12 Batch   20/215   train_loss = 4.341\n",
      "Epoch  12 Batch   30/215   train_loss = 4.206\n",
      "Epoch  12 Batch   40/215   train_loss = 4.614\n",
      "Epoch  12 Batch   50/215   train_loss = 4.398\n",
      "Epoch  12 Batch   60/215   train_loss = 4.532\n",
      "Epoch  12 Batch   70/215   train_loss = 4.338\n",
      "Epoch  12 Batch   80/215   train_loss = 4.225\n",
      "Epoch  12 Batch   90/215   train_loss = 4.281\n",
      "Epoch  12 Batch  100/215   train_loss = 4.190\n",
      "Epoch  12 Batch  110/215   train_loss = 4.324\n",
      "Epoch  12 Batch  120/215   train_loss = 4.304\n",
      "Epoch  12 Batch  130/215   train_loss = 4.625\n",
      "Epoch  12 Batch  140/215   train_loss = 4.600\n",
      "Epoch  12 Batch  150/215   train_loss = 4.347\n",
      "Epoch  12 Batch  160/215   train_loss = 4.301\n",
      "Epoch  12 Batch  170/215   train_loss = 4.176\n",
      "Epoch  12 Batch  180/215   train_loss = 4.483\n",
      "Epoch  12 Batch  190/215   train_loss = 4.326\n",
      "Epoch  12 Batch  200/215   train_loss = 4.006\n",
      "Epoch  12 Batch  210/215   train_loss = 3.992\n",
      "Epoch  13 Batch    5/215   train_loss = 4.469\n",
      "Epoch  13 Batch   15/215   train_loss = 4.415\n",
      "Epoch  13 Batch   25/215   train_loss = 4.190\n",
      "Epoch  13 Batch   35/215   train_loss = 4.238\n",
      "Epoch  13 Batch   45/215   train_loss = 4.254\n",
      "Epoch  13 Batch   55/215   train_loss = 4.269\n",
      "Epoch  13 Batch   65/215   train_loss = 4.440\n",
      "Epoch  13 Batch   75/215   train_loss = 4.193\n",
      "Epoch  13 Batch   85/215   train_loss = 4.325\n",
      "Epoch  13 Batch   95/215   train_loss = 4.131\n",
      "Epoch  13 Batch  105/215   train_loss = 4.086\n",
      "Epoch  13 Batch  115/215   train_loss = 4.416\n",
      "Epoch  13 Batch  125/215   train_loss = 4.099\n",
      "Epoch  13 Batch  135/215   train_loss = 4.136\n",
      "Epoch  13 Batch  145/215   train_loss = 3.992\n",
      "Epoch  13 Batch  155/215   train_loss = 4.218\n",
      "Epoch  13 Batch  165/215   train_loss = 4.223\n",
      "Epoch  13 Batch  175/215   train_loss = 4.330\n",
      "Epoch  13 Batch  185/215   train_loss = 4.302\n",
      "Epoch  13 Batch  195/215   train_loss = 4.213\n",
      "Epoch  13 Batch  205/215   train_loss = 4.343\n",
      "Epoch  14 Batch    0/215   train_loss = 4.221\n",
      "Epoch  14 Batch   10/215   train_loss = 4.460\n",
      "Epoch  14 Batch   20/215   train_loss = 4.288\n",
      "Epoch  14 Batch   30/215   train_loss = 4.008\n",
      "Epoch  14 Batch   40/215   train_loss = 4.303\n",
      "Epoch  14 Batch   50/215   train_loss = 4.286\n",
      "Epoch  14 Batch   60/215   train_loss = 4.363\n",
      "Epoch  14 Batch   70/215   train_loss = 4.119\n",
      "Epoch  14 Batch   80/215   train_loss = 4.170\n",
      "Epoch  14 Batch   90/215   train_loss = 4.222\n",
      "Epoch  14 Batch  100/215   train_loss = 4.178\n",
      "Epoch  14 Batch  110/215   train_loss = 4.190\n",
      "Epoch  14 Batch  120/215   train_loss = 4.275\n",
      "Epoch  14 Batch  130/215   train_loss = 4.517\n",
      "Epoch  14 Batch  140/215   train_loss = 4.408\n",
      "Epoch  14 Batch  150/215   train_loss = 4.190\n",
      "Epoch  14 Batch  160/215   train_loss = 4.182\n",
      "Epoch  14 Batch  170/215   train_loss = 4.136\n",
      "Epoch  14 Batch  180/215   train_loss = 4.366\n",
      "Epoch  14 Batch  190/215   train_loss = 4.208\n",
      "Epoch  14 Batch  200/215   train_loss = 4.018\n",
      "Epoch  14 Batch  210/215   train_loss = 3.858\n",
      "Epoch  15 Batch    5/215   train_loss = 4.426\n",
      "Epoch  15 Batch   15/215   train_loss = 4.323\n",
      "Epoch  15 Batch   25/215   train_loss = 4.048\n",
      "Epoch  15 Batch   35/215   train_loss = 4.249\n",
      "Epoch  15 Batch   45/215   train_loss = 4.146\n",
      "Epoch  15 Batch   55/215   train_loss = 4.161\n",
      "Epoch  15 Batch   65/215   train_loss = 4.321\n",
      "Epoch  15 Batch   75/215   train_loss = 4.047\n",
      "Epoch  15 Batch   85/215   train_loss = 4.158\n",
      "Epoch  15 Batch   95/215   train_loss = 4.073\n",
      "Epoch  15 Batch  105/215   train_loss = 4.028\n",
      "Epoch  15 Batch  115/215   train_loss = 4.237\n",
      "Epoch  15 Batch  125/215   train_loss = 3.985\n",
      "Epoch  15 Batch  135/215   train_loss = 4.024\n",
      "Epoch  15 Batch  145/215   train_loss = 3.917\n",
      "Epoch  15 Batch  155/215   train_loss = 4.090\n",
      "Epoch  15 Batch  165/215   train_loss = 4.110\n",
      "Epoch  15 Batch  175/215   train_loss = 4.293\n",
      "Epoch  15 Batch  185/215   train_loss = 4.166\n",
      "Epoch  15 Batch  195/215   train_loss = 4.154\n",
      "Epoch  15 Batch  205/215   train_loss = 4.209\n",
      "Epoch  16 Batch    0/215   train_loss = 4.070\n",
      "Epoch  16 Batch   10/215   train_loss = 4.394\n",
      "Epoch  16 Batch   20/215   train_loss = 4.112\n",
      "Epoch  16 Batch   30/215   train_loss = 3.985\n",
      "Epoch  16 Batch   40/215   train_loss = 4.247\n",
      "Epoch  16 Batch   50/215   train_loss = 4.212\n",
      "Epoch  16 Batch   60/215   train_loss = 4.317\n",
      "Epoch  16 Batch   70/215   train_loss = 4.094\n",
      "Epoch  16 Batch   80/215   train_loss = 4.035\n",
      "Epoch  16 Batch   90/215   train_loss = 4.110\n",
      "Epoch  16 Batch  100/215   train_loss = 4.201\n",
      "Epoch  16 Batch  110/215   train_loss = 3.998\n",
      "Epoch  16 Batch  120/215   train_loss = 4.169\n",
      "Epoch  16 Batch  130/215   train_loss = 4.441\n",
      "Epoch  16 Batch  140/215   train_loss = 4.249\n",
      "Epoch  16 Batch  150/215   train_loss = 4.037\n",
      "Epoch  16 Batch  160/215   train_loss = 4.006\n",
      "Epoch  16 Batch  170/215   train_loss = 4.046\n",
      "Epoch  16 Batch  180/215   train_loss = 4.275\n",
      "Epoch  16 Batch  190/215   train_loss = 4.088\n",
      "Epoch  16 Batch  200/215   train_loss = 3.940\n",
      "Epoch  16 Batch  210/215   train_loss = 3.706\n",
      "Epoch  17 Batch    5/215   train_loss = 4.335\n",
      "Epoch  17 Batch   15/215   train_loss = 4.361\n",
      "Epoch  17 Batch   25/215   train_loss = 3.878\n",
      "Epoch  17 Batch   35/215   train_loss = 4.137\n",
      "Epoch  17 Batch   45/215   train_loss = 4.113\n",
      "Epoch  17 Batch   55/215   train_loss = 4.064\n",
      "Epoch  17 Batch   65/215   train_loss = 4.181\n",
      "Epoch  17 Batch   75/215   train_loss = 4.146\n",
      "Epoch  17 Batch   85/215   train_loss = 4.122\n",
      "Epoch  17 Batch   95/215   train_loss = 4.101\n",
      "Epoch  17 Batch  105/215   train_loss = 3.876\n",
      "Epoch  17 Batch  115/215   train_loss = 4.070\n",
      "Epoch  17 Batch  125/215   train_loss = 3.846\n",
      "Epoch  17 Batch  135/215   train_loss = 3.902\n",
      "Epoch  17 Batch  145/215   train_loss = 3.817\n",
      "Epoch  17 Batch  155/215   train_loss = 4.035\n",
      "Epoch  17 Batch  165/215   train_loss = 4.031\n",
      "Epoch  17 Batch  175/215   train_loss = 4.232\n",
      "Epoch  17 Batch  185/215   train_loss = 4.122\n",
      "Epoch  17 Batch  195/215   train_loss = 4.155\n",
      "Epoch  17 Batch  205/215   train_loss = 4.157\n",
      "Epoch  18 Batch    0/215   train_loss = 3.984\n",
      "Epoch  18 Batch   10/215   train_loss = 4.282\n",
      "Epoch  18 Batch   20/215   train_loss = 4.147\n",
      "Epoch  18 Batch   30/215   train_loss = 3.987\n",
      "Epoch  18 Batch   40/215   train_loss = 4.210\n",
      "Epoch  18 Batch   50/215   train_loss = 4.190\n",
      "Epoch  18 Batch   60/215   train_loss = 4.255\n",
      "Epoch  18 Batch   70/215   train_loss = 3.971\n",
      "Epoch  18 Batch   80/215   train_loss = 3.900\n",
      "Epoch  18 Batch   90/215   train_loss = 4.146\n",
      "Epoch  18 Batch  100/215   train_loss = 4.058\n",
      "Epoch  18 Batch  110/215   train_loss = 4.068\n",
      "Epoch  18 Batch  120/215   train_loss = 4.103\n",
      "Epoch  18 Batch  130/215   train_loss = 4.287\n",
      "Epoch  18 Batch  140/215   train_loss = 4.316\n",
      "Epoch  18 Batch  150/215   train_loss = 4.047\n",
      "Epoch  18 Batch  160/215   train_loss = 3.971\n",
      "Epoch  18 Batch  170/215   train_loss = 3.977\n",
      "Epoch  18 Batch  180/215   train_loss = 4.133\n",
      "Epoch  18 Batch  190/215   train_loss = 4.117\n",
      "Epoch  18 Batch  200/215   train_loss = 3.882\n",
      "Epoch  18 Batch  210/215   train_loss = 3.704\n",
      "Epoch  19 Batch    5/215   train_loss = 4.319\n",
      "Epoch  19 Batch   15/215   train_loss = 4.081\n",
      "Epoch  19 Batch   25/215   train_loss = 3.855\n",
      "Epoch  19 Batch   35/215   train_loss = 3.983\n",
      "Epoch  19 Batch   45/215   train_loss = 3.960\n",
      "Epoch  19 Batch   55/215   train_loss = 4.038\n",
      "Epoch  19 Batch   65/215   train_loss = 4.137\n",
      "Epoch  19 Batch   75/215   train_loss = 4.006\n",
      "Epoch  19 Batch   85/215   train_loss = 4.028\n",
      "Epoch  19 Batch   95/215   train_loss = 3.873\n",
      "Epoch  19 Batch  105/215   train_loss = 3.885\n",
      "Epoch  19 Batch  115/215   train_loss = 3.998\n",
      "Epoch  19 Batch  125/215   train_loss = 3.795\n",
      "Epoch  19 Batch  135/215   train_loss = 3.923\n",
      "Epoch  19 Batch  145/215   train_loss = 3.928\n",
      "Epoch  19 Batch  155/215   train_loss = 4.045\n",
      "Epoch  19 Batch  165/215   train_loss = 3.942\n",
      "Epoch  19 Batch  175/215   train_loss = 4.062\n",
      "Epoch  19 Batch  185/215   train_loss = 4.000\n",
      "Epoch  19 Batch  195/215   train_loss = 4.055\n",
      "Epoch  19 Batch  205/215   train_loss = 4.110\n",
      "Epoch  20 Batch    0/215   train_loss = 3.879\n",
      "Epoch  20 Batch   10/215   train_loss = 4.241\n",
      "Epoch  20 Batch   20/215   train_loss = 4.024\n",
      "Epoch  20 Batch   30/215   train_loss = 3.995\n",
      "Epoch  20 Batch   40/215   train_loss = 4.172\n",
      "Epoch  20 Batch   50/215   train_loss = 4.049\n",
      "Epoch  20 Batch   60/215   train_loss = 4.200\n",
      "Epoch  20 Batch   70/215   train_loss = 3.808\n",
      "Epoch  20 Batch   80/215   train_loss = 3.865\n",
      "Epoch  20 Batch   90/215   train_loss = 4.044\n",
      "Epoch  20 Batch  100/215   train_loss = 3.868\n",
      "Epoch  20 Batch  110/215   train_loss = 3.929\n",
      "Epoch  20 Batch  120/215   train_loss = 4.002\n",
      "Epoch  20 Batch  130/215   train_loss = 4.341\n",
      "Epoch  20 Batch  140/215   train_loss = 4.215\n",
      "Epoch  20 Batch  150/215   train_loss = 3.928\n",
      "Epoch  20 Batch  160/215   train_loss = 3.973\n",
      "Epoch  20 Batch  170/215   train_loss = 3.904\n",
      "Epoch  20 Batch  180/215   train_loss = 4.038\n",
      "Epoch  20 Batch  190/215   train_loss = 4.025\n",
      "Epoch  20 Batch  200/215   train_loss = 3.824\n",
      "Epoch  20 Batch  210/215   train_loss = 3.792\n",
      "Epoch  21 Batch    5/215   train_loss = 4.220\n",
      "Epoch  21 Batch   15/215   train_loss = 4.012\n",
      "Epoch  21 Batch   25/215   train_loss = 3.911\n",
      "Epoch  21 Batch   35/215   train_loss = 4.025\n",
      "Epoch  21 Batch   45/215   train_loss = 3.873\n",
      "Epoch  21 Batch   55/215   train_loss = 4.106\n",
      "Epoch  21 Batch   65/215   train_loss = 4.123\n",
      "Epoch  21 Batch   75/215   train_loss = 4.008\n",
      "Epoch  21 Batch   85/215   train_loss = 4.029\n",
      "Epoch  21 Batch   95/215   train_loss = 3.850\n",
      "Epoch  21 Batch  105/215   train_loss = 3.885\n",
      "Epoch  21 Batch  115/215   train_loss = 3.949\n",
      "Epoch  21 Batch  125/215   train_loss = 3.736\n",
      "Epoch  21 Batch  135/215   train_loss = 3.850\n",
      "Epoch  21 Batch  145/215   train_loss = 3.796\n",
      "Epoch  21 Batch  155/215   train_loss = 3.895\n",
      "Epoch  21 Batch  165/215   train_loss = 3.872\n",
      "Epoch  21 Batch  175/215   train_loss = 4.139\n",
      "Epoch  21 Batch  185/215   train_loss = 3.972\n",
      "Epoch  21 Batch  195/215   train_loss = 3.831\n",
      "Epoch  21 Batch  205/215   train_loss = 4.000\n",
      "Epoch  22 Batch    0/215   train_loss = 3.800\n",
      "Epoch  22 Batch   10/215   train_loss = 4.170\n",
      "Epoch  22 Batch   20/215   train_loss = 4.020\n",
      "Epoch  22 Batch   30/215   train_loss = 3.899\n",
      "Epoch  22 Batch   40/215   train_loss = 3.989\n",
      "Epoch  22 Batch   50/215   train_loss = 4.056\n",
      "Epoch  22 Batch   60/215   train_loss = 4.126\n",
      "Epoch  22 Batch   70/215   train_loss = 3.875\n",
      "Epoch  22 Batch   80/215   train_loss = 3.885\n",
      "Epoch  22 Batch   90/215   train_loss = 3.847\n",
      "Epoch  22 Batch  100/215   train_loss = 4.021\n",
      "Epoch  22 Batch  110/215   train_loss = 3.886\n",
      "Epoch  22 Batch  120/215   train_loss = 3.867\n",
      "Epoch  22 Batch  130/215   train_loss = 4.299\n",
      "Epoch  22 Batch  140/215   train_loss = 4.160\n",
      "Epoch  22 Batch  150/215   train_loss = 3.768\n",
      "Epoch  22 Batch  160/215   train_loss = 3.933\n",
      "Epoch  22 Batch  170/215   train_loss = 3.856\n",
      "Epoch  22 Batch  180/215   train_loss = 3.959\n",
      "Epoch  22 Batch  190/215   train_loss = 3.943\n",
      "Epoch  22 Batch  200/215   train_loss = 3.762\n",
      "Epoch  22 Batch  210/215   train_loss = 3.459\n",
      "Epoch  23 Batch    5/215   train_loss = 4.237\n",
      "Epoch  23 Batch   15/215   train_loss = 4.039\n",
      "Epoch  23 Batch   25/215   train_loss = 3.778\n",
      "Epoch  23 Batch   35/215   train_loss = 4.049\n",
      "Epoch  23 Batch   45/215   train_loss = 3.876\n",
      "Epoch  23 Batch   55/215   train_loss = 3.924\n",
      "Epoch  23 Batch   65/215   train_loss = 3.942\n",
      "Epoch  23 Batch   75/215   train_loss = 3.799\n",
      "Epoch  23 Batch   85/215   train_loss = 3.926\n",
      "Epoch  23 Batch   95/215   train_loss = 3.895\n",
      "Epoch  23 Batch  105/215   train_loss = 3.788\n",
      "Epoch  23 Batch  115/215   train_loss = 3.861\n",
      "Epoch  23 Batch  125/215   train_loss = 3.706\n",
      "Epoch  23 Batch  135/215   train_loss = 3.840\n",
      "Epoch  23 Batch  145/215   train_loss = 3.651\n",
      "Epoch  23 Batch  155/215   train_loss = 3.908\n",
      "Epoch  23 Batch  165/215   train_loss = 3.674\n",
      "Epoch  23 Batch  175/215   train_loss = 3.983\n",
      "Epoch  23 Batch  185/215   train_loss = 3.943\n",
      "Epoch  23 Batch  195/215   train_loss = 3.751\n",
      "Epoch  23 Batch  205/215   train_loss = 3.967\n",
      "Epoch  24 Batch    0/215   train_loss = 3.777\n",
      "Epoch  24 Batch   10/215   train_loss = 4.121\n",
      "Epoch  24 Batch   20/215   train_loss = 4.047\n",
      "Epoch  24 Batch   30/215   train_loss = 3.710\n",
      "Epoch  24 Batch   40/215   train_loss = 3.802\n",
      "Epoch  24 Batch   50/215   train_loss = 3.932\n",
      "Epoch  24 Batch   60/215   train_loss = 3.991\n",
      "Epoch  24 Batch   70/215   train_loss = 3.844\n",
      "Epoch  24 Batch   80/215   train_loss = 3.746\n",
      "Epoch  24 Batch   90/215   train_loss = 3.978\n",
      "Epoch  24 Batch  100/215   train_loss = 4.018\n",
      "Epoch  24 Batch  110/215   train_loss = 3.841\n",
      "Epoch  24 Batch  120/215   train_loss = 3.845\n",
      "Epoch  24 Batch  130/215   train_loss = 4.315\n",
      "Epoch  24 Batch  140/215   train_loss = 4.054\n",
      "Epoch  24 Batch  150/215   train_loss = 3.892\n",
      "Epoch  24 Batch  160/215   train_loss = 3.749\n",
      "Epoch  24 Batch  170/215   train_loss = 3.793\n",
      "Epoch  24 Batch  180/215   train_loss = 3.935\n",
      "Epoch  24 Batch  190/215   train_loss = 3.911\n",
      "Epoch  24 Batch  200/215   train_loss = 3.714\n",
      "Epoch  24 Batch  210/215   train_loss = 3.419\n",
      "Epoch  25 Batch    5/215   train_loss = 4.192\n",
      "Epoch  25 Batch   15/215   train_loss = 3.976\n",
      "Epoch  25 Batch   25/215   train_loss = 3.842\n",
      "Epoch  25 Batch   35/215   train_loss = 3.894\n",
      "Epoch  25 Batch   45/215   train_loss = 3.853\n",
      "Epoch  25 Batch   55/215   train_loss = 3.915\n",
      "Epoch  25 Batch   65/215   train_loss = 3.990\n",
      "Epoch  25 Batch   75/215   train_loss = 3.880\n",
      "Epoch  25 Batch   85/215   train_loss = 3.823\n",
      "Epoch  25 Batch   95/215   train_loss = 3.714\n",
      "Epoch  25 Batch  105/215   train_loss = 3.843\n",
      "Epoch  25 Batch  115/215   train_loss = 3.825\n",
      "Epoch  25 Batch  125/215   train_loss = 3.589\n",
      "Epoch  25 Batch  135/215   train_loss = 3.866\n",
      "Epoch  25 Batch  145/215   train_loss = 3.778\n",
      "Epoch  25 Batch  155/215   train_loss = 3.798\n",
      "Epoch  25 Batch  165/215   train_loss = 3.685\n",
      "Epoch  25 Batch  175/215   train_loss = 3.983\n",
      "Epoch  25 Batch  185/215   train_loss = 3.811\n",
      "Epoch  25 Batch  195/215   train_loss = 3.707\n",
      "Epoch  25 Batch  205/215   train_loss = 3.931\n",
      "Epoch  26 Batch    0/215   train_loss = 3.830\n",
      "Epoch  26 Batch   10/215   train_loss = 3.934\n",
      "Epoch  26 Batch   20/215   train_loss = 3.851\n",
      "Epoch  26 Batch   30/215   train_loss = 3.709\n",
      "Epoch  26 Batch   40/215   train_loss = 3.988\n",
      "Epoch  26 Batch   50/215   train_loss = 4.000\n",
      "Epoch  26 Batch   60/215   train_loss = 3.952\n",
      "Epoch  26 Batch   70/215   train_loss = 3.848\n",
      "Epoch  26 Batch   80/215   train_loss = 3.758\n",
      "Epoch  26 Batch   90/215   train_loss = 3.780\n",
      "Epoch  26 Batch  100/215   train_loss = 3.808\n",
      "Epoch  26 Batch  110/215   train_loss = 3.674\n",
      "Epoch  26 Batch  120/215   train_loss = 3.779\n",
      "Epoch  26 Batch  130/215   train_loss = 4.061\n",
      "Epoch  26 Batch  140/215   train_loss = 4.001\n",
      "Epoch  26 Batch  150/215   train_loss = 3.805\n",
      "Epoch  26 Batch  160/215   train_loss = 3.769\n",
      "Epoch  26 Batch  170/215   train_loss = 3.874\n",
      "Epoch  26 Batch  180/215   train_loss = 3.856\n",
      "Epoch  26 Batch  190/215   train_loss = 3.803\n",
      "Epoch  26 Batch  200/215   train_loss = 3.772\n",
      "Epoch  26 Batch  210/215   train_loss = 3.546\n",
      "Epoch  27 Batch    5/215   train_loss = 3.985\n",
      "Epoch  27 Batch   15/215   train_loss = 3.761\n",
      "Epoch  27 Batch   25/215   train_loss = 3.822\n",
      "Epoch  27 Batch   35/215   train_loss = 3.782\n",
      "Epoch  27 Batch   45/215   train_loss = 3.852\n",
      "Epoch  27 Batch   55/215   train_loss = 3.795\n",
      "Epoch  27 Batch   65/215   train_loss = 3.898\n",
      "Epoch  27 Batch   75/215   train_loss = 3.824\n",
      "Epoch  27 Batch   85/215   train_loss = 3.796\n",
      "Epoch  27 Batch   95/215   train_loss = 3.735\n",
      "Epoch  27 Batch  105/215   train_loss = 3.671\n",
      "Epoch  27 Batch  115/215   train_loss = 3.883\n",
      "Epoch  27 Batch  125/215   train_loss = 3.700\n",
      "Epoch  27 Batch  135/215   train_loss = 3.858\n",
      "Epoch  27 Batch  145/215   train_loss = 3.713\n",
      "Epoch  27 Batch  155/215   train_loss = 3.806\n",
      "Epoch  27 Batch  165/215   train_loss = 3.690\n",
      "Epoch  27 Batch  175/215   train_loss = 3.818\n",
      "Epoch  27 Batch  185/215   train_loss = 3.914\n",
      "Epoch  27 Batch  195/215   train_loss = 3.705\n",
      "Epoch  27 Batch  205/215   train_loss = 3.887\n",
      "Epoch  28 Batch    0/215   train_loss = 3.703\n",
      "Epoch  28 Batch   10/215   train_loss = 4.014\n",
      "Epoch  28 Batch   20/215   train_loss = 3.850\n",
      "Epoch  28 Batch   30/215   train_loss = 3.630\n",
      "Epoch  28 Batch   40/215   train_loss = 3.899\n",
      "Epoch  28 Batch   50/215   train_loss = 3.871\n",
      "Epoch  28 Batch   60/215   train_loss = 3.911\n",
      "Epoch  28 Batch   70/215   train_loss = 3.868\n",
      "Epoch  28 Batch   80/215   train_loss = 3.673\n",
      "Epoch  28 Batch   90/215   train_loss = 3.898\n",
      "Epoch  28 Batch  100/215   train_loss = 3.840\n",
      "Epoch  28 Batch  110/215   train_loss = 3.719\n",
      "Epoch  28 Batch  120/215   train_loss = 3.778\n",
      "Epoch  28 Batch  130/215   train_loss = 3.927\n",
      "Epoch  28 Batch  140/215   train_loss = 3.916\n",
      "Epoch  28 Batch  150/215   train_loss = 3.821\n",
      "Epoch  28 Batch  160/215   train_loss = 3.616\n",
      "Epoch  28 Batch  170/215   train_loss = 3.742\n",
      "Epoch  28 Batch  180/215   train_loss = 3.900\n",
      "Epoch  28 Batch  190/215   train_loss = 3.808\n",
      "Epoch  28 Batch  200/215   train_loss = 3.617\n",
      "Epoch  28 Batch  210/215   train_loss = 3.347\n",
      "Epoch  29 Batch    5/215   train_loss = 4.067\n",
      "Epoch  29 Batch   15/215   train_loss = 3.755\n",
      "Epoch  29 Batch   25/215   train_loss = 3.688\n",
      "Epoch  29 Batch   35/215   train_loss = 3.851\n",
      "Epoch  29 Batch   45/215   train_loss = 3.792\n",
      "Epoch  29 Batch   55/215   train_loss = 3.884\n",
      "Epoch  29 Batch   65/215   train_loss = 3.856\n",
      "Epoch  29 Batch   75/215   train_loss = 3.831\n",
      "Epoch  29 Batch   85/215   train_loss = 3.680\n",
      "Epoch  29 Batch   95/215   train_loss = 3.726\n",
      "Epoch  29 Batch  105/215   train_loss = 3.708\n",
      "Epoch  29 Batch  115/215   train_loss = 3.879\n",
      "Epoch  29 Batch  125/215   train_loss = 3.469\n",
      "Epoch  29 Batch  135/215   train_loss = 3.803\n",
      "Epoch  29 Batch  145/215   train_loss = 3.684\n",
      "Epoch  29 Batch  155/215   train_loss = 3.887\n",
      "Epoch  29 Batch  165/215   train_loss = 3.702\n",
      "Epoch  29 Batch  175/215   train_loss = 3.905\n",
      "Epoch  29 Batch  185/215   train_loss = 3.854\n",
      "Epoch  29 Batch  195/215   train_loss = 3.630\n",
      "Epoch  29 Batch  205/215   train_loss = 3.922\n",
      "Epoch  30 Batch    0/215   train_loss = 3.740\n",
      "Epoch  30 Batch   10/215   train_loss = 4.076\n",
      "Epoch  30 Batch   20/215   train_loss = 3.676\n",
      "Epoch  30 Batch   30/215   train_loss = 3.713\n",
      "Epoch  30 Batch   40/215   train_loss = 3.850\n",
      "Epoch  30 Batch   50/215   train_loss = 3.668\n",
      "Epoch  30 Batch   60/215   train_loss = 3.789\n",
      "Epoch  30 Batch   70/215   train_loss = 3.758\n",
      "Epoch  30 Batch   80/215   train_loss = 3.601\n",
      "Epoch  30 Batch   90/215   train_loss = 3.622\n",
      "Epoch  30 Batch  100/215   train_loss = 3.747\n",
      "Epoch  30 Batch  110/215   train_loss = 3.784\n",
      "Epoch  30 Batch  120/215   train_loss = 3.721\n",
      "Epoch  30 Batch  130/215   train_loss = 4.169\n",
      "Epoch  30 Batch  140/215   train_loss = 3.980\n",
      "Epoch  30 Batch  150/215   train_loss = 3.645\n",
      "Epoch  30 Batch  160/215   train_loss = 3.761\n",
      "Epoch  30 Batch  170/215   train_loss = 3.676\n",
      "Epoch  30 Batch  180/215   train_loss = 3.910\n",
      "Epoch  30 Batch  190/215   train_loss = 3.777\n",
      "Epoch  30 Batch  200/215   train_loss = 3.629\n",
      "Epoch  30 Batch  210/215   train_loss = 3.367\n",
      "Epoch  31 Batch    5/215   train_loss = 4.007\n",
      "Epoch  31 Batch   15/215   train_loss = 3.856\n",
      "Epoch  31 Batch   25/215   train_loss = 3.660\n",
      "Epoch  31 Batch   35/215   train_loss = 3.846\n",
      "Epoch  31 Batch   45/215   train_loss = 3.578\n",
      "Epoch  31 Batch   55/215   train_loss = 3.714\n",
      "Epoch  31 Batch   65/215   train_loss = 3.901\n",
      "Epoch  31 Batch   75/215   train_loss = 3.710\n",
      "Epoch  31 Batch   85/215   train_loss = 3.795\n",
      "Epoch  31 Batch   95/215   train_loss = 3.811\n",
      "Epoch  31 Batch  105/215   train_loss = 3.650\n",
      "Epoch  31 Batch  115/215   train_loss = 3.677\n",
      "Epoch  31 Batch  125/215   train_loss = 3.610\n",
      "Epoch  31 Batch  135/215   train_loss = 3.736\n",
      "Epoch  31 Batch  145/215   train_loss = 3.517\n",
      "Epoch  31 Batch  155/215   train_loss = 3.770\n",
      "Epoch  31 Batch  165/215   train_loss = 3.631\n",
      "Epoch  31 Batch  175/215   train_loss = 3.771\n",
      "Epoch  31 Batch  185/215   train_loss = 3.744\n",
      "Epoch  31 Batch  195/215   train_loss = 3.722\n",
      "Epoch  31 Batch  205/215   train_loss = 3.797\n",
      "Epoch  32 Batch    0/215   train_loss = 3.666\n",
      "Epoch  32 Batch   10/215   train_loss = 4.053\n",
      "Epoch  32 Batch   20/215   train_loss = 3.690\n",
      "Epoch  32 Batch   30/215   train_loss = 3.655\n",
      "Epoch  32 Batch   40/215   train_loss = 3.662\n",
      "Epoch  32 Batch   50/215   train_loss = 3.666\n",
      "Epoch  32 Batch   60/215   train_loss = 3.938\n",
      "Epoch  32 Batch   70/215   train_loss = 3.638\n",
      "Epoch  32 Batch   80/215   train_loss = 3.666\n",
      "Epoch  32 Batch   90/215   train_loss = 3.808\n",
      "Epoch  32 Batch  100/215   train_loss = 3.721\n",
      "Epoch  32 Batch  110/215   train_loss = 3.632\n",
      "Epoch  32 Batch  120/215   train_loss = 3.776\n",
      "Epoch  32 Batch  130/215   train_loss = 3.942\n",
      "Epoch  32 Batch  140/215   train_loss = 4.016\n",
      "Epoch  32 Batch  150/215   train_loss = 3.707\n",
      "Epoch  32 Batch  160/215   train_loss = 3.857\n",
      "Epoch  32 Batch  170/215   train_loss = 3.767\n",
      "Epoch  32 Batch  180/215   train_loss = 3.749\n",
      "Epoch  32 Batch  190/215   train_loss = 3.717\n",
      "Epoch  32 Batch  200/215   train_loss = 3.566\n",
      "Epoch  32 Batch  210/215   train_loss = 3.349\n",
      "Epoch  33 Batch    5/215   train_loss = 3.981\n",
      "Epoch  33 Batch   15/215   train_loss = 3.732\n",
      "Epoch  33 Batch   25/215   train_loss = 3.704\n",
      "Epoch  33 Batch   35/215   train_loss = 3.664\n",
      "Epoch  33 Batch   45/215   train_loss = 3.744\n",
      "Epoch  33 Batch   55/215   train_loss = 3.591\n",
      "Epoch  33 Batch   65/215   train_loss = 3.764\n",
      "Epoch  33 Batch   75/215   train_loss = 3.759\n",
      "Epoch  33 Batch   85/215   train_loss = 3.687\n",
      "Epoch  33 Batch   95/215   train_loss = 3.595\n",
      "Epoch  33 Batch  105/215   train_loss = 3.650\n",
      "Epoch  33 Batch  115/215   train_loss = 3.786\n",
      "Epoch  33 Batch  125/215   train_loss = 3.580\n",
      "Epoch  33 Batch  135/215   train_loss = 3.753\n",
      "Epoch  33 Batch  145/215   train_loss = 3.587\n",
      "Epoch  33 Batch  155/215   train_loss = 3.726\n",
      "Epoch  33 Batch  165/215   train_loss = 3.579\n",
      "Epoch  33 Batch  175/215   train_loss = 3.779\n",
      "Epoch  33 Batch  185/215   train_loss = 3.683\n",
      "Epoch  33 Batch  195/215   train_loss = 3.614\n",
      "Epoch  33 Batch  205/215   train_loss = 3.683\n",
      "Epoch  34 Batch    0/215   train_loss = 3.788\n",
      "Epoch  34 Batch   10/215   train_loss = 4.019\n",
      "Epoch  34 Batch   20/215   train_loss = 3.795\n",
      "Epoch  34 Batch   30/215   train_loss = 3.627\n",
      "Epoch  34 Batch   40/215   train_loss = 3.791\n",
      "Epoch  34 Batch   50/215   train_loss = 3.676\n",
      "Epoch  34 Batch   60/215   train_loss = 3.803\n",
      "Epoch  34 Batch   70/215   train_loss = 3.777\n",
      "Epoch  34 Batch   80/215   train_loss = 3.508\n",
      "Epoch  34 Batch   90/215   train_loss = 3.907\n",
      "Epoch  34 Batch  100/215   train_loss = 3.713\n",
      "Epoch  34 Batch  110/215   train_loss = 3.656\n",
      "Epoch  34 Batch  120/215   train_loss = 3.700\n",
      "Epoch  34 Batch  130/215   train_loss = 3.987\n",
      "Epoch  34 Batch  140/215   train_loss = 3.759\n",
      "Epoch  34 Batch  150/215   train_loss = 3.683\n",
      "Epoch  34 Batch  160/215   train_loss = 3.748\n",
      "Epoch  34 Batch  170/215   train_loss = 3.644\n",
      "Epoch  34 Batch  180/215   train_loss = 3.694\n",
      "Epoch  34 Batch  190/215   train_loss = 3.748\n",
      "Epoch  34 Batch  200/215   train_loss = 3.425\n",
      "Epoch  34 Batch  210/215   train_loss = 3.347\n",
      "Epoch  35 Batch    5/215   train_loss = 4.015\n",
      "Epoch  35 Batch   15/215   train_loss = 3.813\n",
      "Epoch  35 Batch   25/215   train_loss = 3.633\n",
      "Epoch  35 Batch   35/215   train_loss = 3.682\n",
      "Epoch  35 Batch   45/215   train_loss = 3.547\n",
      "Epoch  35 Batch   55/215   train_loss = 3.641\n",
      "Epoch  35 Batch   65/215   train_loss = 3.721\n",
      "Epoch  35 Batch   75/215   train_loss = 3.740\n",
      "Epoch  35 Batch   85/215   train_loss = 3.831\n",
      "Epoch  35 Batch   95/215   train_loss = 3.565\n",
      "Epoch  35 Batch  105/215   train_loss = 3.565\n",
      "Epoch  35 Batch  115/215   train_loss = 3.794\n",
      "Epoch  35 Batch  125/215   train_loss = 3.538\n",
      "Epoch  35 Batch  135/215   train_loss = 3.749\n",
      "Epoch  35 Batch  145/215   train_loss = 3.610\n",
      "Epoch  35 Batch  155/215   train_loss = 3.683\n",
      "Epoch  35 Batch  165/215   train_loss = 3.527\n",
      "Epoch  35 Batch  175/215   train_loss = 3.696\n",
      "Epoch  35 Batch  185/215   train_loss = 3.660\n",
      "Epoch  35 Batch  195/215   train_loss = 3.517\n",
      "Epoch  35 Batch  205/215   train_loss = 3.762\n",
      "Epoch  36 Batch    0/215   train_loss = 3.624\n",
      "Epoch  36 Batch   10/215   train_loss = 3.806\n",
      "Epoch  36 Batch   20/215   train_loss = 3.769\n",
      "Epoch  36 Batch   30/215   train_loss = 3.437\n",
      "Epoch  36 Batch   40/215   train_loss = 3.668\n",
      "Epoch  36 Batch   50/215   train_loss = 3.614\n",
      "Epoch  36 Batch   60/215   train_loss = 3.781\n",
      "Epoch  36 Batch   70/215   train_loss = 3.675\n",
      "Epoch  36 Batch   80/215   train_loss = 3.635\n",
      "Epoch  36 Batch   90/215   train_loss = 3.813\n",
      "Epoch  36 Batch  100/215   train_loss = 3.719\n",
      "Epoch  36 Batch  110/215   train_loss = 3.755\n",
      "Epoch  36 Batch  120/215   train_loss = 3.713\n",
      "Epoch  36 Batch  130/215   train_loss = 3.990\n",
      "Epoch  36 Batch  140/215   train_loss = 3.930\n",
      "Epoch  36 Batch  150/215   train_loss = 3.642\n",
      "Epoch  36 Batch  160/215   train_loss = 3.592\n",
      "Epoch  36 Batch  170/215   train_loss = 3.477\n",
      "Epoch  36 Batch  180/215   train_loss = 3.701\n",
      "Epoch  36 Batch  190/215   train_loss = 3.682\n",
      "Epoch  36 Batch  200/215   train_loss = 3.501\n",
      "Epoch  36 Batch  210/215   train_loss = 3.442\n",
      "Epoch  37 Batch    5/215   train_loss = 3.941\n",
      "Epoch  37 Batch   15/215   train_loss = 3.707\n",
      "Epoch  37 Batch   25/215   train_loss = 3.521\n",
      "Epoch  37 Batch   35/215   train_loss = 3.599\n",
      "Epoch  37 Batch   45/215   train_loss = 3.585\n",
      "Epoch  37 Batch   55/215   train_loss = 3.677\n",
      "Epoch  37 Batch   65/215   train_loss = 3.627\n",
      "Epoch  37 Batch   75/215   train_loss = 3.532\n",
      "Epoch  37 Batch   85/215   train_loss = 3.758\n",
      "Epoch  37 Batch   95/215   train_loss = 3.590\n",
      "Epoch  37 Batch  105/215   train_loss = 3.566\n",
      "Epoch  37 Batch  115/215   train_loss = 3.647\n",
      "Epoch  37 Batch  125/215   train_loss = 3.630\n",
      "Epoch  37 Batch  135/215   train_loss = 3.663\n",
      "Epoch  37 Batch  145/215   train_loss = 3.503\n",
      "Epoch  37 Batch  155/215   train_loss = 3.578\n",
      "Epoch  37 Batch  165/215   train_loss = 3.565\n",
      "Epoch  37 Batch  175/215   train_loss = 3.731\n",
      "Epoch  37 Batch  185/215   train_loss = 3.584\n",
      "Epoch  37 Batch  195/215   train_loss = 3.657\n",
      "Epoch  37 Batch  205/215   train_loss = 3.655\n",
      "Epoch  38 Batch    0/215   train_loss = 3.701\n",
      "Epoch  38 Batch   10/215   train_loss = 3.967\n",
      "Epoch  38 Batch   20/215   train_loss = 3.724\n",
      "Epoch  38 Batch   30/215   train_loss = 3.496\n",
      "Epoch  38 Batch   40/215   train_loss = 3.588\n",
      "Epoch  38 Batch   50/215   train_loss = 3.600\n",
      "Epoch  38 Batch   60/215   train_loss = 3.785\n",
      "Epoch  38 Batch   70/215   train_loss = 3.629\n",
      "Epoch  38 Batch   80/215   train_loss = 3.457\n",
      "Epoch  38 Batch   90/215   train_loss = 3.693\n",
      "Epoch  38 Batch  100/215   train_loss = 3.804\n",
      "Epoch  38 Batch  110/215   train_loss = 3.582\n",
      "Epoch  38 Batch  120/215   train_loss = 3.582\n",
      "Epoch  38 Batch  130/215   train_loss = 3.962\n",
      "Epoch  38 Batch  140/215   train_loss = 3.846\n",
      "Epoch  38 Batch  150/215   train_loss = 3.593\n",
      "Epoch  38 Batch  160/215   train_loss = 3.661\n",
      "Epoch  38 Batch  170/215   train_loss = 3.510\n",
      "Epoch  38 Batch  180/215   train_loss = 3.473\n",
      "Epoch  38 Batch  190/215   train_loss = 3.674\n",
      "Epoch  38 Batch  200/215   train_loss = 3.372\n",
      "Epoch  38 Batch  210/215   train_loss = 3.251\n",
      "Epoch  39 Batch    5/215   train_loss = 3.881\n",
      "Epoch  39 Batch   15/215   train_loss = 3.543\n",
      "Epoch  39 Batch   25/215   train_loss = 3.426\n",
      "Epoch  39 Batch   35/215   train_loss = 3.690\n",
      "Epoch  39 Batch   45/215   train_loss = 3.705\n",
      "Epoch  39 Batch   55/215   train_loss = 3.593\n",
      "Epoch  39 Batch   65/215   train_loss = 3.549\n",
      "Epoch  39 Batch   75/215   train_loss = 3.616\n",
      "Epoch  39 Batch   85/215   train_loss = 3.606\n",
      "Epoch  39 Batch   95/215   train_loss = 3.512\n",
      "Epoch  39 Batch  105/215   train_loss = 3.476\n",
      "Epoch  39 Batch  115/215   train_loss = 3.736\n",
      "Epoch  39 Batch  125/215   train_loss = 3.424\n",
      "Epoch  39 Batch  135/215   train_loss = 3.612\n",
      "Epoch  39 Batch  145/215   train_loss = 3.748\n",
      "Epoch  39 Batch  155/215   train_loss = 3.699\n",
      "Epoch  39 Batch  165/215   train_loss = 3.553\n",
      "Epoch  39 Batch  175/215   train_loss = 3.649\n",
      "Epoch  39 Batch  185/215   train_loss = 3.651\n",
      "Epoch  39 Batch  195/215   train_loss = 3.535\n",
      "Epoch  39 Batch  205/215   train_loss = 3.694\n",
      "Epoch  40 Batch    0/215   train_loss = 3.570\n",
      "Epoch  40 Batch   10/215   train_loss = 3.894\n",
      "Epoch  40 Batch   20/215   train_loss = 3.602\n",
      "Epoch  40 Batch   30/215   train_loss = 3.380\n",
      "Epoch  40 Batch   40/215   train_loss = 3.497\n",
      "Epoch  40 Batch   50/215   train_loss = 3.617\n",
      "Epoch  40 Batch   60/215   train_loss = 3.703\n",
      "Epoch  40 Batch   70/215   train_loss = 3.608\n",
      "Epoch  40 Batch   80/215   train_loss = 3.587\n",
      "Epoch  40 Batch   90/215   train_loss = 3.682\n",
      "Epoch  40 Batch  100/215   train_loss = 3.771\n",
      "Epoch  40 Batch  110/215   train_loss = 3.652\n",
      "Epoch  40 Batch  120/215   train_loss = 3.638\n",
      "Epoch  40 Batch  130/215   train_loss = 3.975\n",
      "Epoch  40 Batch  140/215   train_loss = 3.806\n",
      "Epoch  40 Batch  150/215   train_loss = 3.653\n",
      "Epoch  40 Batch  160/215   train_loss = 3.548\n",
      "Epoch  40 Batch  170/215   train_loss = 3.475\n",
      "Epoch  40 Batch  180/215   train_loss = 3.644\n",
      "Epoch  40 Batch  190/215   train_loss = 3.511\n",
      "Epoch  40 Batch  200/215   train_loss = 3.345\n",
      "Epoch  40 Batch  210/215   train_loss = 3.385\n",
      "Epoch  41 Batch    5/215   train_loss = 3.672\n",
      "Epoch  41 Batch   15/215   train_loss = 3.559\n",
      "Epoch  41 Batch   25/215   train_loss = 3.525\n",
      "Epoch  41 Batch   35/215   train_loss = 3.779\n",
      "Epoch  41 Batch   45/215   train_loss = 3.510\n",
      "Epoch  41 Batch   55/215   train_loss = 3.615\n",
      "Epoch  41 Batch   65/215   train_loss = 3.674\n",
      "Epoch  41 Batch   75/215   train_loss = 3.539\n",
      "Epoch  41 Batch   85/215   train_loss = 3.603\n",
      "Epoch  41 Batch   95/215   train_loss = 3.554\n",
      "Epoch  41 Batch  105/215   train_loss = 3.588\n",
      "Epoch  41 Batch  115/215   train_loss = 3.495\n",
      "Epoch  41 Batch  125/215   train_loss = 3.464\n",
      "Epoch  41 Batch  135/215   train_loss = 3.673\n",
      "Epoch  41 Batch  145/215   train_loss = 3.461\n",
      "Epoch  41 Batch  155/215   train_loss = 3.640\n",
      "Epoch  41 Batch  165/215   train_loss = 3.457\n",
      "Epoch  41 Batch  175/215   train_loss = 3.799\n",
      "Epoch  41 Batch  185/215   train_loss = 3.588\n",
      "Epoch  41 Batch  195/215   train_loss = 3.603\n",
      "Epoch  41 Batch  205/215   train_loss = 3.598\n",
      "Epoch  42 Batch    0/215   train_loss = 3.672\n",
      "Epoch  42 Batch   10/215   train_loss = 3.675\n",
      "Epoch  42 Batch   20/215   train_loss = 3.590\n",
      "Epoch  42 Batch   30/215   train_loss = 3.484\n",
      "Epoch  42 Batch   40/215   train_loss = 3.600\n",
      "Epoch  42 Batch   50/215   train_loss = 3.422\n",
      "Epoch  42 Batch   60/215   train_loss = 3.658\n",
      "Epoch  42 Batch   70/215   train_loss = 3.549\n",
      "Epoch  42 Batch   80/215   train_loss = 3.613\n",
      "Epoch  42 Batch   90/215   train_loss = 3.731\n",
      "Epoch  42 Batch  100/215   train_loss = 3.487\n",
      "Epoch  42 Batch  110/215   train_loss = 3.471\n",
      "Epoch  42 Batch  120/215   train_loss = 3.639\n",
      "Epoch  42 Batch  130/215   train_loss = 3.796\n",
      "Epoch  42 Batch  140/215   train_loss = 3.673\n",
      "Epoch  42 Batch  150/215   train_loss = 3.536\n",
      "Epoch  42 Batch  160/215   train_loss = 3.683\n",
      "Epoch  42 Batch  170/215   train_loss = 3.574\n",
      "Epoch  42 Batch  180/215   train_loss = 3.620\n",
      "Epoch  42 Batch  190/215   train_loss = 3.633\n",
      "Epoch  42 Batch  200/215   train_loss = 3.406\n",
      "Epoch  42 Batch  210/215   train_loss = 3.225\n",
      "Epoch  43 Batch    5/215   train_loss = 3.677\n",
      "Epoch  43 Batch   15/215   train_loss = 3.597\n",
      "Epoch  43 Batch   25/215   train_loss = 3.390\n",
      "Epoch  43 Batch   35/215   train_loss = 3.642\n",
      "Epoch  43 Batch   45/215   train_loss = 3.566\n",
      "Epoch  43 Batch   55/215   train_loss = 3.646\n",
      "Epoch  43 Batch   65/215   train_loss = 3.568\n",
      "Epoch  43 Batch   75/215   train_loss = 3.534\n",
      "Epoch  43 Batch   85/215   train_loss = 3.644\n",
      "Epoch  43 Batch   95/215   train_loss = 3.583\n",
      "Epoch  43 Batch  105/215   train_loss = 3.460\n",
      "Epoch  43 Batch  115/215   train_loss = 3.526\n",
      "Epoch  43 Batch  125/215   train_loss = 3.445\n",
      "Epoch  43 Batch  135/215   train_loss = 3.601\n",
      "Epoch  43 Batch  145/215   train_loss = 3.549\n",
      "Epoch  43 Batch  155/215   train_loss = 3.605\n",
      "Epoch  43 Batch  165/215   train_loss = 3.547\n",
      "Epoch  43 Batch  175/215   train_loss = 3.828\n",
      "Epoch  43 Batch  185/215   train_loss = 3.659\n",
      "Epoch  43 Batch  195/215   train_loss = 3.431\n",
      "Epoch  43 Batch  205/215   train_loss = 3.528\n",
      "Epoch  44 Batch    0/215   train_loss = 3.590\n",
      "Epoch  44 Batch   10/215   train_loss = 3.607\n",
      "Epoch  44 Batch   20/215   train_loss = 3.713\n",
      "Epoch  44 Batch   30/215   train_loss = 3.352\n",
      "Epoch  44 Batch   40/215   train_loss = 3.457\n",
      "Epoch  44 Batch   50/215   train_loss = 3.456\n",
      "Epoch  44 Batch   60/215   train_loss = 3.619\n",
      "Epoch  44 Batch   70/215   train_loss = 3.592\n",
      "Epoch  44 Batch   80/215   train_loss = 3.543\n",
      "Epoch  44 Batch   90/215   train_loss = 3.636\n",
      "Epoch  44 Batch  100/215   train_loss = 3.582\n",
      "Epoch  44 Batch  110/215   train_loss = 3.529\n",
      "Epoch  44 Batch  120/215   train_loss = 3.610\n",
      "Epoch  44 Batch  130/215   train_loss = 3.884\n",
      "Epoch  44 Batch  140/215   train_loss = 3.747\n",
      "Epoch  44 Batch  150/215   train_loss = 3.549\n",
      "Epoch  44 Batch  160/215   train_loss = 3.677\n",
      "Epoch  44 Batch  170/215   train_loss = 3.400\n",
      "Epoch  44 Batch  180/215   train_loss = 3.727\n",
      "Epoch  44 Batch  190/215   train_loss = 3.585\n",
      "Epoch  44 Batch  200/215   train_loss = 3.409\n",
      "Epoch  44 Batch  210/215   train_loss = 3.202\n",
      "Epoch  45 Batch    5/215   train_loss = 3.708\n",
      "Epoch  45 Batch   15/215   train_loss = 3.601\n",
      "Epoch  45 Batch   25/215   train_loss = 3.520\n",
      "Epoch  45 Batch   35/215   train_loss = 3.635\n",
      "Epoch  45 Batch   45/215   train_loss = 3.692\n",
      "Epoch  45 Batch   55/215   train_loss = 3.508\n",
      "Epoch  45 Batch   65/215   train_loss = 3.590\n",
      "Epoch  45 Batch   75/215   train_loss = 3.467\n",
      "Epoch  45 Batch   85/215   train_loss = 3.497\n",
      "Epoch  45 Batch   95/215   train_loss = 3.643\n",
      "Epoch  45 Batch  105/215   train_loss = 3.605\n",
      "Epoch  45 Batch  115/215   train_loss = 3.603\n",
      "Epoch  45 Batch  125/215   train_loss = 3.318\n",
      "Epoch  45 Batch  135/215   train_loss = 3.404\n",
      "Epoch  45 Batch  145/215   train_loss = 3.470\n",
      "Epoch  45 Batch  155/215   train_loss = 3.624\n",
      "Epoch  45 Batch  165/215   train_loss = 3.457\n",
      "Epoch  45 Batch  175/215   train_loss = 3.604\n",
      "Epoch  45 Batch  185/215   train_loss = 3.571\n",
      "Epoch  45 Batch  195/215   train_loss = 3.536\n",
      "Epoch  45 Batch  205/215   train_loss = 3.582\n",
      "Epoch  46 Batch    0/215   train_loss = 3.554\n",
      "Epoch  46 Batch   10/215   train_loss = 3.741\n",
      "Epoch  46 Batch   20/215   train_loss = 3.534\n",
      "Epoch  46 Batch   30/215   train_loss = 3.332\n",
      "Epoch  46 Batch   40/215   train_loss = 3.568\n",
      "Epoch  46 Batch   50/215   train_loss = 3.614\n",
      "Epoch  46 Batch   60/215   train_loss = 3.666\n",
      "Epoch  46 Batch   70/215   train_loss = 3.552\n",
      "Epoch  46 Batch   80/215   train_loss = 3.330\n",
      "Epoch  46 Batch   90/215   train_loss = 3.538\n",
      "Epoch  46 Batch  100/215   train_loss = 3.493\n",
      "Epoch  46 Batch  110/215   train_loss = 3.490\n",
      "Epoch  46 Batch  120/215   train_loss = 3.484\n",
      "Epoch  46 Batch  130/215   train_loss = 3.861\n",
      "Epoch  46 Batch  140/215   train_loss = 3.698\n",
      "Epoch  46 Batch  150/215   train_loss = 3.565\n",
      "Epoch  46 Batch  160/215   train_loss = 3.562\n",
      "Epoch  46 Batch  170/215   train_loss = 3.464\n",
      "Epoch  46 Batch  180/215   train_loss = 3.582\n",
      "Epoch  46 Batch  190/215   train_loss = 3.643\n",
      "Epoch  46 Batch  200/215   train_loss = 3.384\n",
      "Epoch  46 Batch  210/215   train_loss = 3.225\n",
      "Epoch  47 Batch    5/215   train_loss = 3.682\n",
      "Epoch  47 Batch   15/215   train_loss = 3.532\n",
      "Epoch  47 Batch   25/215   train_loss = 3.594\n",
      "Epoch  47 Batch   35/215   train_loss = 3.549\n",
      "Epoch  47 Batch   45/215   train_loss = 3.352\n",
      "Epoch  47 Batch   55/215   train_loss = 3.685\n",
      "Epoch  47 Batch   65/215   train_loss = 3.591\n",
      "Epoch  47 Batch   75/215   train_loss = 3.558\n",
      "Epoch  47 Batch   85/215   train_loss = 3.566\n",
      "Epoch  47 Batch   95/215   train_loss = 3.473\n",
      "Epoch  47 Batch  105/215   train_loss = 3.501\n",
      "Epoch  47 Batch  115/215   train_loss = 3.496\n",
      "Epoch  47 Batch  125/215   train_loss = 3.443\n",
      "Epoch  47 Batch  135/215   train_loss = 3.546\n",
      "Epoch  47 Batch  145/215   train_loss = 3.516\n",
      "Epoch  47 Batch  155/215   train_loss = 3.601\n",
      "Epoch  47 Batch  165/215   train_loss = 3.516\n",
      "Epoch  47 Batch  175/215   train_loss = 3.710\n",
      "Epoch  47 Batch  185/215   train_loss = 3.558\n",
      "Epoch  47 Batch  195/215   train_loss = 3.312\n",
      "Epoch  47 Batch  205/215   train_loss = 3.462\n",
      "Epoch  48 Batch    0/215   train_loss = 3.567\n",
      "Epoch  48 Batch   10/215   train_loss = 3.535\n",
      "Epoch  48 Batch   20/215   train_loss = 3.599\n",
      "Epoch  48 Batch   30/215   train_loss = 3.215\n",
      "Epoch  48 Batch   40/215   train_loss = 3.600\n",
      "Epoch  48 Batch   50/215   train_loss = 3.455\n",
      "Epoch  48 Batch   60/215   train_loss = 3.604\n",
      "Epoch  48 Batch   70/215   train_loss = 3.497\n",
      "Epoch  48 Batch   80/215   train_loss = 3.363\n",
      "Epoch  48 Batch   90/215   train_loss = 3.514\n",
      "Epoch  48 Batch  100/215   train_loss = 3.555\n",
      "Epoch  48 Batch  110/215   train_loss = 3.433\n",
      "Epoch  48 Batch  120/215   train_loss = 3.472\n",
      "Epoch  48 Batch  130/215   train_loss = 3.661\n",
      "Epoch  48 Batch  140/215   train_loss = 3.720\n",
      "Epoch  48 Batch  150/215   train_loss = 3.452\n",
      "Epoch  48 Batch  160/215   train_loss = 3.687\n",
      "Epoch  48 Batch  170/215   train_loss = 3.347\n",
      "Epoch  48 Batch  180/215   train_loss = 3.495\n",
      "Epoch  48 Batch  190/215   train_loss = 3.555\n",
      "Epoch  48 Batch  200/215   train_loss = 3.473\n",
      "Epoch  48 Batch  210/215   train_loss = 3.252\n",
      "Epoch  49 Batch    5/215   train_loss = 3.606\n",
      "Epoch  49 Batch   15/215   train_loss = 3.575\n",
      "Epoch  49 Batch   25/215   train_loss = 3.452\n",
      "Epoch  49 Batch   35/215   train_loss = 3.655\n",
      "Epoch  49 Batch   45/215   train_loss = 3.564\n",
      "Epoch  49 Batch   55/215   train_loss = 3.453\n",
      "Epoch  49 Batch   65/215   train_loss = 3.573\n",
      "Epoch  49 Batch   75/215   train_loss = 3.376\n",
      "Epoch  49 Batch   85/215   train_loss = 3.474\n",
      "Epoch  49 Batch   95/215   train_loss = 3.447\n",
      "Epoch  49 Batch  105/215   train_loss = 3.448\n",
      "Epoch  49 Batch  115/215   train_loss = 3.610\n",
      "Epoch  49 Batch  125/215   train_loss = 3.407\n",
      "Epoch  49 Batch  135/215   train_loss = 3.398\n",
      "Epoch  49 Batch  145/215   train_loss = 3.352\n",
      "Epoch  49 Batch  155/215   train_loss = 3.585\n",
      "Epoch  49 Batch  165/215   train_loss = 3.508\n",
      "Epoch  49 Batch  175/215   train_loss = 3.753\n",
      "Epoch  49 Batch  185/215   train_loss = 3.605\n",
      "Epoch  49 Batch  195/215   train_loss = 3.364\n",
      "Epoch  49 Batch  205/215   train_loss = 3.351\n",
      "Epoch  50 Batch    0/215   train_loss = 3.575\n",
      "Epoch  50 Batch   10/215   train_loss = 3.758\n",
      "Epoch  50 Batch   20/215   train_loss = 3.587\n",
      "Epoch  50 Batch   30/215   train_loss = 3.420\n",
      "Epoch  50 Batch   40/215   train_loss = 3.435\n",
      "Epoch  50 Batch   50/215   train_loss = 3.420\n",
      "Epoch  50 Batch   60/215   train_loss = 3.659\n",
      "Epoch  50 Batch   70/215   train_loss = 3.501\n",
      "Epoch  50 Batch   80/215   train_loss = 3.442\n",
      "Epoch  50 Batch   90/215   train_loss = 3.473\n",
      "Epoch  50 Batch  100/215   train_loss = 3.554\n",
      "Epoch  50 Batch  110/215   train_loss = 3.545\n",
      "Epoch  50 Batch  120/215   train_loss = 3.542\n",
      "Epoch  50 Batch  130/215   train_loss = 3.835\n",
      "Epoch  50 Batch  140/215   train_loss = 3.698\n",
      "Epoch  50 Batch  150/215   train_loss = 3.429\n",
      "Epoch  50 Batch  160/215   train_loss = 3.472\n",
      "Epoch  50 Batch  170/215   train_loss = 3.501\n",
      "Epoch  50 Batch  180/215   train_loss = 3.618\n",
      "Epoch  50 Batch  190/215   train_loss = 3.493\n",
      "Epoch  50 Batch  200/215   train_loss = 3.305\n",
      "Epoch  50 Batch  210/215   train_loss = 3.205\n",
      "Epoch  51 Batch    5/215   train_loss = 3.598\n",
      "Epoch  51 Batch   15/215   train_loss = 3.497\n",
      "Epoch  51 Batch   25/215   train_loss = 3.331\n",
      "Epoch  51 Batch   35/215   train_loss = 3.567\n",
      "Epoch  51 Batch   45/215   train_loss = 3.548\n",
      "Epoch  51 Batch   55/215   train_loss = 3.422\n",
      "Epoch  51 Batch   65/215   train_loss = 3.610\n",
      "Epoch  51 Batch   75/215   train_loss = 3.441\n",
      "Epoch  51 Batch   85/215   train_loss = 3.487\n",
      "Epoch  51 Batch   95/215   train_loss = 3.455\n",
      "Epoch  51 Batch  105/215   train_loss = 3.517\n",
      "Epoch  51 Batch  115/215   train_loss = 3.509\n",
      "Epoch  51 Batch  125/215   train_loss = 3.348\n",
      "Epoch  51 Batch  135/215   train_loss = 3.472\n",
      "Epoch  51 Batch  145/215   train_loss = 3.373\n",
      "Epoch  51 Batch  155/215   train_loss = 3.668\n",
      "Epoch  51 Batch  165/215   train_loss = 3.522\n",
      "Epoch  51 Batch  175/215   train_loss = 3.582\n",
      "Epoch  51 Batch  185/215   train_loss = 3.359\n",
      "Epoch  51 Batch  195/215   train_loss = 3.350\n",
      "Epoch  51 Batch  205/215   train_loss = 3.424\n",
      "Epoch  52 Batch    0/215   train_loss = 3.661\n",
      "Epoch  52 Batch   10/215   train_loss = 3.619\n",
      "Epoch  52 Batch   20/215   train_loss = 3.548\n",
      "Epoch  52 Batch   30/215   train_loss = 3.545\n",
      "Epoch  52 Batch   40/215   train_loss = 3.535\n",
      "Epoch  52 Batch   50/215   train_loss = 3.394\n",
      "Epoch  52 Batch   60/215   train_loss = 3.590\n",
      "Epoch  52 Batch   70/215   train_loss = 3.480\n",
      "Epoch  52 Batch   80/215   train_loss = 3.421\n",
      "Epoch  52 Batch   90/215   train_loss = 3.562\n",
      "Epoch  52 Batch  100/215   train_loss = 3.568\n",
      "Epoch  52 Batch  110/215   train_loss = 3.449\n",
      "Epoch  52 Batch  120/215   train_loss = 3.533\n",
      "Epoch  52 Batch  130/215   train_loss = 3.727\n",
      "Epoch  52 Batch  140/215   train_loss = 3.484\n",
      "Epoch  52 Batch  150/215   train_loss = 3.486\n",
      "Epoch  52 Batch  160/215   train_loss = 3.518\n",
      "Epoch  52 Batch  170/215   train_loss = 3.468\n",
      "Epoch  52 Batch  180/215   train_loss = 3.587\n",
      "Epoch  52 Batch  190/215   train_loss = 3.526\n",
      "Epoch  52 Batch  200/215   train_loss = 3.382\n",
      "Epoch  52 Batch  210/215   train_loss = 3.182\n",
      "Epoch  53 Batch    5/215   train_loss = 3.648\n",
      "Epoch  53 Batch   15/215   train_loss = 3.471\n",
      "Epoch  53 Batch   25/215   train_loss = 3.434\n",
      "Epoch  53 Batch   35/215   train_loss = 3.415\n",
      "Epoch  53 Batch   45/215   train_loss = 3.489\n",
      "Epoch  53 Batch   55/215   train_loss = 3.525\n",
      "Epoch  53 Batch   65/215   train_loss = 3.646\n",
      "Epoch  53 Batch   75/215   train_loss = 3.437\n",
      "Epoch  53 Batch   85/215   train_loss = 3.386\n",
      "Epoch  53 Batch   95/215   train_loss = 3.479\n",
      "Epoch  53 Batch  105/215   train_loss = 3.460\n",
      "Epoch  53 Batch  115/215   train_loss = 3.517\n",
      "Epoch  53 Batch  125/215   train_loss = 3.291\n",
      "Epoch  53 Batch  135/215   train_loss = 3.434\n",
      "Epoch  53 Batch  145/215   train_loss = 3.448\n",
      "Epoch  53 Batch  155/215   train_loss = 3.658\n",
      "Epoch  53 Batch  165/215   train_loss = 3.342\n",
      "Epoch  53 Batch  175/215   train_loss = 3.605\n",
      "Epoch  53 Batch  185/215   train_loss = 3.512\n",
      "Epoch  53 Batch  195/215   train_loss = 3.511\n",
      "Epoch  53 Batch  205/215   train_loss = 3.459\n",
      "Epoch  54 Batch    0/215   train_loss = 3.569\n",
      "Epoch  54 Batch   10/215   train_loss = 3.675\n",
      "Epoch  54 Batch   20/215   train_loss = 3.424\n",
      "Epoch  54 Batch   30/215   train_loss = 3.248\n",
      "Epoch  54 Batch   40/215   train_loss = 3.634\n",
      "Epoch  54 Batch   50/215   train_loss = 3.512\n",
      "Epoch  54 Batch   60/215   train_loss = 3.446\n",
      "Epoch  54 Batch   70/215   train_loss = 3.472\n",
      "Epoch  54 Batch   80/215   train_loss = 3.501\n",
      "Epoch  54 Batch   90/215   train_loss = 3.437\n",
      "Epoch  54 Batch  100/215   train_loss = 3.605\n",
      "Epoch  54 Batch  110/215   train_loss = 3.421\n",
      "Epoch  54 Batch  120/215   train_loss = 3.440\n",
      "Epoch  54 Batch  130/215   train_loss = 3.848\n",
      "Epoch  54 Batch  140/215   train_loss = 3.502\n",
      "Epoch  54 Batch  150/215   train_loss = 3.366\n",
      "Epoch  54 Batch  160/215   train_loss = 3.483\n",
      "Epoch  54 Batch  170/215   train_loss = 3.457\n",
      "Epoch  54 Batch  180/215   train_loss = 3.590\n",
      "Epoch  54 Batch  190/215   train_loss = 3.508\n",
      "Epoch  54 Batch  200/215   train_loss = 3.500\n",
      "Epoch  54 Batch  210/215   train_loss = 3.194\n",
      "Epoch  55 Batch    5/215   train_loss = 3.751\n",
      "Epoch  55 Batch   15/215   train_loss = 3.401\n",
      "Epoch  55 Batch   25/215   train_loss = 3.478\n",
      "Epoch  55 Batch   35/215   train_loss = 3.373\n",
      "Epoch  55 Batch   45/215   train_loss = 3.469\n",
      "Epoch  55 Batch   55/215   train_loss = 3.577\n",
      "Epoch  55 Batch   65/215   train_loss = 3.564\n",
      "Epoch  55 Batch   75/215   train_loss = 3.523\n",
      "Epoch  55 Batch   85/215   train_loss = 3.630\n",
      "Epoch  55 Batch   95/215   train_loss = 3.435\n",
      "Epoch  55 Batch  105/215   train_loss = 3.395\n",
      "Epoch  55 Batch  115/215   train_loss = 3.394\n",
      "Epoch  55 Batch  125/215   train_loss = 3.270\n",
      "Epoch  55 Batch  135/215   train_loss = 3.451\n",
      "Epoch  55 Batch  145/215   train_loss = 3.373\n",
      "Epoch  55 Batch  155/215   train_loss = 3.352\n",
      "Epoch  55 Batch  165/215   train_loss = 3.512\n",
      "Epoch  55 Batch  175/215   train_loss = 3.624\n",
      "Epoch  55 Batch  185/215   train_loss = 3.587\n",
      "Epoch  55 Batch  195/215   train_loss = 3.544\n",
      "Epoch  55 Batch  205/215   train_loss = 3.438\n",
      "Epoch  56 Batch    0/215   train_loss = 3.481\n",
      "Epoch  56 Batch   10/215   train_loss = 3.638\n",
      "Epoch  56 Batch   20/215   train_loss = 3.510\n",
      "Epoch  56 Batch   30/215   train_loss = 3.412\n",
      "Epoch  56 Batch   40/215   train_loss = 3.366\n",
      "Epoch  56 Batch   50/215   train_loss = 3.583\n",
      "Epoch  56 Batch   60/215   train_loss = 3.628\n",
      "Epoch  56 Batch   70/215   train_loss = 3.408\n",
      "Epoch  56 Batch   80/215   train_loss = 3.390\n",
      "Epoch  56 Batch   90/215   train_loss = 3.540\n",
      "Epoch  56 Batch  100/215   train_loss = 3.459\n",
      "Epoch  56 Batch  110/215   train_loss = 3.406\n",
      "Epoch  56 Batch  120/215   train_loss = 3.380\n",
      "Epoch  56 Batch  130/215   train_loss = 3.748\n",
      "Epoch  56 Batch  140/215   train_loss = 3.467\n",
      "Epoch  56 Batch  150/215   train_loss = 3.464\n",
      "Epoch  56 Batch  160/215   train_loss = 3.427\n",
      "Epoch  56 Batch  170/215   train_loss = 3.326\n",
      "Epoch  56 Batch  180/215   train_loss = 3.503\n",
      "Epoch  56 Batch  190/215   train_loss = 3.507\n",
      "Epoch  56 Batch  200/215   train_loss = 3.257\n",
      "Epoch  56 Batch  210/215   train_loss = 3.191\n",
      "Epoch  57 Batch    5/215   train_loss = 3.481\n",
      "Epoch  57 Batch   15/215   train_loss = 3.527\n",
      "Epoch  57 Batch   25/215   train_loss = 3.301\n",
      "Epoch  57 Batch   35/215   train_loss = 3.512\n",
      "Epoch  57 Batch   45/215   train_loss = 3.521\n",
      "Epoch  57 Batch   55/215   train_loss = 3.477\n",
      "Epoch  57 Batch   65/215   train_loss = 3.656\n",
      "Epoch  57 Batch   75/215   train_loss = 3.443\n",
      "Epoch  57 Batch   85/215   train_loss = 3.304\n",
      "Epoch  57 Batch   95/215   train_loss = 3.366\n",
      "Epoch  57 Batch  105/215   train_loss = 3.221\n",
      "Epoch  57 Batch  115/215   train_loss = 3.349\n",
      "Epoch  57 Batch  125/215   train_loss = 3.133\n",
      "Epoch  57 Batch  135/215   train_loss = 3.406\n",
      "Epoch  57 Batch  145/215   train_loss = 3.354\n",
      "Epoch  57 Batch  155/215   train_loss = 3.490\n",
      "Epoch  57 Batch  165/215   train_loss = 3.430\n",
      "Epoch  57 Batch  175/215   train_loss = 3.549\n",
      "Epoch  57 Batch  185/215   train_loss = 3.423\n",
      "Epoch  57 Batch  195/215   train_loss = 3.503\n",
      "Epoch  57 Batch  205/215   train_loss = 3.474\n",
      "Epoch  58 Batch    0/215   train_loss = 3.407\n",
      "Epoch  58 Batch   10/215   train_loss = 3.559\n",
      "Epoch  58 Batch   20/215   train_loss = 3.480\n",
      "Epoch  58 Batch   30/215   train_loss = 3.326\n",
      "Epoch  58 Batch   40/215   train_loss = 3.493\n",
      "Epoch  58 Batch   50/215   train_loss = 3.441\n",
      "Epoch  58 Batch   60/215   train_loss = 3.594\n",
      "Epoch  58 Batch   70/215   train_loss = 3.482\n",
      "Epoch  58 Batch   80/215   train_loss = 3.502\n",
      "Epoch  58 Batch   90/215   train_loss = 3.555\n",
      "Epoch  58 Batch  100/215   train_loss = 3.299\n",
      "Epoch  58 Batch  110/215   train_loss = 3.357\n",
      "Epoch  58 Batch  120/215   train_loss = 3.356\n",
      "Epoch  58 Batch  130/215   train_loss = 3.739\n",
      "Epoch  58 Batch  140/215   train_loss = 3.557\n",
      "Epoch  58 Batch  150/215   train_loss = 3.557\n",
      "Epoch  58 Batch  160/215   train_loss = 3.457\n",
      "Epoch  58 Batch  170/215   train_loss = 3.615\n",
      "Epoch  58 Batch  180/215   train_loss = 3.537\n",
      "Epoch  58 Batch  190/215   train_loss = 3.380\n",
      "Epoch  58 Batch  200/215   train_loss = 3.345\n",
      "Epoch  58 Batch  210/215   train_loss = 3.177\n",
      "Epoch  59 Batch    5/215   train_loss = 3.577\n",
      "Epoch  59 Batch   15/215   train_loss = 3.362\n",
      "Epoch  59 Batch   25/215   train_loss = 3.423\n",
      "Epoch  59 Batch   35/215   train_loss = 3.377\n",
      "Epoch  59 Batch   45/215   train_loss = 3.445\n",
      "Epoch  59 Batch   55/215   train_loss = 3.359\n",
      "Epoch  59 Batch   65/215   train_loss = 3.601\n",
      "Epoch  59 Batch   75/215   train_loss = 3.522\n",
      "Epoch  59 Batch   85/215   train_loss = 3.314\n",
      "Epoch  59 Batch   95/215   train_loss = 3.317\n",
      "Epoch  59 Batch  105/215   train_loss = 3.411\n",
      "Epoch  59 Batch  115/215   train_loss = 3.320\n",
      "Epoch  59 Batch  125/215   train_loss = 3.133\n",
      "Epoch  59 Batch  135/215   train_loss = 3.411\n",
      "Epoch  59 Batch  145/215   train_loss = 3.432\n",
      "Epoch  59 Batch  155/215   train_loss = 3.406\n",
      "Epoch  59 Batch  165/215   train_loss = 3.367\n",
      "Epoch  59 Batch  175/215   train_loss = 3.637\n",
      "Epoch  59 Batch  185/215   train_loss = 3.466\n",
      "Epoch  59 Batch  195/215   train_loss = 3.564\n",
      "Epoch  59 Batch  205/215   train_loss = 3.381\n",
      "Epoch  60 Batch    0/215   train_loss = 3.395\n",
      "Epoch  60 Batch   10/215   train_loss = 3.469\n",
      "Epoch  60 Batch   20/215   train_loss = 3.450\n",
      "Epoch  60 Batch   30/215   train_loss = 3.405\n",
      "Epoch  60 Batch   40/215   train_loss = 3.515\n",
      "Epoch  60 Batch   50/215   train_loss = 3.333\n",
      "Epoch  60 Batch   60/215   train_loss = 3.391\n",
      "Epoch  60 Batch   70/215   train_loss = 3.361\n",
      "Epoch  60 Batch   80/215   train_loss = 3.508\n",
      "Epoch  60 Batch   90/215   train_loss = 3.530\n",
      "Epoch  60 Batch  100/215   train_loss = 3.548\n",
      "Epoch  60 Batch  110/215   train_loss = 3.497\n",
      "Epoch  60 Batch  120/215   train_loss = 3.547\n",
      "Epoch  60 Batch  130/215   train_loss = 3.857\n",
      "Epoch  60 Batch  140/215   train_loss = 3.476\n",
      "Epoch  60 Batch  150/215   train_loss = 3.390\n",
      "Epoch  60 Batch  160/215   train_loss = 3.658\n",
      "Epoch  60 Batch  170/215   train_loss = 3.262\n",
      "Epoch  60 Batch  180/215   train_loss = 3.457\n",
      "Epoch  60 Batch  190/215   train_loss = 3.476\n",
      "Epoch  60 Batch  200/215   train_loss = 3.265\n",
      "Epoch  60 Batch  210/215   train_loss = 3.205\n",
      "Epoch  61 Batch    5/215   train_loss = 3.602\n",
      "Epoch  61 Batch   15/215   train_loss = 3.446\n",
      "Epoch  61 Batch   25/215   train_loss = 3.361\n",
      "Epoch  61 Batch   35/215   train_loss = 3.480\n",
      "Epoch  61 Batch   45/215   train_loss = 3.453\n",
      "Epoch  61 Batch   55/215   train_loss = 3.369\n",
      "Epoch  61 Batch   65/215   train_loss = 3.465\n",
      "Epoch  61 Batch   75/215   train_loss = 3.533\n",
      "Epoch  61 Batch   85/215   train_loss = 3.319\n",
      "Epoch  61 Batch   95/215   train_loss = 3.506\n",
      "Epoch  61 Batch  105/215   train_loss = 3.319\n",
      "Epoch  61 Batch  115/215   train_loss = 3.348\n",
      "Epoch  61 Batch  125/215   train_loss = 3.329\n",
      "Epoch  61 Batch  135/215   train_loss = 3.327\n",
      "Epoch  61 Batch  145/215   train_loss = 3.281\n",
      "Epoch  61 Batch  155/215   train_loss = 3.379\n",
      "Epoch  61 Batch  165/215   train_loss = 3.323\n",
      "Epoch  61 Batch  175/215   train_loss = 3.473\n",
      "Epoch  61 Batch  185/215   train_loss = 3.566\n",
      "Epoch  61 Batch  195/215   train_loss = 3.313\n",
      "Epoch  61 Batch  205/215   train_loss = 3.370\n",
      "Epoch  62 Batch    0/215   train_loss = 3.300\n",
      "Epoch  62 Batch   10/215   train_loss = 3.519\n",
      "Epoch  62 Batch   20/215   train_loss = 3.477\n",
      "Epoch  62 Batch   30/215   train_loss = 3.363\n",
      "Epoch  62 Batch   40/215   train_loss = 3.300\n",
      "Epoch  62 Batch   50/215   train_loss = 3.293\n",
      "Epoch  62 Batch   60/215   train_loss = 3.486\n",
      "Epoch  62 Batch   70/215   train_loss = 3.256\n",
      "Epoch  62 Batch   80/215   train_loss = 3.329\n",
      "Epoch  62 Batch   90/215   train_loss = 3.403\n",
      "Epoch  62 Batch  100/215   train_loss = 3.420\n",
      "Epoch  62 Batch  110/215   train_loss = 3.354\n",
      "Epoch  62 Batch  120/215   train_loss = 3.389\n",
      "Epoch  62 Batch  130/215   train_loss = 3.684\n",
      "Epoch  62 Batch  140/215   train_loss = 3.535\n",
      "Epoch  62 Batch  150/215   train_loss = 3.475\n",
      "Epoch  62 Batch  160/215   train_loss = 3.349\n",
      "Epoch  62 Batch  170/215   train_loss = 3.263\n",
      "Epoch  62 Batch  180/215   train_loss = 3.423\n",
      "Epoch  62 Batch  190/215   train_loss = 3.412\n",
      "Epoch  62 Batch  200/215   train_loss = 3.275\n",
      "Epoch  62 Batch  210/215   train_loss = 3.173\n",
      "Epoch  63 Batch    5/215   train_loss = 3.590\n",
      "Epoch  63 Batch   15/215   train_loss = 3.310\n",
      "Epoch  63 Batch   25/215   train_loss = 3.415\n",
      "Epoch  63 Batch   35/215   train_loss = 3.446\n",
      "Epoch  63 Batch   45/215   train_loss = 3.293\n",
      "Epoch  63 Batch   55/215   train_loss = 3.410\n",
      "Epoch  63 Batch   65/215   train_loss = 3.428\n",
      "Epoch  63 Batch   75/215   train_loss = 3.475\n",
      "Epoch  63 Batch   85/215   train_loss = 3.365\n",
      "Epoch  63 Batch   95/215   train_loss = 3.417\n",
      "Epoch  63 Batch  105/215   train_loss = 3.294\n",
      "Epoch  63 Batch  115/215   train_loss = 3.152\n",
      "Epoch  63 Batch  125/215   train_loss = 3.236\n",
      "Epoch  63 Batch  135/215   train_loss = 3.191\n",
      "Epoch  63 Batch  145/215   train_loss = 3.276\n",
      "Epoch  63 Batch  155/215   train_loss = 3.572\n",
      "Epoch  63 Batch  165/215   train_loss = 3.373\n",
      "Epoch  63 Batch  175/215   train_loss = 3.532\n",
      "Epoch  63 Batch  185/215   train_loss = 3.407\n",
      "Epoch  63 Batch  195/215   train_loss = 3.351\n",
      "Epoch  63 Batch  205/215   train_loss = 3.361\n",
      "Epoch  64 Batch    0/215   train_loss = 3.329\n",
      "Epoch  64 Batch   10/215   train_loss = 3.469\n",
      "Epoch  64 Batch   20/215   train_loss = 3.472\n",
      "Epoch  64 Batch   30/215   train_loss = 3.226\n",
      "Epoch  64 Batch   40/215   train_loss = 3.380\n",
      "Epoch  64 Batch   50/215   train_loss = 3.315\n",
      "Epoch  64 Batch   60/215   train_loss = 3.609\n",
      "Epoch  64 Batch   70/215   train_loss = 3.444\n",
      "Epoch  64 Batch   80/215   train_loss = 3.440\n",
      "Epoch  64 Batch   90/215   train_loss = 3.597\n",
      "Epoch  64 Batch  100/215   train_loss = 3.313\n",
      "Epoch  64 Batch  110/215   train_loss = 3.377\n",
      "Epoch  64 Batch  120/215   train_loss = 3.328\n",
      "Epoch  64 Batch  130/215   train_loss = 3.548\n",
      "Epoch  64 Batch  140/215   train_loss = 3.479\n",
      "Epoch  64 Batch  150/215   train_loss = 3.503\n",
      "Epoch  64 Batch  160/215   train_loss = 3.571\n",
      "Epoch  64 Batch  170/215   train_loss = 3.185\n",
      "Epoch  64 Batch  180/215   train_loss = 3.408\n",
      "Epoch  64 Batch  190/215   train_loss = 3.344\n",
      "Epoch  64 Batch  200/215   train_loss = 3.244\n",
      "Epoch  64 Batch  210/215   train_loss = 3.255\n",
      "Epoch  65 Batch    5/215   train_loss = 3.513\n",
      "Epoch  65 Batch   15/215   train_loss = 3.386\n",
      "Epoch  65 Batch   25/215   train_loss = 3.379\n",
      "Epoch  65 Batch   35/215   train_loss = 3.409\n",
      "Epoch  65 Batch   45/215   train_loss = 3.296\n",
      "Epoch  65 Batch   55/215   train_loss = 3.269\n",
      "Epoch  65 Batch   65/215   train_loss = 3.429\n",
      "Epoch  65 Batch   75/215   train_loss = 3.409\n",
      "Epoch  65 Batch   85/215   train_loss = 3.262\n",
      "Epoch  65 Batch   95/215   train_loss = 3.428\n",
      "Epoch  65 Batch  105/215   train_loss = 3.219\n",
      "Epoch  65 Batch  115/215   train_loss = 3.334\n",
      "Epoch  65 Batch  125/215   train_loss = 3.215\n",
      "Epoch  65 Batch  135/215   train_loss = 3.346\n",
      "Epoch  65 Batch  145/215   train_loss = 3.392\n",
      "Epoch  65 Batch  155/215   train_loss = 3.529\n",
      "Epoch  65 Batch  165/215   train_loss = 3.302\n",
      "Epoch  65 Batch  175/215   train_loss = 3.510\n",
      "Epoch  65 Batch  185/215   train_loss = 3.415\n",
      "Epoch  65 Batch  195/215   train_loss = 3.384\n",
      "Epoch  65 Batch  205/215   train_loss = 3.316\n",
      "Epoch  66 Batch    0/215   train_loss = 3.346\n",
      "Epoch  66 Batch   10/215   train_loss = 3.459\n",
      "Epoch  66 Batch   20/215   train_loss = 3.324\n",
      "Epoch  66 Batch   30/215   train_loss = 3.272\n",
      "Epoch  66 Batch   40/215   train_loss = 3.333\n",
      "Epoch  66 Batch   50/215   train_loss = 3.388\n",
      "Epoch  66 Batch   60/215   train_loss = 3.443\n",
      "Epoch  66 Batch   70/215   train_loss = 3.289\n",
      "Epoch  66 Batch   80/215   train_loss = 3.304\n",
      "Epoch  66 Batch   90/215   train_loss = 3.510\n",
      "Epoch  66 Batch  100/215   train_loss = 3.465\n",
      "Epoch  66 Batch  110/215   train_loss = 3.330\n",
      "Epoch  66 Batch  120/215   train_loss = 3.359\n",
      "Epoch  66 Batch  130/215   train_loss = 3.659\n",
      "Epoch  66 Batch  140/215   train_loss = 3.522\n",
      "Epoch  66 Batch  150/215   train_loss = 3.573\n",
      "Epoch  66 Batch  160/215   train_loss = 3.444\n",
      "Epoch  66 Batch  170/215   train_loss = 3.191\n",
      "Epoch  66 Batch  180/215   train_loss = 3.470\n",
      "Epoch  66 Batch  190/215   train_loss = 3.542\n",
      "Epoch  66 Batch  200/215   train_loss = 3.298\n",
      "Epoch  66 Batch  210/215   train_loss = 3.287\n",
      "Epoch  67 Batch    5/215   train_loss = 3.394\n",
      "Epoch  67 Batch   15/215   train_loss = 3.314\n",
      "Epoch  67 Batch   25/215   train_loss = 3.438\n",
      "Epoch  67 Batch   35/215   train_loss = 3.458\n",
      "Epoch  67 Batch   45/215   train_loss = 3.296\n",
      "Epoch  67 Batch   55/215   train_loss = 3.393\n",
      "Epoch  67 Batch   65/215   train_loss = 3.414\n",
      "Epoch  67 Batch   75/215   train_loss = 3.450\n",
      "Epoch  67 Batch   85/215   train_loss = 3.359\n",
      "Epoch  67 Batch   95/215   train_loss = 3.265\n",
      "Epoch  67 Batch  105/215   train_loss = 3.420\n",
      "Epoch  67 Batch  115/215   train_loss = 3.253\n",
      "Epoch  67 Batch  125/215   train_loss = 3.153\n",
      "Epoch  67 Batch  135/215   train_loss = 3.170\n",
      "Epoch  67 Batch  145/215   train_loss = 3.301\n",
      "Epoch  67 Batch  155/215   train_loss = 3.550\n",
      "Epoch  67 Batch  165/215   train_loss = 3.395\n",
      "Epoch  67 Batch  175/215   train_loss = 3.453\n",
      "Epoch  67 Batch  185/215   train_loss = 3.454\n",
      "Epoch  67 Batch  195/215   train_loss = 3.428\n",
      "Epoch  67 Batch  205/215   train_loss = 3.404\n",
      "Epoch  68 Batch    0/215   train_loss = 3.382\n",
      "Epoch  68 Batch   10/215   train_loss = 3.572\n",
      "Epoch  68 Batch   20/215   train_loss = 3.427\n",
      "Epoch  68 Batch   30/215   train_loss = 3.254\n",
      "Epoch  68 Batch   40/215   train_loss = 3.252\n",
      "Epoch  68 Batch   50/215   train_loss = 3.359\n",
      "Epoch  68 Batch   60/215   train_loss = 3.553\n",
      "Epoch  68 Batch   70/215   train_loss = 3.302\n",
      "Epoch  68 Batch   80/215   train_loss = 3.350\n",
      "Epoch  68 Batch   90/215   train_loss = 3.422\n",
      "Epoch  68 Batch  100/215   train_loss = 3.424\n",
      "Epoch  68 Batch  110/215   train_loss = 3.262\n",
      "Epoch  68 Batch  120/215   train_loss = 3.440\n",
      "Epoch  68 Batch  130/215   train_loss = 3.438\n",
      "Epoch  68 Batch  140/215   train_loss = 3.405\n",
      "Epoch  68 Batch  150/215   train_loss = 3.304\n",
      "Epoch  68 Batch  160/215   train_loss = 3.436\n",
      "Epoch  68 Batch  170/215   train_loss = 3.441\n",
      "Epoch  68 Batch  180/215   train_loss = 3.328\n",
      "Epoch  68 Batch  190/215   train_loss = 3.329\n",
      "Epoch  68 Batch  200/215   train_loss = 3.312\n",
      "Epoch  68 Batch  210/215   train_loss = 3.127\n",
      "Epoch  69 Batch    5/215   train_loss = 3.466\n",
      "Epoch  69 Batch   15/215   train_loss = 3.441\n",
      "Epoch  69 Batch   25/215   train_loss = 3.299\n",
      "Epoch  69 Batch   35/215   train_loss = 3.478\n",
      "Epoch  69 Batch   45/215   train_loss = 3.291\n",
      "Epoch  69 Batch   55/215   train_loss = 3.356\n",
      "Epoch  69 Batch   65/215   train_loss = 3.361\n",
      "Epoch  69 Batch   75/215   train_loss = 3.518\n",
      "Epoch  69 Batch   85/215   train_loss = 3.327\n",
      "Epoch  69 Batch   95/215   train_loss = 3.303\n",
      "Epoch  69 Batch  105/215   train_loss = 3.204\n",
      "Epoch  69 Batch  115/215   train_loss = 3.289\n",
      "Epoch  69 Batch  125/215   train_loss = 3.313\n",
      "Epoch  69 Batch  135/215   train_loss = 3.286\n",
      "Epoch  69 Batch  145/215   train_loss = 3.276\n",
      "Epoch  69 Batch  155/215   train_loss = 3.432\n",
      "Epoch  69 Batch  165/215   train_loss = 3.408\n",
      "Epoch  69 Batch  175/215   train_loss = 3.543\n",
      "Epoch  69 Batch  185/215   train_loss = 3.473\n",
      "Epoch  69 Batch  195/215   train_loss = 3.405\n",
      "Epoch  69 Batch  205/215   train_loss = 3.220\n",
      "Epoch  70 Batch    0/215   train_loss = 3.345\n",
      "Epoch  70 Batch   10/215   train_loss = 3.497\n",
      "Epoch  70 Batch   20/215   train_loss = 3.273\n",
      "Epoch  70 Batch   30/215   train_loss = 3.325\n",
      "Epoch  70 Batch   40/215   train_loss = 3.284\n",
      "Epoch  70 Batch   50/215   train_loss = 3.271\n",
      "Epoch  70 Batch   60/215   train_loss = 3.424\n",
      "Epoch  70 Batch   70/215   train_loss = 3.468\n",
      "Epoch  70 Batch   80/215   train_loss = 3.393\n",
      "Epoch  70 Batch   90/215   train_loss = 3.449\n",
      "Epoch  70 Batch  100/215   train_loss = 3.476\n",
      "Epoch  70 Batch  110/215   train_loss = 3.339\n",
      "Epoch  70 Batch  120/215   train_loss = 3.392\n",
      "Epoch  70 Batch  130/215   train_loss = 3.506\n",
      "Epoch  70 Batch  140/215   train_loss = 3.454\n",
      "Epoch  70 Batch  150/215   train_loss = 3.381\n",
      "Epoch  70 Batch  160/215   train_loss = 3.412\n",
      "Epoch  70 Batch  170/215   train_loss = 3.243\n",
      "Epoch  70 Batch  180/215   train_loss = 3.374\n",
      "Epoch  70 Batch  190/215   train_loss = 3.621\n",
      "Epoch  70 Batch  200/215   train_loss = 3.206\n",
      "Epoch  70 Batch  210/215   train_loss = 3.157\n",
      "Epoch  71 Batch    5/215   train_loss = 3.397\n",
      "Epoch  71 Batch   15/215   train_loss = 3.343\n",
      "Epoch  71 Batch   25/215   train_loss = 3.254\n",
      "Epoch  71 Batch   35/215   train_loss = 3.300\n",
      "Epoch  71 Batch   45/215   train_loss = 3.333\n",
      "Epoch  71 Batch   55/215   train_loss = 3.220\n",
      "Epoch  71 Batch   65/215   train_loss = 3.459\n",
      "Epoch  71 Batch   75/215   train_loss = 3.370\n",
      "Epoch  71 Batch   85/215   train_loss = 3.193\n",
      "Epoch  71 Batch   95/215   train_loss = 3.258\n",
      "Epoch  71 Batch  105/215   train_loss = 3.206\n",
      "Epoch  71 Batch  115/215   train_loss = 3.233\n",
      "Epoch  71 Batch  125/215   train_loss = 3.144\n",
      "Epoch  71 Batch  135/215   train_loss = 3.330\n",
      "Epoch  71 Batch  145/215   train_loss = 3.293\n",
      "Epoch  71 Batch  155/215   train_loss = 3.464\n",
      "Epoch  71 Batch  165/215   train_loss = 3.270\n",
      "Epoch  71 Batch  175/215   train_loss = 3.519\n",
      "Epoch  71 Batch  185/215   train_loss = 3.343\n",
      "Epoch  71 Batch  195/215   train_loss = 3.295\n",
      "Epoch  71 Batch  205/215   train_loss = 3.259\n",
      "Epoch  72 Batch    0/215   train_loss = 3.177\n",
      "Epoch  72 Batch   10/215   train_loss = 3.520\n",
      "Epoch  72 Batch   20/215   train_loss = 3.321\n",
      "Epoch  72 Batch   30/215   train_loss = 3.487\n",
      "Epoch  72 Batch   40/215   train_loss = 3.140\n",
      "Epoch  72 Batch   50/215   train_loss = 3.265\n",
      "Epoch  72 Batch   60/215   train_loss = 3.453\n",
      "Epoch  72 Batch   70/215   train_loss = 3.318\n",
      "Epoch  72 Batch   80/215   train_loss = 3.337\n",
      "Epoch  72 Batch   90/215   train_loss = 3.341\n",
      "Epoch  72 Batch  100/215   train_loss = 3.466\n",
      "Epoch  72 Batch  110/215   train_loss = 3.494\n",
      "Epoch  72 Batch  120/215   train_loss = 3.393\n",
      "Epoch  72 Batch  130/215   train_loss = 3.631\n",
      "Epoch  72 Batch  140/215   train_loss = 3.470\n",
      "Epoch  72 Batch  150/215   train_loss = 3.375\n",
      "Epoch  72 Batch  160/215   train_loss = 3.450\n",
      "Epoch  72 Batch  170/215   train_loss = 3.287\n",
      "Epoch  72 Batch  180/215   train_loss = 3.273\n",
      "Epoch  72 Batch  190/215   train_loss = 3.356\n",
      "Epoch  72 Batch  200/215   train_loss = 3.096\n",
      "Epoch  72 Batch  210/215   train_loss = 3.075\n",
      "Epoch  73 Batch    5/215   train_loss = 3.472\n",
      "Epoch  73 Batch   15/215   train_loss = 3.315\n",
      "Epoch  73 Batch   25/215   train_loss = 3.322\n",
      "Epoch  73 Batch   35/215   train_loss = 3.351\n",
      "Epoch  73 Batch   45/215   train_loss = 3.376\n",
      "Epoch  73 Batch   55/215   train_loss = 3.234\n",
      "Epoch  73 Batch   65/215   train_loss = 3.575\n",
      "Epoch  73 Batch   75/215   train_loss = 3.382\n",
      "Epoch  73 Batch   85/215   train_loss = 3.222\n",
      "Epoch  73 Batch   95/215   train_loss = 3.256\n",
      "Epoch  73 Batch  105/215   train_loss = 3.113\n",
      "Epoch  73 Batch  115/215   train_loss = 3.363\n",
      "Epoch  73 Batch  125/215   train_loss = 3.253\n",
      "Epoch  73 Batch  135/215   train_loss = 3.193\n",
      "Epoch  73 Batch  145/215   train_loss = 3.326\n",
      "Epoch  73 Batch  155/215   train_loss = 3.581\n",
      "Epoch  73 Batch  165/215   train_loss = 3.204\n",
      "Epoch  73 Batch  175/215   train_loss = 3.444\n",
      "Epoch  73 Batch  185/215   train_loss = 3.302\n",
      "Epoch  73 Batch  195/215   train_loss = 3.455\n",
      "Epoch  73 Batch  205/215   train_loss = 3.304\n",
      "Epoch  74 Batch    0/215   train_loss = 3.325\n",
      "Epoch  74 Batch   10/215   train_loss = 3.463\n",
      "Epoch  74 Batch   20/215   train_loss = 3.493\n",
      "Epoch  74 Batch   30/215   train_loss = 3.429\n",
      "Epoch  74 Batch   40/215   train_loss = 3.360\n",
      "Epoch  74 Batch   50/215   train_loss = 3.302\n",
      "Epoch  74 Batch   60/215   train_loss = 3.462\n",
      "Epoch  74 Batch   70/215   train_loss = 3.446\n",
      "Epoch  74 Batch   80/215   train_loss = 3.259\n",
      "Epoch  74 Batch   90/215   train_loss = 3.433\n",
      "Epoch  74 Batch  100/215   train_loss = 3.254\n",
      "Epoch  74 Batch  110/215   train_loss = 3.273\n",
      "Epoch  74 Batch  120/215   train_loss = 3.406\n",
      "Epoch  74 Batch  130/215   train_loss = 3.660\n",
      "Epoch  74 Batch  140/215   train_loss = 3.591\n",
      "Epoch  74 Batch  150/215   train_loss = 3.257\n",
      "Epoch  74 Batch  160/215   train_loss = 3.284\n",
      "Epoch  74 Batch  170/215   train_loss = 3.441\n",
      "Epoch  74 Batch  180/215   train_loss = 3.298\n",
      "Epoch  74 Batch  190/215   train_loss = 3.294\n",
      "Epoch  74 Batch  200/215   train_loss = 3.455\n",
      "Epoch  74 Batch  210/215   train_loss = 3.186\n",
      "Epoch  75 Batch    5/215   train_loss = 3.263\n",
      "Epoch  75 Batch   15/215   train_loss = 3.179\n",
      "Epoch  75 Batch   25/215   train_loss = 3.273\n",
      "Epoch  75 Batch   35/215   train_loss = 3.295\n",
      "Epoch  75 Batch   45/215   train_loss = 3.263\n",
      "Epoch  75 Batch   55/215   train_loss = 3.278\n",
      "Epoch  75 Batch   65/215   train_loss = 3.306\n",
      "Epoch  75 Batch   75/215   train_loss = 3.168\n",
      "Epoch  75 Batch   85/215   train_loss = 3.194\n",
      "Epoch  75 Batch   95/215   train_loss = 3.296\n",
      "Epoch  75 Batch  105/215   train_loss = 3.184\n",
      "Epoch  75 Batch  115/215   train_loss = 3.191\n",
      "Epoch  75 Batch  125/215   train_loss = 3.145\n",
      "Epoch  75 Batch  135/215   train_loss = 3.135\n",
      "Epoch  75 Batch  145/215   train_loss = 3.169\n",
      "Epoch  75 Batch  155/215   train_loss = 3.421\n",
      "Epoch  75 Batch  165/215   train_loss = 3.300\n",
      "Epoch  75 Batch  175/215   train_loss = 3.369\n",
      "Epoch  75 Batch  185/215   train_loss = 3.393\n",
      "Epoch  75 Batch  195/215   train_loss = 3.492\n",
      "Epoch  75 Batch  205/215   train_loss = 3.256\n",
      "Epoch  76 Batch    0/215   train_loss = 3.426\n",
      "Epoch  76 Batch   10/215   train_loss = 3.283\n",
      "Epoch  76 Batch   20/215   train_loss = 3.282\n",
      "Epoch  76 Batch   30/215   train_loss = 3.250\n",
      "Epoch  76 Batch   40/215   train_loss = 3.300\n",
      "Epoch  76 Batch   50/215   train_loss = 3.152\n",
      "Epoch  76 Batch   60/215   train_loss = 3.333\n",
      "Epoch  76 Batch   70/215   train_loss = 3.418\n",
      "Epoch  76 Batch   80/215   train_loss = 3.241\n",
      "Epoch  76 Batch   90/215   train_loss = 3.278\n",
      "Epoch  76 Batch  100/215   train_loss = 3.291\n",
      "Epoch  76 Batch  110/215   train_loss = 3.136\n",
      "Epoch  76 Batch  120/215   train_loss = 3.347\n",
      "Epoch  76 Batch  130/215   train_loss = 3.484\n",
      "Epoch  76 Batch  140/215   train_loss = 3.448\n",
      "Epoch  76 Batch  150/215   train_loss = 3.463\n",
      "Epoch  76 Batch  160/215   train_loss = 3.414\n",
      "Epoch  76 Batch  170/215   train_loss = 3.136\n",
      "Epoch  76 Batch  180/215   train_loss = 3.340\n",
      "Epoch  76 Batch  190/215   train_loss = 3.368\n",
      "Epoch  76 Batch  200/215   train_loss = 3.275\n",
      "Epoch  76 Batch  210/215   train_loss = 3.071\n",
      "Epoch  77 Batch    5/215   train_loss = 3.494\n",
      "Epoch  77 Batch   15/215   train_loss = 3.297\n",
      "Epoch  77 Batch   25/215   train_loss = 3.258\n",
      "Epoch  77 Batch   35/215   train_loss = 3.394\n",
      "Epoch  77 Batch   45/215   train_loss = 3.271\n",
      "Epoch  77 Batch   55/215   train_loss = 3.313\n",
      "Epoch  77 Batch   65/215   train_loss = 3.184\n",
      "Epoch  77 Batch   75/215   train_loss = 3.237\n",
      "Epoch  77 Batch   85/215   train_loss = 3.336\n",
      "Epoch  77 Batch   95/215   train_loss = 3.182\n",
      "Epoch  77 Batch  105/215   train_loss = 3.278\n",
      "Epoch  77 Batch  115/215   train_loss = 3.330\n",
      "Epoch  77 Batch  125/215   train_loss = 3.228\n",
      "Epoch  77 Batch  135/215   train_loss = 3.172\n",
      "Epoch  77 Batch  145/215   train_loss = 3.152\n",
      "Epoch  77 Batch  155/215   train_loss = 3.468\n",
      "Epoch  77 Batch  165/215   train_loss = 3.351\n",
      "Epoch  77 Batch  175/215   train_loss = 3.505\n",
      "Epoch  77 Batch  185/215   train_loss = 3.268\n",
      "Epoch  77 Batch  195/215   train_loss = 3.320\n",
      "Epoch  77 Batch  205/215   train_loss = 3.222\n",
      "Epoch  78 Batch    0/215   train_loss = 3.387\n",
      "Epoch  78 Batch   10/215   train_loss = 3.454\n",
      "Epoch  78 Batch   20/215   train_loss = 3.294\n",
      "Epoch  78 Batch   30/215   train_loss = 3.309\n",
      "Epoch  78 Batch   40/215   train_loss = 3.146\n",
      "Epoch  78 Batch   50/215   train_loss = 3.197\n",
      "Epoch  78 Batch   60/215   train_loss = 3.429\n",
      "Epoch  78 Batch   70/215   train_loss = 3.409\n",
      "Epoch  78 Batch   80/215   train_loss = 3.197\n",
      "Epoch  78 Batch   90/215   train_loss = 3.260\n",
      "Epoch  78 Batch  100/215   train_loss = 3.377\n",
      "Epoch  78 Batch  110/215   train_loss = 3.302\n",
      "Epoch  78 Batch  120/215   train_loss = 3.457\n",
      "Epoch  78 Batch  130/215   train_loss = 3.679\n",
      "Epoch  78 Batch  140/215   train_loss = 3.412\n",
      "Epoch  78 Batch  150/215   train_loss = 3.266\n",
      "Epoch  78 Batch  160/215   train_loss = 3.471\n",
      "Epoch  78 Batch  170/215   train_loss = 3.166\n",
      "Epoch  78 Batch  180/215   train_loss = 3.289\n",
      "Epoch  78 Batch  190/215   train_loss = 3.470\n",
      "Epoch  78 Batch  200/215   train_loss = 3.105\n",
      "Epoch  78 Batch  210/215   train_loss = 3.074\n",
      "Epoch  79 Batch    5/215   train_loss = 3.652\n",
      "Epoch  79 Batch   15/215   train_loss = 3.352\n",
      "Epoch  79 Batch   25/215   train_loss = 3.202\n",
      "Epoch  79 Batch   35/215   train_loss = 3.474\n",
      "Epoch  79 Batch   45/215   train_loss = 3.170\n",
      "Epoch  79 Batch   55/215   train_loss = 3.296\n",
      "Epoch  79 Batch   65/215   train_loss = 3.464\n",
      "Epoch  79 Batch   75/215   train_loss = 3.362\n",
      "Epoch  79 Batch   85/215   train_loss = 3.227\n",
      "Epoch  79 Batch   95/215   train_loss = 3.337\n",
      "Epoch  79 Batch  105/215   train_loss = 3.258\n",
      "Epoch  79 Batch  115/215   train_loss = 3.237\n",
      "Epoch  79 Batch  125/215   train_loss = 3.229\n",
      "Epoch  79 Batch  135/215   train_loss = 3.163\n",
      "Epoch  79 Batch  145/215   train_loss = 3.167\n",
      "Epoch  79 Batch  155/215   train_loss = 3.288\n",
      "Epoch  79 Batch  165/215   train_loss = 3.309\n",
      "Epoch  79 Batch  175/215   train_loss = 3.445\n",
      "Epoch  79 Batch  185/215   train_loss = 3.405\n",
      "Epoch  79 Batch  195/215   train_loss = 3.218\n",
      "Epoch  79 Batch  205/215   train_loss = 3.378\n",
      "Epoch  80 Batch    0/215   train_loss = 3.404\n",
      "Epoch  80 Batch   10/215   train_loss = 3.334\n",
      "Epoch  80 Batch   20/215   train_loss = 3.233\n",
      "Epoch  80 Batch   30/215   train_loss = 3.274\n",
      "Epoch  80 Batch   40/215   train_loss = 3.222\n",
      "Epoch  80 Batch   50/215   train_loss = 3.206\n",
      "Epoch  80 Batch   60/215   train_loss = 3.495\n",
      "Epoch  80 Batch   70/215   train_loss = 3.389\n",
      "Epoch  80 Batch   80/215   train_loss = 3.201\n",
      "Epoch  80 Batch   90/215   train_loss = 3.366\n",
      "Epoch  80 Batch  100/215   train_loss = 3.173\n",
      "Epoch  80 Batch  110/215   train_loss = 3.161\n",
      "Epoch  80 Batch  120/215   train_loss = 3.361\n",
      "Epoch  80 Batch  130/215   train_loss = 3.516\n",
      "Epoch  80 Batch  140/215   train_loss = 3.340\n",
      "Epoch  80 Batch  150/215   train_loss = 3.476\n",
      "Epoch  80 Batch  160/215   train_loss = 3.372\n",
      "Epoch  80 Batch  170/215   train_loss = 3.202\n",
      "Epoch  80 Batch  180/215   train_loss = 3.284\n",
      "Epoch  80 Batch  190/215   train_loss = 3.257\n",
      "Epoch  80 Batch  200/215   train_loss = 3.291\n",
      "Epoch  80 Batch  210/215   train_loss = 3.116\n",
      "Epoch  81 Batch    5/215   train_loss = 3.517\n",
      "Epoch  81 Batch   15/215   train_loss = 3.348\n",
      "Epoch  81 Batch   25/215   train_loss = 3.223\n",
      "Epoch  81 Batch   35/215   train_loss = 3.236\n",
      "Epoch  81 Batch   45/215   train_loss = 3.107\n",
      "Epoch  81 Batch   55/215   train_loss = 3.201\n",
      "Epoch  81 Batch   65/215   train_loss = 3.190\n",
      "Epoch  81 Batch   75/215   train_loss = 3.363\n",
      "Epoch  81 Batch   85/215   train_loss = 3.296\n",
      "Epoch  81 Batch   95/215   train_loss = 3.188\n",
      "Epoch  81 Batch  105/215   train_loss = 3.097\n",
      "Epoch  81 Batch  115/215   train_loss = 3.120\n",
      "Epoch  81 Batch  125/215   train_loss = 3.072\n",
      "Epoch  81 Batch  135/215   train_loss = 3.243\n",
      "Epoch  81 Batch  145/215   train_loss = 3.225\n",
      "Epoch  81 Batch  155/215   train_loss = 3.349\n",
      "Epoch  81 Batch  165/215   train_loss = 3.391\n",
      "Epoch  81 Batch  175/215   train_loss = 3.300\n",
      "Epoch  81 Batch  185/215   train_loss = 3.363\n",
      "Epoch  81 Batch  195/215   train_loss = 3.367\n",
      "Epoch  81 Batch  205/215   train_loss = 3.289\n",
      "Epoch  82 Batch    0/215   train_loss = 3.234\n",
      "Epoch  82 Batch   10/215   train_loss = 3.393\n",
      "Epoch  82 Batch   20/215   train_loss = 3.220\n",
      "Epoch  82 Batch   30/215   train_loss = 3.107\n",
      "Epoch  82 Batch   40/215   train_loss = 3.243\n",
      "Epoch  82 Batch   50/215   train_loss = 3.174\n",
      "Epoch  82 Batch   60/215   train_loss = 3.452\n",
      "Epoch  82 Batch   70/215   train_loss = 3.319\n",
      "Epoch  82 Batch   80/215   train_loss = 3.275\n",
      "Epoch  82 Batch   90/215   train_loss = 3.385\n",
      "Epoch  82 Batch  100/215   train_loss = 3.404\n",
      "Epoch  82 Batch  110/215   train_loss = 3.296\n",
      "Epoch  82 Batch  120/215   train_loss = 3.281\n",
      "Epoch  82 Batch  130/215   train_loss = 3.410\n",
      "Epoch  82 Batch  140/215   train_loss = 3.438\n",
      "Epoch  82 Batch  150/215   train_loss = 3.389\n",
      "Epoch  82 Batch  160/215   train_loss = 3.458\n",
      "Epoch  82 Batch  170/215   train_loss = 3.261\n",
      "Epoch  82 Batch  180/215   train_loss = 3.476\n",
      "Epoch  82 Batch  190/215   train_loss = 3.179\n",
      "Epoch  82 Batch  200/215   train_loss = 3.150\n",
      "Epoch  82 Batch  210/215   train_loss = 3.139\n",
      "Epoch  83 Batch    5/215   train_loss = 3.402\n",
      "Epoch  83 Batch   15/215   train_loss = 3.303\n",
      "Epoch  83 Batch   25/215   train_loss = 3.257\n",
      "Epoch  83 Batch   35/215   train_loss = 3.474\n",
      "Epoch  83 Batch   45/215   train_loss = 3.110\n",
      "Epoch  83 Batch   55/215   train_loss = 3.266\n",
      "Epoch  83 Batch   65/215   train_loss = 3.312\n",
      "Epoch  83 Batch   75/215   train_loss = 3.282\n",
      "Epoch  83 Batch   85/215   train_loss = 3.258\n",
      "Epoch  83 Batch   95/215   train_loss = 3.035\n",
      "Epoch  83 Batch  105/215   train_loss = 3.115\n",
      "Epoch  83 Batch  115/215   train_loss = 3.133\n",
      "Epoch  83 Batch  125/215   train_loss = 3.023\n",
      "Epoch  83 Batch  135/215   train_loss = 3.281\n",
      "Epoch  83 Batch  145/215   train_loss = 3.309\n",
      "Epoch  83 Batch  155/215   train_loss = 3.359\n",
      "Epoch  83 Batch  165/215   train_loss = 3.358\n",
      "Epoch  83 Batch  175/215   train_loss = 3.412\n",
      "Epoch  83 Batch  185/215   train_loss = 3.280\n",
      "Epoch  83 Batch  195/215   train_loss = 3.314\n",
      "Epoch  83 Batch  205/215   train_loss = 3.198\n",
      "Epoch  84 Batch    0/215   train_loss = 3.309\n",
      "Epoch  84 Batch   10/215   train_loss = 3.251\n",
      "Epoch  84 Batch   20/215   train_loss = 3.282\n",
      "Epoch  84 Batch   30/215   train_loss = 3.375\n",
      "Epoch  84 Batch   40/215   train_loss = 3.182\n",
      "Epoch  84 Batch   50/215   train_loss = 3.187\n",
      "Epoch  84 Batch   60/215   train_loss = 3.302\n",
      "Epoch  84 Batch   70/215   train_loss = 3.167\n",
      "Epoch  84 Batch   80/215   train_loss = 3.197\n",
      "Epoch  84 Batch   90/215   train_loss = 3.323\n",
      "Epoch  84 Batch  100/215   train_loss = 3.358\n",
      "Epoch  84 Batch  110/215   train_loss = 3.326\n",
      "Epoch  84 Batch  120/215   train_loss = 3.411\n",
      "Epoch  84 Batch  130/215   train_loss = 3.465\n",
      "Epoch  84 Batch  140/215   train_loss = 3.412\n",
      "Epoch  84 Batch  150/215   train_loss = 3.217\n",
      "Epoch  84 Batch  160/215   train_loss = 3.510\n",
      "Epoch  84 Batch  170/215   train_loss = 3.249\n",
      "Epoch  84 Batch  180/215   train_loss = 3.143\n",
      "Epoch  84 Batch  190/215   train_loss = 3.411\n",
      "Epoch  84 Batch  200/215   train_loss = 3.190\n",
      "Epoch  84 Batch  210/215   train_loss = 3.150\n",
      "Epoch  85 Batch    5/215   train_loss = 3.301\n",
      "Epoch  85 Batch   15/215   train_loss = 3.150\n",
      "Epoch  85 Batch   25/215   train_loss = 3.127\n",
      "Epoch  85 Batch   35/215   train_loss = 3.192\n",
      "Epoch  85 Batch   45/215   train_loss = 3.254\n",
      "Epoch  85 Batch   55/215   train_loss = 3.195\n",
      "Epoch  85 Batch   65/215   train_loss = 3.326\n",
      "Epoch  85 Batch   75/215   train_loss = 3.258\n",
      "Epoch  85 Batch   85/215   train_loss = 3.092\n",
      "Epoch  85 Batch   95/215   train_loss = 3.151\n",
      "Epoch  85 Batch  105/215   train_loss = 3.017\n",
      "Epoch  85 Batch  115/215   train_loss = 3.225\n",
      "Epoch  85 Batch  125/215   train_loss = 3.041\n",
      "Epoch  85 Batch  135/215   train_loss = 3.134\n",
      "Epoch  85 Batch  145/215   train_loss = 3.283\n",
      "Epoch  85 Batch  155/215   train_loss = 3.412\n",
      "Epoch  85 Batch  165/215   train_loss = 3.211\n",
      "Epoch  85 Batch  175/215   train_loss = 3.431\n",
      "Epoch  85 Batch  185/215   train_loss = 3.255\n",
      "Epoch  85 Batch  195/215   train_loss = 3.183\n",
      "Epoch  85 Batch  205/215   train_loss = 3.273\n",
      "Epoch  86 Batch    0/215   train_loss = 3.233\n",
      "Epoch  86 Batch   10/215   train_loss = 3.459\n",
      "Epoch  86 Batch   20/215   train_loss = 3.340\n",
      "Epoch  86 Batch   30/215   train_loss = 3.174\n",
      "Epoch  86 Batch   40/215   train_loss = 3.343\n",
      "Epoch  86 Batch   50/215   train_loss = 3.187\n",
      "Epoch  86 Batch   60/215   train_loss = 3.329\n",
      "Epoch  86 Batch   70/215   train_loss = 3.179\n",
      "Epoch  86 Batch   80/215   train_loss = 3.214\n",
      "Epoch  86 Batch   90/215   train_loss = 3.237\n",
      "Epoch  86 Batch  100/215   train_loss = 3.415\n",
      "Epoch  86 Batch  110/215   train_loss = 3.235\n",
      "Epoch  86 Batch  120/215   train_loss = 3.213\n",
      "Epoch  86 Batch  130/215   train_loss = 3.389\n",
      "Epoch  86 Batch  140/215   train_loss = 3.276\n",
      "Epoch  86 Batch  150/215   train_loss = 3.354\n",
      "Epoch  86 Batch  160/215   train_loss = 3.314\n",
      "Epoch  86 Batch  170/215   train_loss = 3.068\n",
      "Epoch  86 Batch  180/215   train_loss = 3.237\n",
      "Epoch  86 Batch  190/215   train_loss = 3.395\n",
      "Epoch  86 Batch  200/215   train_loss = 3.006\n",
      "Epoch  86 Batch  210/215   train_loss = 3.037\n",
      "Epoch  87 Batch    5/215   train_loss = 3.339\n",
      "Epoch  87 Batch   15/215   train_loss = 3.201\n",
      "Epoch  87 Batch   25/215   train_loss = 3.112\n",
      "Epoch  87 Batch   35/215   train_loss = 3.232\n",
      "Epoch  87 Batch   45/215   train_loss = 3.108\n",
      "Epoch  87 Batch   55/215   train_loss = 3.280\n",
      "Epoch  87 Batch   65/215   train_loss = 3.296\n",
      "Epoch  87 Batch   75/215   train_loss = 3.235\n",
      "Epoch  87 Batch   85/215   train_loss = 3.198\n",
      "Epoch  87 Batch   95/215   train_loss = 3.328\n",
      "Epoch  87 Batch  105/215   train_loss = 3.161\n",
      "Epoch  87 Batch  115/215   train_loss = 3.185\n",
      "Epoch  87 Batch  125/215   train_loss = 3.078\n",
      "Epoch  87 Batch  135/215   train_loss = 3.334\n",
      "Epoch  87 Batch  145/215   train_loss = 3.012\n",
      "Epoch  87 Batch  155/215   train_loss = 3.230\n",
      "Epoch  87 Batch  165/215   train_loss = 3.308\n",
      "Epoch  87 Batch  175/215   train_loss = 3.363\n",
      "Epoch  87 Batch  185/215   train_loss = 3.239\n",
      "Epoch  87 Batch  195/215   train_loss = 3.198\n",
      "Epoch  87 Batch  205/215   train_loss = 3.249\n",
      "Epoch  88 Batch    0/215   train_loss = 3.242\n",
      "Epoch  88 Batch   10/215   train_loss = 3.397\n",
      "Epoch  88 Batch   20/215   train_loss = 3.164\n",
      "Epoch  88 Batch   30/215   train_loss = 3.232\n",
      "Epoch  88 Batch   40/215   train_loss = 3.145\n",
      "Epoch  88 Batch   50/215   train_loss = 3.026\n",
      "Epoch  88 Batch   60/215   train_loss = 3.407\n",
      "Epoch  88 Batch   70/215   train_loss = 3.218\n",
      "Epoch  88 Batch   80/215   train_loss = 3.280\n",
      "Epoch  88 Batch   90/215   train_loss = 3.240\n",
      "Epoch  88 Batch  100/215   train_loss = 3.314\n",
      "Epoch  88 Batch  110/215   train_loss = 3.151\n",
      "Epoch  88 Batch  120/215   train_loss = 3.298\n",
      "Epoch  88 Batch  130/215   train_loss = 3.563\n",
      "Epoch  88 Batch  140/215   train_loss = 3.486\n",
      "Epoch  88 Batch  150/215   train_loss = 3.212\n",
      "Epoch  88 Batch  160/215   train_loss = 3.219\n",
      "Epoch  88 Batch  170/215   train_loss = 3.197\n",
      "Epoch  88 Batch  180/215   train_loss = 3.179\n",
      "Epoch  88 Batch  190/215   train_loss = 3.192\n",
      "Epoch  88 Batch  200/215   train_loss = 3.193\n",
      "Epoch  88 Batch  210/215   train_loss = 3.088\n",
      "Epoch  89 Batch    5/215   train_loss = 3.261\n",
      "Epoch  89 Batch   15/215   train_loss = 3.153\n",
      "Epoch  89 Batch   25/215   train_loss = 3.182\n",
      "Epoch  89 Batch   35/215   train_loss = 3.360\n",
      "Epoch  89 Batch   45/215   train_loss = 3.174\n",
      "Epoch  89 Batch   55/215   train_loss = 3.171\n",
      "Epoch  89 Batch   65/215   train_loss = 3.318\n",
      "Epoch  89 Batch   75/215   train_loss = 3.227\n",
      "Epoch  89 Batch   85/215   train_loss = 3.122\n",
      "Epoch  89 Batch   95/215   train_loss = 3.306\n",
      "Epoch  89 Batch  105/215   train_loss = 3.143\n",
      "Epoch  89 Batch  115/215   train_loss = 3.080\n",
      "Epoch  89 Batch  125/215   train_loss = 2.961\n",
      "Epoch  89 Batch  135/215   train_loss = 3.167\n",
      "Epoch  89 Batch  145/215   train_loss = 3.185\n",
      "Epoch  89 Batch  155/215   train_loss = 3.257\n",
      "Epoch  89 Batch  165/215   train_loss = 3.319\n",
      "Epoch  89 Batch  175/215   train_loss = 3.530\n",
      "Epoch  89 Batch  185/215   train_loss = 3.236\n",
      "Epoch  89 Batch  195/215   train_loss = 3.261\n",
      "Epoch  89 Batch  205/215   train_loss = 3.336\n",
      "Epoch  90 Batch    0/215   train_loss = 3.244\n",
      "Epoch  90 Batch   10/215   train_loss = 3.401\n",
      "Epoch  90 Batch   20/215   train_loss = 3.217\n",
      "Epoch  90 Batch   30/215   train_loss = 3.125\n",
      "Epoch  90 Batch   40/215   train_loss = 3.129\n",
      "Epoch  90 Batch   50/215   train_loss = 3.063\n",
      "Epoch  90 Batch   60/215   train_loss = 3.327\n",
      "Epoch  90 Batch   70/215   train_loss = 3.123\n",
      "Epoch  90 Batch   80/215   train_loss = 3.166\n",
      "Epoch  90 Batch   90/215   train_loss = 3.294\n",
      "Epoch  90 Batch  100/215   train_loss = 3.264\n",
      "Epoch  90 Batch  110/215   train_loss = 3.181\n",
      "Epoch  90 Batch  120/215   train_loss = 3.161\n",
      "Epoch  90 Batch  130/215   train_loss = 3.459\n",
      "Epoch  90 Batch  140/215   train_loss = 3.396\n",
      "Epoch  90 Batch  150/215   train_loss = 3.276\n",
      "Epoch  90 Batch  160/215   train_loss = 3.403\n",
      "Epoch  90 Batch  170/215   train_loss = 3.168\n",
      "Epoch  90 Batch  180/215   train_loss = 3.295\n",
      "Epoch  90 Batch  190/215   train_loss = 3.216\n",
      "Epoch  90 Batch  200/215   train_loss = 3.289\n",
      "Epoch  90 Batch  210/215   train_loss = 3.127\n",
      "Epoch  91 Batch    5/215   train_loss = 3.275\n",
      "Epoch  91 Batch   15/215   train_loss = 3.092\n",
      "Epoch  91 Batch   25/215   train_loss = 3.128\n",
      "Epoch  91 Batch   35/215   train_loss = 3.172\n",
      "Epoch  91 Batch   45/215   train_loss = 3.099\n",
      "Epoch  91 Batch   55/215   train_loss = 3.328\n",
      "Epoch  91 Batch   65/215   train_loss = 3.229\n",
      "Epoch  91 Batch   75/215   train_loss = 3.321\n",
      "Epoch  91 Batch   85/215   train_loss = 3.311\n",
      "Epoch  91 Batch   95/215   train_loss = 3.125\n",
      "Epoch  91 Batch  105/215   train_loss = 3.174\n",
      "Epoch  91 Batch  115/215   train_loss = 3.024\n",
      "Epoch  91 Batch  125/215   train_loss = 2.915\n",
      "Epoch  91 Batch  135/215   train_loss = 3.030\n",
      "Epoch  91 Batch  145/215   train_loss = 3.196\n",
      "Epoch  91 Batch  155/215   train_loss = 3.446\n",
      "Epoch  91 Batch  165/215   train_loss = 3.130\n",
      "Epoch  91 Batch  175/215   train_loss = 3.627\n",
      "Epoch  91 Batch  185/215   train_loss = 3.231\n",
      "Epoch  91 Batch  195/215   train_loss = 3.025\n",
      "Epoch  91 Batch  205/215   train_loss = 3.257\n",
      "Epoch  92 Batch    0/215   train_loss = 3.386\n",
      "Epoch  92 Batch   10/215   train_loss = 3.387\n",
      "Epoch  92 Batch   20/215   train_loss = 3.021\n",
      "Epoch  92 Batch   30/215   train_loss = 2.999\n",
      "Epoch  92 Batch   40/215   train_loss = 3.155\n",
      "Epoch  92 Batch   50/215   train_loss = 3.047\n",
      "Epoch  92 Batch   60/215   train_loss = 3.354\n",
      "Epoch  92 Batch   70/215   train_loss = 3.115\n",
      "Epoch  92 Batch   80/215   train_loss = 3.143\n",
      "Epoch  92 Batch   90/215   train_loss = 3.301\n",
      "Epoch  92 Batch  100/215   train_loss = 3.423\n",
      "Epoch  92 Batch  110/215   train_loss = 3.175\n",
      "Epoch  92 Batch  120/215   train_loss = 3.382\n",
      "Epoch  92 Batch  130/215   train_loss = 3.387\n",
      "Epoch  92 Batch  140/215   train_loss = 3.219\n",
      "Epoch  92 Batch  150/215   train_loss = 3.212\n",
      "Epoch  92 Batch  160/215   train_loss = 3.293\n",
      "Epoch  92 Batch  170/215   train_loss = 3.201\n",
      "Epoch  92 Batch  180/215   train_loss = 3.392\n",
      "Epoch  92 Batch  190/215   train_loss = 3.280\n",
      "Epoch  92 Batch  200/215   train_loss = 3.006\n",
      "Epoch  92 Batch  210/215   train_loss = 3.024\n",
      "Epoch  93 Batch    5/215   train_loss = 3.258\n",
      "Epoch  93 Batch   15/215   train_loss = 3.198\n",
      "Epoch  93 Batch   25/215   train_loss = 3.164\n",
      "Epoch  93 Batch   35/215   train_loss = 3.334\n",
      "Epoch  93 Batch   45/215   train_loss = 3.253\n",
      "Epoch  93 Batch   55/215   train_loss = 3.154\n",
      "Epoch  93 Batch   65/215   train_loss = 3.313\n",
      "Epoch  93 Batch   75/215   train_loss = 3.054\n",
      "Epoch  93 Batch   85/215   train_loss = 3.041\n",
      "Epoch  93 Batch   95/215   train_loss = 3.166\n",
      "Epoch  93 Batch  105/215   train_loss = 3.284\n",
      "Epoch  93 Batch  115/215   train_loss = 3.101\n",
      "Epoch  93 Batch  125/215   train_loss = 3.026\n",
      "Epoch  93 Batch  135/215   train_loss = 3.098\n",
      "Epoch  93 Batch  145/215   train_loss = 3.251\n",
      "Epoch  93 Batch  155/215   train_loss = 3.286\n",
      "Epoch  93 Batch  165/215   train_loss = 3.157\n",
      "Epoch  93 Batch  175/215   train_loss = 3.442\n",
      "Epoch  93 Batch  185/215   train_loss = 3.267\n",
      "Epoch  93 Batch  195/215   train_loss = 3.131\n",
      "Epoch  93 Batch  205/215   train_loss = 3.235\n",
      "Epoch  94 Batch    0/215   train_loss = 3.158\n",
      "Epoch  94 Batch   10/215   train_loss = 3.393\n",
      "Epoch  94 Batch   20/215   train_loss = 3.149\n",
      "Epoch  94 Batch   30/215   train_loss = 3.111\n",
      "Epoch  94 Batch   40/215   train_loss = 3.050\n",
      "Epoch  94 Batch   50/215   train_loss = 3.244\n",
      "Epoch  94 Batch   60/215   train_loss = 3.252\n",
      "Epoch  94 Batch   70/215   train_loss = 3.197\n",
      "Epoch  94 Batch   80/215   train_loss = 3.058\n",
      "Epoch  94 Batch   90/215   train_loss = 3.272\n",
      "Epoch  94 Batch  100/215   train_loss = 3.457\n",
      "Epoch  94 Batch  110/215   train_loss = 3.174\n",
      "Epoch  94 Batch  120/215   train_loss = 3.328\n",
      "Epoch  94 Batch  130/215   train_loss = 3.306\n",
      "Epoch  94 Batch  140/215   train_loss = 3.255\n",
      "Epoch  94 Batch  150/215   train_loss = 3.253\n",
      "Epoch  94 Batch  160/215   train_loss = 3.312\n",
      "Epoch  94 Batch  170/215   train_loss = 3.158\n",
      "Epoch  94 Batch  180/215   train_loss = 3.105\n",
      "Epoch  94 Batch  190/215   train_loss = 3.278\n",
      "Epoch  94 Batch  200/215   train_loss = 3.136\n",
      "Epoch  94 Batch  210/215   train_loss = 2.923\n",
      "Epoch  95 Batch    5/215   train_loss = 3.224\n",
      "Epoch  95 Batch   15/215   train_loss = 3.222\n",
      "Epoch  95 Batch   25/215   train_loss = 3.198\n",
      "Epoch  95 Batch   35/215   train_loss = 3.272\n",
      "Epoch  95 Batch   45/215   train_loss = 3.099\n",
      "Epoch  95 Batch   55/215   train_loss = 3.173\n",
      "Epoch  95 Batch   65/215   train_loss = 3.371\n",
      "Epoch  95 Batch   75/215   train_loss = 3.209\n",
      "Epoch  95 Batch   85/215   train_loss = 3.164\n",
      "Epoch  95 Batch   95/215   train_loss = 3.265\n",
      "Epoch  95 Batch  105/215   train_loss = 3.268\n",
      "Epoch  95 Batch  115/215   train_loss = 3.025\n",
      "Epoch  95 Batch  125/215   train_loss = 2.980\n",
      "Epoch  95 Batch  135/215   train_loss = 3.075\n",
      "Epoch  95 Batch  145/215   train_loss = 3.110\n",
      "Epoch  95 Batch  155/215   train_loss = 3.240\n",
      "Epoch  95 Batch  165/215   train_loss = 3.216\n",
      "Epoch  95 Batch  175/215   train_loss = 3.420\n",
      "Epoch  95 Batch  185/215   train_loss = 3.274\n",
      "Epoch  95 Batch  195/215   train_loss = 3.208\n",
      "Epoch  95 Batch  205/215   train_loss = 3.109\n",
      "Epoch  96 Batch    0/215   train_loss = 3.109\n",
      "Epoch  96 Batch   10/215   train_loss = 3.218\n",
      "Epoch  96 Batch   20/215   train_loss = 3.167\n",
      "Epoch  96 Batch   30/215   train_loss = 3.071\n",
      "Epoch  96 Batch   40/215   train_loss = 3.242\n",
      "Epoch  96 Batch   50/215   train_loss = 3.267\n",
      "Epoch  96 Batch   60/215   train_loss = 3.312\n",
      "Epoch  96 Batch   70/215   train_loss = 3.155\n",
      "Epoch  96 Batch   80/215   train_loss = 3.225\n",
      "Epoch  96 Batch   90/215   train_loss = 3.240\n",
      "Epoch  96 Batch  100/215   train_loss = 3.224\n",
      "Epoch  96 Batch  110/215   train_loss = 3.245\n",
      "Epoch  96 Batch  120/215   train_loss = 3.282\n",
      "Epoch  96 Batch  130/215   train_loss = 3.326\n",
      "Epoch  96 Batch  140/215   train_loss = 3.190\n",
      "Epoch  96 Batch  150/215   train_loss = 3.063\n",
      "Epoch  96 Batch  160/215   train_loss = 3.363\n",
      "Epoch  96 Batch  170/215   train_loss = 3.147\n",
      "Epoch  96 Batch  180/215   train_loss = 3.186\n",
      "Epoch  96 Batch  190/215   train_loss = 3.252\n",
      "Epoch  96 Batch  200/215   train_loss = 3.151\n",
      "Epoch  96 Batch  210/215   train_loss = 2.913\n",
      "Epoch  97 Batch    5/215   train_loss = 3.239\n",
      "Epoch  97 Batch   15/215   train_loss = 3.021\n",
      "Epoch  97 Batch   25/215   train_loss = 3.213\n",
      "Epoch  97 Batch   35/215   train_loss = 3.357\n",
      "Epoch  97 Batch   45/215   train_loss = 3.128\n",
      "Epoch  97 Batch   55/215   train_loss = 3.087\n",
      "Epoch  97 Batch   65/215   train_loss = 3.202\n",
      "Epoch  97 Batch   75/215   train_loss = 3.140\n",
      "Epoch  97 Batch   85/215   train_loss = 3.122\n",
      "Epoch  97 Batch   95/215   train_loss = 3.294\n",
      "Epoch  97 Batch  105/215   train_loss = 3.154\n",
      "Epoch  97 Batch  115/215   train_loss = 3.073\n",
      "Epoch  97 Batch  125/215   train_loss = 3.008\n",
      "Epoch  97 Batch  135/215   train_loss = 3.079\n",
      "Epoch  97 Batch  145/215   train_loss = 3.252\n",
      "Epoch  97 Batch  155/215   train_loss = 3.094\n",
      "Epoch  97 Batch  165/215   train_loss = 3.082\n",
      "Epoch  97 Batch  175/215   train_loss = 3.368\n",
      "Epoch  97 Batch  185/215   train_loss = 3.326\n",
      "Epoch  97 Batch  195/215   train_loss = 3.173\n",
      "Epoch  97 Batch  205/215   train_loss = 3.245\n",
      "Epoch  98 Batch    0/215   train_loss = 3.252\n",
      "Epoch  98 Batch   10/215   train_loss = 3.394\n",
      "Epoch  98 Batch   20/215   train_loss = 3.340\n",
      "Epoch  98 Batch   30/215   train_loss = 3.010\n",
      "Epoch  98 Batch   40/215   train_loss = 3.071\n",
      "Epoch  98 Batch   50/215   train_loss = 3.195\n",
      "Epoch  98 Batch   60/215   train_loss = 3.187\n",
      "Epoch  98 Batch   70/215   train_loss = 3.096\n",
      "Epoch  98 Batch   80/215   train_loss = 3.160\n",
      "Epoch  98 Batch   90/215   train_loss = 3.331\n",
      "Epoch  98 Batch  100/215   train_loss = 3.283\n",
      "Epoch  98 Batch  110/215   train_loss = 3.200\n",
      "Epoch  98 Batch  120/215   train_loss = 3.245\n",
      "Epoch  98 Batch  130/215   train_loss = 3.368\n",
      "Epoch  98 Batch  140/215   train_loss = 3.075\n",
      "Epoch  98 Batch  150/215   train_loss = 3.241\n",
      "Epoch  98 Batch  160/215   train_loss = 3.288\n",
      "Epoch  98 Batch  170/215   train_loss = 3.179\n",
      "Epoch  98 Batch  180/215   train_loss = 3.218\n",
      "Epoch  98 Batch  190/215   train_loss = 3.252\n",
      "Epoch  98 Batch  200/215   train_loss = 3.105\n",
      "Epoch  98 Batch  210/215   train_loss = 3.133\n",
      "Epoch  99 Batch    5/215   train_loss = 3.294\n",
      "Epoch  99 Batch   15/215   train_loss = 3.032\n",
      "Epoch  99 Batch   25/215   train_loss = 3.275\n",
      "Epoch  99 Batch   35/215   train_loss = 3.313\n",
      "Epoch  99 Batch   45/215   train_loss = 3.163\n",
      "Epoch  99 Batch   55/215   train_loss = 3.041\n",
      "Epoch  99 Batch   65/215   train_loss = 3.281\n",
      "Epoch  99 Batch   75/215   train_loss = 3.069\n",
      "Epoch  99 Batch   85/215   train_loss = 3.068\n",
      "Epoch  99 Batch   95/215   train_loss = 3.025\n",
      "Epoch  99 Batch  105/215   train_loss = 3.059\n",
      "Epoch  99 Batch  115/215   train_loss = 3.020\n",
      "Epoch  99 Batch  125/215   train_loss = 2.945\n",
      "Epoch  99 Batch  135/215   train_loss = 2.934\n",
      "Epoch  99 Batch  145/215   train_loss = 3.198\n",
      "Epoch  99 Batch  155/215   train_loss = 3.120\n",
      "Epoch  99 Batch  165/215   train_loss = 3.214\n",
      "Epoch  99 Batch  175/215   train_loss = 3.407\n",
      "Epoch  99 Batch  185/215   train_loss = 3.296\n",
      "Epoch  99 Batch  195/215   train_loss = 2.975\n",
      "Epoch  99 Batch  205/215   train_loss = 3.257\n",
      "Epoch 100 Batch    0/215   train_loss = 3.208\n",
      "Epoch 100 Batch   10/215   train_loss = 3.204\n",
      "Epoch 100 Batch   20/215   train_loss = 3.206\n",
      "Epoch 100 Batch   30/215   train_loss = 3.149\n",
      "Epoch 100 Batch   40/215   train_loss = 3.079\n",
      "Epoch 100 Batch   50/215   train_loss = 3.040\n",
      "Epoch 100 Batch   60/215   train_loss = 3.193\n",
      "Epoch 100 Batch   70/215   train_loss = 3.038\n",
      "Epoch 100 Batch   80/215   train_loss = 3.174\n",
      "Epoch 100 Batch   90/215   train_loss = 3.271\n",
      "Epoch 100 Batch  100/215   train_loss = 3.123\n",
      "Epoch 100 Batch  110/215   train_loss = 3.100\n",
      "Epoch 100 Batch  120/215   train_loss = 3.078\n",
      "Epoch 100 Batch  130/215   train_loss = 3.412\n",
      "Epoch 100 Batch  140/215   train_loss = 3.271\n",
      "Epoch 100 Batch  150/215   train_loss = 3.076\n",
      "Epoch 100 Batch  160/215   train_loss = 3.255\n",
      "Epoch 100 Batch  170/215   train_loss = 3.130\n",
      "Epoch 100 Batch  180/215   train_loss = 3.217\n",
      "Epoch 100 Batch  190/215   train_loss = 3.302\n",
      "Epoch 100 Batch  200/215   train_loss = 3.126\n",
      "Epoch 100 Batch  210/215   train_loss = 3.062\n",
      "Epoch 101 Batch    5/215   train_loss = 3.272\n",
      "Epoch 101 Batch   15/215   train_loss = 3.210\n",
      "Epoch 101 Batch   25/215   train_loss = 3.097\n",
      "Epoch 101 Batch   35/215   train_loss = 3.288\n",
      "Epoch 101 Batch   45/215   train_loss = 3.249\n",
      "Epoch 101 Batch   55/215   train_loss = 3.082\n",
      "Epoch 101 Batch   65/215   train_loss = 3.135\n",
      "Epoch 101 Batch   75/215   train_loss = 3.169\n",
      "Epoch 101 Batch   85/215   train_loss = 3.060\n",
      "Epoch 101 Batch   95/215   train_loss = 3.141\n",
      "Epoch 101 Batch  105/215   train_loss = 3.104\n",
      "Epoch 101 Batch  115/215   train_loss = 3.063\n",
      "Epoch 101 Batch  125/215   train_loss = 2.943\n",
      "Epoch 101 Batch  135/215   train_loss = 3.069\n",
      "Epoch 101 Batch  145/215   train_loss = 2.892\n",
      "Epoch 101 Batch  155/215   train_loss = 3.260\n",
      "Epoch 101 Batch  165/215   train_loss = 3.055\n",
      "Epoch 101 Batch  175/215   train_loss = 3.364\n",
      "Epoch 101 Batch  185/215   train_loss = 3.331\n",
      "Epoch 101 Batch  195/215   train_loss = 3.132\n",
      "Epoch 101 Batch  205/215   train_loss = 3.090\n",
      "Epoch 102 Batch    0/215   train_loss = 3.276\n",
      "Epoch 102 Batch   10/215   train_loss = 3.346\n",
      "Epoch 102 Batch   20/215   train_loss = 3.054\n",
      "Epoch 102 Batch   30/215   train_loss = 3.041\n",
      "Epoch 102 Batch   40/215   train_loss = 3.287\n",
      "Epoch 102 Batch   50/215   train_loss = 3.115\n",
      "Epoch 102 Batch   60/215   train_loss = 3.260\n",
      "Epoch 102 Batch   70/215   train_loss = 3.167\n",
      "Epoch 102 Batch   80/215   train_loss = 3.212\n",
      "Epoch 102 Batch   90/215   train_loss = 3.272\n",
      "Epoch 102 Batch  100/215   train_loss = 3.359\n",
      "Epoch 102 Batch  110/215   train_loss = 2.978\n",
      "Epoch 102 Batch  120/215   train_loss = 3.227\n",
      "Epoch 102 Batch  130/215   train_loss = 3.257\n",
      "Epoch 102 Batch  140/215   train_loss = 3.128\n",
      "Epoch 102 Batch  150/215   train_loss = 3.091\n",
      "Epoch 102 Batch  160/215   train_loss = 3.314\n",
      "Epoch 102 Batch  170/215   train_loss = 3.193\n",
      "Epoch 102 Batch  180/215   train_loss = 3.232\n",
      "Epoch 102 Batch  190/215   train_loss = 3.149\n",
      "Epoch 102 Batch  200/215   train_loss = 2.971\n",
      "Epoch 102 Batch  210/215   train_loss = 3.120\n",
      "Epoch 103 Batch    5/215   train_loss = 3.076\n",
      "Epoch 103 Batch   15/215   train_loss = 3.018\n",
      "Epoch 103 Batch   25/215   train_loss = 3.016\n",
      "Epoch 103 Batch   35/215   train_loss = 3.219\n",
      "Epoch 103 Batch   45/215   train_loss = 3.145\n",
      "Epoch 103 Batch   55/215   train_loss = 3.095\n",
      "Epoch 103 Batch   65/215   train_loss = 3.200\n",
      "Epoch 103 Batch   75/215   train_loss = 3.226\n",
      "Epoch 103 Batch   85/215   train_loss = 3.035\n",
      "Epoch 103 Batch   95/215   train_loss = 3.266\n",
      "Epoch 103 Batch  105/215   train_loss = 3.226\n",
      "Epoch 103 Batch  115/215   train_loss = 3.034\n",
      "Epoch 103 Batch  125/215   train_loss = 2.981\n",
      "Epoch 103 Batch  135/215   train_loss = 3.057\n",
      "Epoch 103 Batch  145/215   train_loss = 3.148\n",
      "Epoch 103 Batch  155/215   train_loss = 3.190\n",
      "Epoch 103 Batch  165/215   train_loss = 3.309\n",
      "Epoch 103 Batch  175/215   train_loss = 3.417\n",
      "Epoch 103 Batch  185/215   train_loss = 3.368\n",
      "Epoch 103 Batch  195/215   train_loss = 3.136\n",
      "Epoch 103 Batch  205/215   train_loss = 3.164\n",
      "Epoch 104 Batch    0/215   train_loss = 3.105\n",
      "Epoch 104 Batch   10/215   train_loss = 3.428\n",
      "Epoch 104 Batch   20/215   train_loss = 3.217\n",
      "Epoch 104 Batch   30/215   train_loss = 3.124\n",
      "Epoch 104 Batch   40/215   train_loss = 3.149\n",
      "Epoch 104 Batch   50/215   train_loss = 3.038\n",
      "Epoch 104 Batch   60/215   train_loss = 3.223\n",
      "Epoch 104 Batch   70/215   train_loss = 2.948\n",
      "Epoch 104 Batch   80/215   train_loss = 3.004\n",
      "Epoch 104 Batch   90/215   train_loss = 3.249\n",
      "Epoch 104 Batch  100/215   train_loss = 3.086\n",
      "Epoch 104 Batch  110/215   train_loss = 2.999\n",
      "Epoch 104 Batch  120/215   train_loss = 3.237\n",
      "Epoch 104 Batch  130/215   train_loss = 3.438\n",
      "Epoch 104 Batch  140/215   train_loss = 3.003\n",
      "Epoch 104 Batch  150/215   train_loss = 3.018\n",
      "Epoch 104 Batch  160/215   train_loss = 3.309\n",
      "Epoch 104 Batch  170/215   train_loss = 2.996\n",
      "Epoch 104 Batch  180/215   train_loss = 3.104\n",
      "Epoch 104 Batch  190/215   train_loss = 3.255\n",
      "Epoch 104 Batch  200/215   train_loss = 3.112\n",
      "Epoch 104 Batch  210/215   train_loss = 2.933\n",
      "Epoch 105 Batch    5/215   train_loss = 3.209\n",
      "Epoch 105 Batch   15/215   train_loss = 3.191\n",
      "Epoch 105 Batch   25/215   train_loss = 3.227\n",
      "Epoch 105 Batch   35/215   train_loss = 3.243\n",
      "Epoch 105 Batch   45/215   train_loss = 3.081\n",
      "Epoch 105 Batch   55/215   train_loss = 3.087\n",
      "Epoch 105 Batch   65/215   train_loss = 3.131\n",
      "Epoch 105 Batch   75/215   train_loss = 3.027\n",
      "Epoch 105 Batch   85/215   train_loss = 3.059\n",
      "Epoch 105 Batch   95/215   train_loss = 3.082\n",
      "Epoch 105 Batch  105/215   train_loss = 3.005\n",
      "Epoch 105 Batch  115/215   train_loss = 3.089\n",
      "Epoch 105 Batch  125/215   train_loss = 2.874\n",
      "Epoch 105 Batch  135/215   train_loss = 3.193\n",
      "Epoch 105 Batch  145/215   train_loss = 3.328\n",
      "Epoch 105 Batch  155/215   train_loss = 3.141\n",
      "Epoch 105 Batch  165/215   train_loss = 3.118\n",
      "Epoch 105 Batch  175/215   train_loss = 3.504\n",
      "Epoch 105 Batch  185/215   train_loss = 3.141\n",
      "Epoch 105 Batch  195/215   train_loss = 3.146\n",
      "Epoch 105 Batch  205/215   train_loss = 3.277\n",
      "Epoch 106 Batch    0/215   train_loss = 3.220\n",
      "Epoch 106 Batch   10/215   train_loss = 3.308\n",
      "Epoch 106 Batch   20/215   train_loss = 3.159\n",
      "Epoch 106 Batch   30/215   train_loss = 3.066\n",
      "Epoch 106 Batch   40/215   train_loss = 3.171\n",
      "Epoch 106 Batch   50/215   train_loss = 3.292\n",
      "Epoch 106 Batch   60/215   train_loss = 3.248\n",
      "Epoch 106 Batch   70/215   train_loss = 2.984\n",
      "Epoch 106 Batch   80/215   train_loss = 3.142\n",
      "Epoch 106 Batch   90/215   train_loss = 3.087\n",
      "Epoch 106 Batch  100/215   train_loss = 3.170\n",
      "Epoch 106 Batch  110/215   train_loss = 3.132\n",
      "Epoch 106 Batch  120/215   train_loss = 3.153\n",
      "Epoch 106 Batch  130/215   train_loss = 3.386\n",
      "Epoch 106 Batch  140/215   train_loss = 3.125\n",
      "Epoch 106 Batch  150/215   train_loss = 3.046\n",
      "Epoch 106 Batch  160/215   train_loss = 3.263\n",
      "Epoch 106 Batch  170/215   train_loss = 3.128\n",
      "Epoch 106 Batch  180/215   train_loss = 3.129\n",
      "Epoch 106 Batch  190/215   train_loss = 3.299\n",
      "Epoch 106 Batch  200/215   train_loss = 2.962\n",
      "Epoch 106 Batch  210/215   train_loss = 3.001\n",
      "Epoch 107 Batch    5/215   train_loss = 3.097\n",
      "Epoch 107 Batch   15/215   train_loss = 3.032\n",
      "Epoch 107 Batch   25/215   train_loss = 3.178\n",
      "Epoch 107 Batch   35/215   train_loss = 3.218\n",
      "Epoch 107 Batch   45/215   train_loss = 3.115\n",
      "Epoch 107 Batch   55/215   train_loss = 3.066\n",
      "Epoch 107 Batch   65/215   train_loss = 3.283\n",
      "Epoch 107 Batch   75/215   train_loss = 2.963\n",
      "Epoch 107 Batch   85/215   train_loss = 3.096\n",
      "Epoch 107 Batch   95/215   train_loss = 3.050\n",
      "Epoch 107 Batch  105/215   train_loss = 2.958\n",
      "Epoch 107 Batch  115/215   train_loss = 3.023\n",
      "Epoch 107 Batch  125/215   train_loss = 3.095\n",
      "Epoch 107 Batch  135/215   train_loss = 3.183\n",
      "Epoch 107 Batch  145/215   train_loss = 3.083\n",
      "Epoch 107 Batch  155/215   train_loss = 3.150\n",
      "Epoch 107 Batch  165/215   train_loss = 2.934\n",
      "Epoch 107 Batch  175/215   train_loss = 3.406\n",
      "Epoch 107 Batch  185/215   train_loss = 3.174\n",
      "Epoch 107 Batch  195/215   train_loss = 2.915\n",
      "Epoch 107 Batch  205/215   train_loss = 3.084\n",
      "Epoch 108 Batch    0/215   train_loss = 3.248\n",
      "Epoch 108 Batch   10/215   train_loss = 3.114\n",
      "Epoch 108 Batch   20/215   train_loss = 3.219\n",
      "Epoch 108 Batch   30/215   train_loss = 3.020\n",
      "Epoch 108 Batch   40/215   train_loss = 3.158\n",
      "Epoch 108 Batch   50/215   train_loss = 3.021\n",
      "Epoch 108 Batch   60/215   train_loss = 3.293\n",
      "Epoch 108 Batch   70/215   train_loss = 3.220\n",
      "Epoch 108 Batch   80/215   train_loss = 3.115\n",
      "Epoch 108 Batch   90/215   train_loss = 3.112\n",
      "Epoch 108 Batch  100/215   train_loss = 3.305\n",
      "Epoch 108 Batch  110/215   train_loss = 3.134\n",
      "Epoch 108 Batch  120/215   train_loss = 2.987\n",
      "Epoch 108 Batch  130/215   train_loss = 3.374\n",
      "Epoch 108 Batch  140/215   train_loss = 3.148\n",
      "Epoch 108 Batch  150/215   train_loss = 3.077\n",
      "Epoch 108 Batch  160/215   train_loss = 3.230\n",
      "Epoch 108 Batch  170/215   train_loss = 3.025\n",
      "Epoch 108 Batch  180/215   train_loss = 3.027\n",
      "Epoch 108 Batch  190/215   train_loss = 3.182\n",
      "Epoch 108 Batch  200/215   train_loss = 3.051\n",
      "Epoch 108 Batch  210/215   train_loss = 2.984\n",
      "Epoch 109 Batch    5/215   train_loss = 3.298\n",
      "Epoch 109 Batch   15/215   train_loss = 3.209\n",
      "Epoch 109 Batch   25/215   train_loss = 3.094\n",
      "Epoch 109 Batch   35/215   train_loss = 3.210\n",
      "Epoch 109 Batch   45/215   train_loss = 3.080\n",
      "Epoch 109 Batch   55/215   train_loss = 3.165\n",
      "Epoch 109 Batch   65/215   train_loss = 3.034\n",
      "Epoch 109 Batch   75/215   train_loss = 3.063\n",
      "Epoch 109 Batch   85/215   train_loss = 2.968\n",
      "Epoch 109 Batch   95/215   train_loss = 3.080\n",
      "Epoch 109 Batch  105/215   train_loss = 2.996\n",
      "Epoch 109 Batch  115/215   train_loss = 3.083\n",
      "Epoch 109 Batch  125/215   train_loss = 2.943\n",
      "Epoch 109 Batch  135/215   train_loss = 3.003\n",
      "Epoch 109 Batch  145/215   train_loss = 3.032\n",
      "Epoch 109 Batch  155/215   train_loss = 3.069\n",
      "Epoch 109 Batch  165/215   train_loss = 2.968\n",
      "Epoch 109 Batch  175/215   train_loss = 3.369\n",
      "Epoch 109 Batch  185/215   train_loss = 3.098\n",
      "Epoch 109 Batch  195/215   train_loss = 3.013\n",
      "Epoch 109 Batch  205/215   train_loss = 3.048\n",
      "Epoch 110 Batch    0/215   train_loss = 3.394\n",
      "Epoch 110 Batch   10/215   train_loss = 3.307\n",
      "Epoch 110 Batch   20/215   train_loss = 3.215\n",
      "Epoch 110 Batch   30/215   train_loss = 3.059\n",
      "Epoch 110 Batch   40/215   train_loss = 3.074\n",
      "Epoch 110 Batch   50/215   train_loss = 2.914\n",
      "Epoch 110 Batch   60/215   train_loss = 3.230\n",
      "Epoch 110 Batch   70/215   train_loss = 2.969\n",
      "Epoch 110 Batch   80/215   train_loss = 2.997\n",
      "Epoch 110 Batch   90/215   train_loss = 3.234\n",
      "Epoch 110 Batch  100/215   train_loss = 3.052\n",
      "Epoch 110 Batch  110/215   train_loss = 3.017\n",
      "Epoch 110 Batch  120/215   train_loss = 3.300\n",
      "Epoch 110 Batch  130/215   train_loss = 3.326\n",
      "Epoch 110 Batch  140/215   train_loss = 3.306\n",
      "Epoch 110 Batch  150/215   train_loss = 3.301\n",
      "Epoch 110 Batch  160/215   train_loss = 3.149\n",
      "Epoch 110 Batch  170/215   train_loss = 3.167\n",
      "Epoch 110 Batch  180/215   train_loss = 3.160\n",
      "Epoch 110 Batch  190/215   train_loss = 3.168\n",
      "Epoch 110 Batch  200/215   train_loss = 2.969\n",
      "Epoch 110 Batch  210/215   train_loss = 2.930\n",
      "Epoch 111 Batch    5/215   train_loss = 3.222\n",
      "Epoch 111 Batch   15/215   train_loss = 2.955\n",
      "Epoch 111 Batch   25/215   train_loss = 2.999\n",
      "Epoch 111 Batch   35/215   train_loss = 3.153\n",
      "Epoch 111 Batch   45/215   train_loss = 3.221\n",
      "Epoch 111 Batch   55/215   train_loss = 3.044\n",
      "Epoch 111 Batch   65/215   train_loss = 2.990\n",
      "Epoch 111 Batch   75/215   train_loss = 3.058\n",
      "Epoch 111 Batch   85/215   train_loss = 3.082\n",
      "Epoch 111 Batch   95/215   train_loss = 3.060\n",
      "Epoch 111 Batch  105/215   train_loss = 2.979\n",
      "Epoch 111 Batch  115/215   train_loss = 2.958\n",
      "Epoch 111 Batch  125/215   train_loss = 2.962\n",
      "Epoch 111 Batch  135/215   train_loss = 3.300\n",
      "Epoch 111 Batch  145/215   train_loss = 3.087\n",
      "Epoch 111 Batch  155/215   train_loss = 3.120\n",
      "Epoch 111 Batch  165/215   train_loss = 3.106\n",
      "Epoch 111 Batch  175/215   train_loss = 3.313\n",
      "Epoch 111 Batch  185/215   train_loss = 3.192\n",
      "Epoch 111 Batch  195/215   train_loss = 3.135\n",
      "Epoch 111 Batch  205/215   train_loss = 3.270\n",
      "Epoch 112 Batch    0/215   train_loss = 3.130\n",
      "Epoch 112 Batch   10/215   train_loss = 3.198\n",
      "Epoch 112 Batch   20/215   train_loss = 3.115\n",
      "Epoch 112 Batch   30/215   train_loss = 3.092\n",
      "Epoch 112 Batch   40/215   train_loss = 3.303\n",
      "Epoch 112 Batch   50/215   train_loss = 2.982\n",
      "Epoch 112 Batch   60/215   train_loss = 3.055\n",
      "Epoch 112 Batch   70/215   train_loss = 3.077\n",
      "Epoch 112 Batch   80/215   train_loss = 2.970\n",
      "Epoch 112 Batch   90/215   train_loss = 3.258\n",
      "Epoch 112 Batch  100/215   train_loss = 3.314\n",
      "Epoch 112 Batch  110/215   train_loss = 3.082\n",
      "Epoch 112 Batch  120/215   train_loss = 3.182\n",
      "Epoch 112 Batch  130/215   train_loss = 3.360\n",
      "Epoch 112 Batch  140/215   train_loss = 3.129\n",
      "Epoch 112 Batch  150/215   train_loss = 3.079\n",
      "Epoch 112 Batch  160/215   train_loss = 2.982\n",
      "Epoch 112 Batch  170/215   train_loss = 3.132\n",
      "Epoch 112 Batch  180/215   train_loss = 3.167\n",
      "Epoch 112 Batch  190/215   train_loss = 3.226\n",
      "Epoch 112 Batch  200/215   train_loss = 2.899\n",
      "Epoch 112 Batch  210/215   train_loss = 3.050\n",
      "Epoch 113 Batch    5/215   train_loss = 3.181\n",
      "Epoch 113 Batch   15/215   train_loss = 3.131\n",
      "Epoch 113 Batch   25/215   train_loss = 2.998\n",
      "Epoch 113 Batch   35/215   train_loss = 3.155\n",
      "Epoch 113 Batch   45/215   train_loss = 3.213\n",
      "Epoch 113 Batch   55/215   train_loss = 3.029\n",
      "Epoch 113 Batch   65/215   train_loss = 3.187\n",
      "Epoch 113 Batch   75/215   train_loss = 3.085\n",
      "Epoch 113 Batch   85/215   train_loss = 3.052\n",
      "Epoch 113 Batch   95/215   train_loss = 3.223\n",
      "Epoch 113 Batch  105/215   train_loss = 2.923\n",
      "Epoch 113 Batch  115/215   train_loss = 2.991\n",
      "Epoch 113 Batch  125/215   train_loss = 2.849\n",
      "Epoch 113 Batch  135/215   train_loss = 3.275\n",
      "Epoch 113 Batch  145/215   train_loss = 3.051\n",
      "Epoch 113 Batch  155/215   train_loss = 3.128\n",
      "Epoch 113 Batch  165/215   train_loss = 3.101\n",
      "Epoch 113 Batch  175/215   train_loss = 3.278\n",
      "Epoch 113 Batch  185/215   train_loss = 3.098\n",
      "Epoch 113 Batch  195/215   train_loss = 3.151\n",
      "Epoch 113 Batch  205/215   train_loss = 3.117\n",
      "Epoch 114 Batch    0/215   train_loss = 3.335\n",
      "Epoch 114 Batch   10/215   train_loss = 3.430\n",
      "Epoch 114 Batch   20/215   train_loss = 3.034\n",
      "Epoch 114 Batch   30/215   train_loss = 3.105\n",
      "Epoch 114 Batch   40/215   train_loss = 3.108\n",
      "Epoch 114 Batch   50/215   train_loss = 3.039\n",
      "Epoch 114 Batch   60/215   train_loss = 3.152\n",
      "Epoch 114 Batch   70/215   train_loss = 3.182\n",
      "Epoch 114 Batch   80/215   train_loss = 3.092\n",
      "Epoch 114 Batch   90/215   train_loss = 3.168\n",
      "Epoch 114 Batch  100/215   train_loss = 3.182\n",
      "Epoch 114 Batch  110/215   train_loss = 3.067\n",
      "Epoch 114 Batch  120/215   train_loss = 3.169\n",
      "Epoch 114 Batch  130/215   train_loss = 3.367\n",
      "Epoch 114 Batch  140/215   train_loss = 3.206\n",
      "Epoch 114 Batch  150/215   train_loss = 3.008\n",
      "Epoch 114 Batch  160/215   train_loss = 3.113\n",
      "Epoch 114 Batch  170/215   train_loss = 3.058\n",
      "Epoch 114 Batch  180/215   train_loss = 3.194\n",
      "Epoch 114 Batch  190/215   train_loss = 3.275\n",
      "Epoch 114 Batch  200/215   train_loss = 3.014\n",
      "Epoch 114 Batch  210/215   train_loss = 2.878\n",
      "Epoch 115 Batch    5/215   train_loss = 3.066\n",
      "Epoch 115 Batch   15/215   train_loss = 2.995\n",
      "Epoch 115 Batch   25/215   train_loss = 2.988\n",
      "Epoch 115 Batch   35/215   train_loss = 3.148\n",
      "Epoch 115 Batch   45/215   train_loss = 2.953\n",
      "Epoch 115 Batch   55/215   train_loss = 3.128\n",
      "Epoch 115 Batch   65/215   train_loss = 3.285\n",
      "Epoch 115 Batch   75/215   train_loss = 2.978\n",
      "Epoch 115 Batch   85/215   train_loss = 3.029\n",
      "Epoch 115 Batch   95/215   train_loss = 3.126\n",
      "Epoch 115 Batch  105/215   train_loss = 2.881\n",
      "Epoch 115 Batch  115/215   train_loss = 2.970\n",
      "Epoch 115 Batch  125/215   train_loss = 3.114\n",
      "Epoch 115 Batch  135/215   train_loss = 3.102\n",
      "Epoch 115 Batch  145/215   train_loss = 2.932\n",
      "Epoch 115 Batch  155/215   train_loss = 3.108\n",
      "Epoch 115 Batch  165/215   train_loss = 2.985\n",
      "Epoch 115 Batch  175/215   train_loss = 3.191\n",
      "Epoch 115 Batch  185/215   train_loss = 3.159\n",
      "Epoch 115 Batch  195/215   train_loss = 3.078\n",
      "Epoch 115 Batch  205/215   train_loss = 3.140\n",
      "Epoch 116 Batch    0/215   train_loss = 3.145\n",
      "Epoch 116 Batch   10/215   train_loss = 3.354\n",
      "Epoch 116 Batch   20/215   train_loss = 3.011\n",
      "Epoch 116 Batch   30/215   train_loss = 2.974\n",
      "Epoch 116 Batch   40/215   train_loss = 3.191\n",
      "Epoch 116 Batch   50/215   train_loss = 3.080\n",
      "Epoch 116 Batch   60/215   train_loss = 3.200\n",
      "Epoch 116 Batch   70/215   train_loss = 2.935\n",
      "Epoch 116 Batch   80/215   train_loss = 3.128\n",
      "Epoch 116 Batch   90/215   train_loss = 3.049\n",
      "Epoch 116 Batch  100/215   train_loss = 3.062\n",
      "Epoch 116 Batch  110/215   train_loss = 3.130\n",
      "Epoch 116 Batch  120/215   train_loss = 3.245\n",
      "Epoch 116 Batch  130/215   train_loss = 3.292\n",
      "Epoch 116 Batch  140/215   train_loss = 3.249\n",
      "Epoch 116 Batch  150/215   train_loss = 3.096\n",
      "Epoch 116 Batch  160/215   train_loss = 3.189\n",
      "Epoch 116 Batch  170/215   train_loss = 3.020\n",
      "Epoch 116 Batch  180/215   train_loss = 3.197\n",
      "Epoch 116 Batch  190/215   train_loss = 3.145\n",
      "Epoch 116 Batch  200/215   train_loss = 2.914\n",
      "Epoch 116 Batch  210/215   train_loss = 2.852\n",
      "Epoch 117 Batch    5/215   train_loss = 3.195\n",
      "Epoch 117 Batch   15/215   train_loss = 3.059\n",
      "Epoch 117 Batch   25/215   train_loss = 3.041\n",
      "Epoch 117 Batch   35/215   train_loss = 3.216\n",
      "Epoch 117 Batch   45/215   train_loss = 2.959\n",
      "Epoch 117 Batch   55/215   train_loss = 3.173\n",
      "Epoch 117 Batch   65/215   train_loss = 3.123\n",
      "Epoch 117 Batch   75/215   train_loss = 2.979\n",
      "Epoch 117 Batch   85/215   train_loss = 3.206\n",
      "Epoch 117 Batch   95/215   train_loss = 2.979\n",
      "Epoch 117 Batch  105/215   train_loss = 2.934\n",
      "Epoch 117 Batch  115/215   train_loss = 2.998\n",
      "Epoch 117 Batch  125/215   train_loss = 2.825\n",
      "Epoch 117 Batch  135/215   train_loss = 3.101\n",
      "Epoch 117 Batch  145/215   train_loss = 3.164\n",
      "Epoch 117 Batch  155/215   train_loss = 3.254\n",
      "Epoch 117 Batch  165/215   train_loss = 3.186\n",
      "Epoch 117 Batch  175/215   train_loss = 3.303\n",
      "Epoch 117 Batch  185/215   train_loss = 3.110\n",
      "Epoch 117 Batch  195/215   train_loss = 3.050\n",
      "Epoch 117 Batch  205/215   train_loss = 3.007\n",
      "Epoch 118 Batch    0/215   train_loss = 3.196\n",
      "Epoch 118 Batch   10/215   train_loss = 3.220\n",
      "Epoch 118 Batch   20/215   train_loss = 3.169\n",
      "Epoch 118 Batch   30/215   train_loss = 3.041\n",
      "Epoch 118 Batch   40/215   train_loss = 3.047\n",
      "Epoch 118 Batch   50/215   train_loss = 2.900\n",
      "Epoch 118 Batch   60/215   train_loss = 3.150\n",
      "Epoch 118 Batch   70/215   train_loss = 2.976\n",
      "Epoch 118 Batch   80/215   train_loss = 3.254\n",
      "Epoch 118 Batch   90/215   train_loss = 3.123\n",
      "Epoch 118 Batch  100/215   train_loss = 3.226\n",
      "Epoch 118 Batch  110/215   train_loss = 3.058\n",
      "Epoch 118 Batch  120/215   train_loss = 3.311\n",
      "Epoch 118 Batch  130/215   train_loss = 3.190\n",
      "Epoch 118 Batch  140/215   train_loss = 3.202\n",
      "Epoch 118 Batch  150/215   train_loss = 3.131\n",
      "Epoch 118 Batch  160/215   train_loss = 3.288\n",
      "Epoch 118 Batch  170/215   train_loss = 3.000\n",
      "Epoch 118 Batch  180/215   train_loss = 3.161\n",
      "Epoch 118 Batch  190/215   train_loss = 3.140\n",
      "Epoch 118 Batch  200/215   train_loss = 2.986\n",
      "Epoch 118 Batch  210/215   train_loss = 2.826\n",
      "Epoch 119 Batch    5/215   train_loss = 3.306\n",
      "Epoch 119 Batch   15/215   train_loss = 3.027\n",
      "Epoch 119 Batch   25/215   train_loss = 3.006\n",
      "Epoch 119 Batch   35/215   train_loss = 3.132\n",
      "Epoch 119 Batch   45/215   train_loss = 3.012\n",
      "Epoch 119 Batch   55/215   train_loss = 2.979\n",
      "Epoch 119 Batch   65/215   train_loss = 3.082\n",
      "Epoch 119 Batch   75/215   train_loss = 3.035\n",
      "Epoch 119 Batch   85/215   train_loss = 3.083\n",
      "Epoch 119 Batch   95/215   train_loss = 2.979\n",
      "Epoch 119 Batch  105/215   train_loss = 2.900\n",
      "Epoch 119 Batch  115/215   train_loss = 2.929\n",
      "Epoch 119 Batch  125/215   train_loss = 2.920\n",
      "Epoch 119 Batch  135/215   train_loss = 3.000\n",
      "Epoch 119 Batch  145/215   train_loss = 3.108\n",
      "Epoch 119 Batch  155/215   train_loss = 3.172\n",
      "Epoch 119 Batch  165/215   train_loss = 2.932\n",
      "Epoch 119 Batch  175/215   train_loss = 3.328\n",
      "Epoch 119 Batch  185/215   train_loss = 3.181\n",
      "Epoch 119 Batch  195/215   train_loss = 3.010\n",
      "Epoch 119 Batch  205/215   train_loss = 3.063\n",
      "Epoch 120 Batch    0/215   train_loss = 3.262\n",
      "Epoch 120 Batch   10/215   train_loss = 3.129\n",
      "Epoch 120 Batch   20/215   train_loss = 3.157\n",
      "Epoch 120 Batch   30/215   train_loss = 3.046\n",
      "Epoch 120 Batch   40/215   train_loss = 3.059\n",
      "Epoch 120 Batch   50/215   train_loss = 2.993\n",
      "Epoch 120 Batch   60/215   train_loss = 3.214\n",
      "Epoch 120 Batch   70/215   train_loss = 3.019\n",
      "Epoch 120 Batch   80/215   train_loss = 3.069\n",
      "Epoch 120 Batch   90/215   train_loss = 3.083\n",
      "Epoch 120 Batch  100/215   train_loss = 3.147\n",
      "Epoch 120 Batch  110/215   train_loss = 2.955\n",
      "Epoch 120 Batch  120/215   train_loss = 2.998\n",
      "Epoch 120 Batch  130/215   train_loss = 3.177\n",
      "Epoch 120 Batch  140/215   train_loss = 2.997\n",
      "Epoch 120 Batch  150/215   train_loss = 2.962\n",
      "Epoch 120 Batch  160/215   train_loss = 3.135\n",
      "Epoch 120 Batch  170/215   train_loss = 3.045\n",
      "Epoch 120 Batch  180/215   train_loss = 3.061\n",
      "Epoch 120 Batch  190/215   train_loss = 2.954\n",
      "Epoch 120 Batch  200/215   train_loss = 3.068\n",
      "Epoch 120 Batch  210/215   train_loss = 2.885\n",
      "Epoch 121 Batch    5/215   train_loss = 3.062\n",
      "Epoch 121 Batch   15/215   train_loss = 3.109\n",
      "Epoch 121 Batch   25/215   train_loss = 2.978\n",
      "Epoch 121 Batch   35/215   train_loss = 3.336\n",
      "Epoch 121 Batch   45/215   train_loss = 2.953\n",
      "Epoch 121 Batch   55/215   train_loss = 2.811\n",
      "Epoch 121 Batch   65/215   train_loss = 3.230\n",
      "Epoch 121 Batch   75/215   train_loss = 2.995\n",
      "Epoch 121 Batch   85/215   train_loss = 3.117\n",
      "Epoch 121 Batch   95/215   train_loss = 3.008\n",
      "Epoch 121 Batch  105/215   train_loss = 2.988\n",
      "Epoch 121 Batch  115/215   train_loss = 2.816\n",
      "Epoch 121 Batch  125/215   train_loss = 2.878\n",
      "Epoch 121 Batch  135/215   train_loss = 3.096\n",
      "Epoch 121 Batch  145/215   train_loss = 2.976\n",
      "Epoch 121 Batch  155/215   train_loss = 3.024\n",
      "Epoch 121 Batch  165/215   train_loss = 3.050\n",
      "Epoch 121 Batch  175/215   train_loss = 3.380\n",
      "Epoch 121 Batch  185/215   train_loss = 3.087\n",
      "Epoch 121 Batch  195/215   train_loss = 2.952\n",
      "Epoch 121 Batch  205/215   train_loss = 3.063\n",
      "Epoch 122 Batch    0/215   train_loss = 3.133\n",
      "Epoch 122 Batch   10/215   train_loss = 3.560\n",
      "Epoch 122 Batch   20/215   train_loss = 3.100\n",
      "Epoch 122 Batch   30/215   train_loss = 3.038\n",
      "Epoch 122 Batch   40/215   train_loss = 3.175\n",
      "Epoch 122 Batch   50/215   train_loss = 2.978\n",
      "Epoch 122 Batch   60/215   train_loss = 3.148\n",
      "Epoch 122 Batch   70/215   train_loss = 3.023\n",
      "Epoch 122 Batch   80/215   train_loss = 2.966\n",
      "Epoch 122 Batch   90/215   train_loss = 3.215\n",
      "Epoch 122 Batch  100/215   train_loss = 3.037\n",
      "Epoch 122 Batch  110/215   train_loss = 3.083\n",
      "Epoch 122 Batch  120/215   train_loss = 3.113\n",
      "Epoch 122 Batch  130/215   train_loss = 3.280\n",
      "Epoch 122 Batch  140/215   train_loss = 3.073\n",
      "Epoch 122 Batch  150/215   train_loss = 3.012\n",
      "Epoch 122 Batch  160/215   train_loss = 3.207\n",
      "Epoch 122 Batch  170/215   train_loss = 3.150\n",
      "Epoch 122 Batch  180/215   train_loss = 3.058\n",
      "Epoch 122 Batch  190/215   train_loss = 3.095\n",
      "Epoch 122 Batch  200/215   train_loss = 2.886\n",
      "Epoch 122 Batch  210/215   train_loss = 2.994\n",
      "Epoch 123 Batch    5/215   train_loss = 3.115\n",
      "Epoch 123 Batch   15/215   train_loss = 3.005\n",
      "Epoch 123 Batch   25/215   train_loss = 3.020\n",
      "Epoch 123 Batch   35/215   train_loss = 3.177\n",
      "Epoch 123 Batch   45/215   train_loss = 2.990\n",
      "Epoch 123 Batch   55/215   train_loss = 2.845\n",
      "Epoch 123 Batch   65/215   train_loss = 3.284\n",
      "Epoch 123 Batch   75/215   train_loss = 3.213\n",
      "Epoch 123 Batch   85/215   train_loss = 2.940\n",
      "Epoch 123 Batch   95/215   train_loss = 3.056\n",
      "Epoch 123 Batch  105/215   train_loss = 2.940\n",
      "Epoch 123 Batch  115/215   train_loss = 2.802\n",
      "Epoch 123 Batch  125/215   train_loss = 3.030\n",
      "Epoch 123 Batch  135/215   train_loss = 3.070\n",
      "Epoch 123 Batch  145/215   train_loss = 2.960\n",
      "Epoch 123 Batch  155/215   train_loss = 3.263\n",
      "Epoch 123 Batch  165/215   train_loss = 3.052\n",
      "Epoch 123 Batch  175/215   train_loss = 3.382\n",
      "Epoch 123 Batch  185/215   train_loss = 3.061\n",
      "Epoch 123 Batch  195/215   train_loss = 2.885\n",
      "Epoch 123 Batch  205/215   train_loss = 3.042\n",
      "Epoch 124 Batch    0/215   train_loss = 3.165\n",
      "Epoch 124 Batch   10/215   train_loss = 3.100\n",
      "Epoch 124 Batch   20/215   train_loss = 3.020\n",
      "Epoch 124 Batch   30/215   train_loss = 3.038\n",
      "Epoch 124 Batch   40/215   train_loss = 3.158\n",
      "Epoch 124 Batch   50/215   train_loss = 2.948\n",
      "Epoch 124 Batch   60/215   train_loss = 3.024\n",
      "Epoch 124 Batch   70/215   train_loss = 2.964\n",
      "Epoch 124 Batch   80/215   train_loss = 3.237\n",
      "Epoch 124 Batch   90/215   train_loss = 3.053\n",
      "Epoch 124 Batch  100/215   train_loss = 3.157\n",
      "Epoch 124 Batch  110/215   train_loss = 2.980\n",
      "Epoch 124 Batch  120/215   train_loss = 3.102\n",
      "Epoch 124 Batch  130/215   train_loss = 3.365\n",
      "Epoch 124 Batch  140/215   train_loss = 3.066\n",
      "Epoch 124 Batch  150/215   train_loss = 3.078\n",
      "Epoch 124 Batch  160/215   train_loss = 3.109\n",
      "Epoch 124 Batch  170/215   train_loss = 2.844\n",
      "Epoch 124 Batch  180/215   train_loss = 3.026\n",
      "Epoch 124 Batch  190/215   train_loss = 3.262\n",
      "Epoch 124 Batch  200/215   train_loss = 2.948\n",
      "Epoch 124 Batch  210/215   train_loss = 2.725\n",
      "Epoch 125 Batch    5/215   train_loss = 3.134\n",
      "Epoch 125 Batch   15/215   train_loss = 3.093\n",
      "Epoch 125 Batch   25/215   train_loss = 2.989\n",
      "Epoch 125 Batch   35/215   train_loss = 3.022\n",
      "Epoch 125 Batch   45/215   train_loss = 3.130\n",
      "Epoch 125 Batch   55/215   train_loss = 2.989\n",
      "Epoch 125 Batch   65/215   train_loss = 3.106\n",
      "Epoch 125 Batch   75/215   train_loss = 3.112\n",
      "Epoch 125 Batch   85/215   train_loss = 2.927\n",
      "Epoch 125 Batch   95/215   train_loss = 3.031\n",
      "Epoch 125 Batch  105/215   train_loss = 2.941\n",
      "Epoch 125 Batch  115/215   train_loss = 2.837\n",
      "Epoch 125 Batch  125/215   train_loss = 2.865\n",
      "Epoch 125 Batch  135/215   train_loss = 2.932\n",
      "Epoch 125 Batch  145/215   train_loss = 3.046\n",
      "Epoch 125 Batch  155/215   train_loss = 3.094\n",
      "Epoch 125 Batch  165/215   train_loss = 3.007\n",
      "Epoch 125 Batch  175/215   train_loss = 3.161\n",
      "Epoch 125 Batch  185/215   train_loss = 3.239\n",
      "Epoch 125 Batch  195/215   train_loss = 2.835\n",
      "Epoch 125 Batch  205/215   train_loss = 3.194\n",
      "Epoch 126 Batch    0/215   train_loss = 3.124\n",
      "Epoch 126 Batch   10/215   train_loss = 3.178\n",
      "Epoch 126 Batch   20/215   train_loss = 3.144\n",
      "Epoch 126 Batch   30/215   train_loss = 3.132\n",
      "Epoch 126 Batch   40/215   train_loss = 2.999\n",
      "Epoch 126 Batch   50/215   train_loss = 3.135\n",
      "Epoch 126 Batch   60/215   train_loss = 3.242\n",
      "Epoch 126 Batch   70/215   train_loss = 3.050\n",
      "Epoch 126 Batch   80/215   train_loss = 3.025\n",
      "Epoch 126 Batch   90/215   train_loss = 3.258\n",
      "Epoch 126 Batch  100/215   train_loss = 3.187\n",
      "Epoch 126 Batch  110/215   train_loss = 3.101\n",
      "Epoch 126 Batch  120/215   train_loss = 3.014\n",
      "Epoch 126 Batch  130/215   train_loss = 3.161\n",
      "Epoch 126 Batch  140/215   train_loss = 3.048\n",
      "Epoch 126 Batch  150/215   train_loss = 3.024\n",
      "Epoch 126 Batch  160/215   train_loss = 3.001\n",
      "Epoch 126 Batch  170/215   train_loss = 3.163\n",
      "Epoch 126 Batch  180/215   train_loss = 3.129\n",
      "Epoch 126 Batch  190/215   train_loss = 3.090\n",
      "Epoch 126 Batch  200/215   train_loss = 3.097\n",
      "Epoch 126 Batch  210/215   train_loss = 2.928\n",
      "Epoch 127 Batch    5/215   train_loss = 3.164\n",
      "Epoch 127 Batch   15/215   train_loss = 3.098\n",
      "Epoch 127 Batch   25/215   train_loss = 3.000\n",
      "Epoch 127 Batch   35/215   train_loss = 2.998\n",
      "Epoch 127 Batch   45/215   train_loss = 3.129\n",
      "Epoch 127 Batch   55/215   train_loss = 2.964\n",
      "Epoch 127 Batch   65/215   train_loss = 3.149\n",
      "Epoch 127 Batch   75/215   train_loss = 2.918\n",
      "Epoch 127 Batch   85/215   train_loss = 2.972\n",
      "Epoch 127 Batch   95/215   train_loss = 2.995\n",
      "Epoch 127 Batch  105/215   train_loss = 2.941\n",
      "Epoch 127 Batch  115/215   train_loss = 2.793\n",
      "Epoch 127 Batch  125/215   train_loss = 2.931\n",
      "Epoch 127 Batch  135/215   train_loss = 2.884\n",
      "Epoch 127 Batch  145/215   train_loss = 3.000\n",
      "Epoch 127 Batch  155/215   train_loss = 3.259\n",
      "Epoch 127 Batch  165/215   train_loss = 3.155\n",
      "Epoch 127 Batch  175/215   train_loss = 3.245\n",
      "Epoch 127 Batch  185/215   train_loss = 3.200\n",
      "Epoch 127 Batch  195/215   train_loss = 3.020\n",
      "Epoch 127 Batch  205/215   train_loss = 3.029\n",
      "Epoch 128 Batch    0/215   train_loss = 3.144\n",
      "Epoch 128 Batch   10/215   train_loss = 3.171\n",
      "Epoch 128 Batch   20/215   train_loss = 3.152\n",
      "Epoch 128 Batch   30/215   train_loss = 2.756\n",
      "Epoch 128 Batch   40/215   train_loss = 3.039\n",
      "Epoch 128 Batch   50/215   train_loss = 3.005\n",
      "Epoch 128 Batch   60/215   train_loss = 3.192\n",
      "Epoch 128 Batch   70/215   train_loss = 3.014\n",
      "Epoch 128 Batch   80/215   train_loss = 2.947\n",
      "Epoch 128 Batch   90/215   train_loss = 3.092\n",
      "Epoch 128 Batch  100/215   train_loss = 3.265\n",
      "Epoch 128 Batch  110/215   train_loss = 3.078\n",
      "Epoch 128 Batch  120/215   train_loss = 3.063\n",
      "Epoch 128 Batch  130/215   train_loss = 3.114\n",
      "Epoch 128 Batch  140/215   train_loss = 3.013\n",
      "Epoch 128 Batch  150/215   train_loss = 3.035\n",
      "Epoch 128 Batch  160/215   train_loss = 3.057\n",
      "Epoch 128 Batch  170/215   train_loss = 2.970\n",
      "Epoch 128 Batch  180/215   train_loss = 3.059\n",
      "Epoch 128 Batch  190/215   train_loss = 3.070\n",
      "Epoch 128 Batch  200/215   train_loss = 2.842\n",
      "Epoch 128 Batch  210/215   train_loss = 3.105\n",
      "Epoch 129 Batch    5/215   train_loss = 3.224\n",
      "Epoch 129 Batch   15/215   train_loss = 2.952\n",
      "Epoch 129 Batch   25/215   train_loss = 3.089\n",
      "Epoch 129 Batch   35/215   train_loss = 3.233\n",
      "Epoch 129 Batch   45/215   train_loss = 3.043\n",
      "Epoch 129 Batch   55/215   train_loss = 2.977\n",
      "Epoch 129 Batch   65/215   train_loss = 3.020\n",
      "Epoch 129 Batch   75/215   train_loss = 2.995\n",
      "Epoch 129 Batch   85/215   train_loss = 3.015\n",
      "Epoch 129 Batch   95/215   train_loss = 3.029\n",
      "Epoch 129 Batch  105/215   train_loss = 3.164\n",
      "Epoch 129 Batch  115/215   train_loss = 2.979\n",
      "Epoch 129 Batch  125/215   train_loss = 2.928\n",
      "Epoch 129 Batch  135/215   train_loss = 2.888\n",
      "Epoch 129 Batch  145/215   train_loss = 3.047\n",
      "Epoch 129 Batch  155/215   train_loss = 3.226\n",
      "Epoch 129 Batch  165/215   train_loss = 2.998\n",
      "Epoch 129 Batch  175/215   train_loss = 3.308\n",
      "Epoch 129 Batch  185/215   train_loss = 3.276\n",
      "Epoch 129 Batch  195/215   train_loss = 2.882\n",
      "Epoch 129 Batch  205/215   train_loss = 3.091\n",
      "Epoch 130 Batch    0/215   train_loss = 3.212\n",
      "Epoch 130 Batch   10/215   train_loss = 3.061\n",
      "Epoch 130 Batch   20/215   train_loss = 3.052\n",
      "Epoch 130 Batch   30/215   train_loss = 2.966\n",
      "Epoch 130 Batch   40/215   train_loss = 2.965\n",
      "Epoch 130 Batch   50/215   train_loss = 2.972\n",
      "Epoch 130 Batch   60/215   train_loss = 3.176\n",
      "Epoch 130 Batch   70/215   train_loss = 2.955\n",
      "Epoch 130 Batch   80/215   train_loss = 3.106\n",
      "Epoch 130 Batch   90/215   train_loss = 3.137\n",
      "Epoch 130 Batch  100/215   train_loss = 3.013\n",
      "Epoch 130 Batch  110/215   train_loss = 3.050\n",
      "Epoch 130 Batch  120/215   train_loss = 2.939\n",
      "Epoch 130 Batch  130/215   train_loss = 3.183\n",
      "Epoch 130 Batch  140/215   train_loss = 3.045\n",
      "Epoch 130 Batch  150/215   train_loss = 3.096\n",
      "Epoch 130 Batch  160/215   train_loss = 3.177\n",
      "Epoch 130 Batch  170/215   train_loss = 2.914\n",
      "Epoch 130 Batch  180/215   train_loss = 3.004\n",
      "Epoch 130 Batch  190/215   train_loss = 3.024\n",
      "Epoch 130 Batch  200/215   train_loss = 2.879\n",
      "Epoch 130 Batch  210/215   train_loss = 2.911\n",
      "Epoch 131 Batch    5/215   train_loss = 3.163\n",
      "Epoch 131 Batch   15/215   train_loss = 2.909\n",
      "Epoch 131 Batch   25/215   train_loss = 2.836\n",
      "Epoch 131 Batch   35/215   train_loss = 3.103\n",
      "Epoch 131 Batch   45/215   train_loss = 2.963\n",
      "Epoch 131 Batch   55/215   train_loss = 2.887\n",
      "Epoch 131 Batch   65/215   train_loss = 2.912\n",
      "Epoch 131 Batch   75/215   train_loss = 2.977\n",
      "Epoch 131 Batch   85/215   train_loss = 2.977\n",
      "Epoch 131 Batch   95/215   train_loss = 3.125\n",
      "Epoch 131 Batch  105/215   train_loss = 2.924\n",
      "Epoch 131 Batch  115/215   train_loss = 3.004\n",
      "Epoch 131 Batch  125/215   train_loss = 2.938\n",
      "Epoch 131 Batch  135/215   train_loss = 2.895\n",
      "Epoch 131 Batch  145/215   train_loss = 3.084\n",
      "Epoch 131 Batch  155/215   train_loss = 3.119\n",
      "Epoch 131 Batch  165/215   train_loss = 2.924\n",
      "Epoch 131 Batch  175/215   train_loss = 3.067\n",
      "Epoch 131 Batch  185/215   train_loss = 3.085\n",
      "Epoch 131 Batch  195/215   train_loss = 2.850\n",
      "Epoch 131 Batch  205/215   train_loss = 2.995\n",
      "Epoch 132 Batch    0/215   train_loss = 3.104\n",
      "Epoch 132 Batch   10/215   train_loss = 3.188\n",
      "Epoch 132 Batch   20/215   train_loss = 2.991\n",
      "Epoch 132 Batch   30/215   train_loss = 3.005\n",
      "Epoch 132 Batch   40/215   train_loss = 3.003\n",
      "Epoch 132 Batch   50/215   train_loss = 3.017\n",
      "Epoch 132 Batch   60/215   train_loss = 3.252\n",
      "Epoch 132 Batch   70/215   train_loss = 2.970\n",
      "Epoch 132 Batch   80/215   train_loss = 2.968\n",
      "Epoch 132 Batch   90/215   train_loss = 3.256\n",
      "Epoch 132 Batch  100/215   train_loss = 2.997\n",
      "Epoch 132 Batch  110/215   train_loss = 2.926\n",
      "Epoch 132 Batch  120/215   train_loss = 2.939\n",
      "Epoch 132 Batch  130/215   train_loss = 3.200\n",
      "Epoch 132 Batch  140/215   train_loss = 2.995\n",
      "Epoch 132 Batch  150/215   train_loss = 3.043\n",
      "Epoch 132 Batch  160/215   train_loss = 2.948\n",
      "Epoch 132 Batch  170/215   train_loss = 3.100\n",
      "Epoch 132 Batch  180/215   train_loss = 3.246\n",
      "Epoch 132 Batch  190/215   train_loss = 3.165\n",
      "Epoch 132 Batch  200/215   train_loss = 2.871\n",
      "Epoch 132 Batch  210/215   train_loss = 2.986\n",
      "Epoch 133 Batch    5/215   train_loss = 3.225\n",
      "Epoch 133 Batch   15/215   train_loss = 2.839\n",
      "Epoch 133 Batch   25/215   train_loss = 3.086\n",
      "Epoch 133 Batch   35/215   train_loss = 3.049\n",
      "Epoch 133 Batch   45/215   train_loss = 2.939\n",
      "Epoch 133 Batch   55/215   train_loss = 3.025\n",
      "Epoch 133 Batch   65/215   train_loss = 2.893\n",
      "Epoch 133 Batch   75/215   train_loss = 3.030\n",
      "Epoch 133 Batch   85/215   train_loss = 3.068\n",
      "Epoch 133 Batch   95/215   train_loss = 2.943\n",
      "Epoch 133 Batch  105/215   train_loss = 3.127\n",
      "Epoch 133 Batch  115/215   train_loss = 3.056\n",
      "Epoch 133 Batch  125/215   train_loss = 2.986\n",
      "Epoch 133 Batch  135/215   train_loss = 2.976\n",
      "Epoch 133 Batch  145/215   train_loss = 2.951\n",
      "Epoch 133 Batch  155/215   train_loss = 3.043\n",
      "Epoch 133 Batch  165/215   train_loss = 3.003\n",
      "Epoch 133 Batch  175/215   train_loss = 3.197\n",
      "Epoch 133 Batch  185/215   train_loss = 3.130\n",
      "Epoch 133 Batch  195/215   train_loss = 3.089\n",
      "Epoch 133 Batch  205/215   train_loss = 3.094\n",
      "Epoch 134 Batch    0/215   train_loss = 3.228\n",
      "Epoch 134 Batch   10/215   train_loss = 3.259\n",
      "Epoch 134 Batch   20/215   train_loss = 3.169\n",
      "Epoch 134 Batch   30/215   train_loss = 2.885\n",
      "Epoch 134 Batch   40/215   train_loss = 2.905\n",
      "Epoch 134 Batch   50/215   train_loss = 2.945\n",
      "Epoch 134 Batch   60/215   train_loss = 3.070\n",
      "Epoch 134 Batch   70/215   train_loss = 2.966\n",
      "Epoch 134 Batch   80/215   train_loss = 2.986\n",
      "Epoch 134 Batch   90/215   train_loss = 3.240\n",
      "Epoch 134 Batch  100/215   train_loss = 2.949\n",
      "Epoch 134 Batch  110/215   train_loss = 3.018\n",
      "Epoch 134 Batch  120/215   train_loss = 2.977\n",
      "Epoch 134 Batch  130/215   train_loss = 3.385\n",
      "Epoch 134 Batch  140/215   train_loss = 3.133\n",
      "Epoch 134 Batch  150/215   train_loss = 2.809\n",
      "Epoch 134 Batch  160/215   train_loss = 2.963\n",
      "Epoch 134 Batch  170/215   train_loss = 2.946\n",
      "Epoch 134 Batch  180/215   train_loss = 3.028\n",
      "Epoch 134 Batch  190/215   train_loss = 3.067\n",
      "Epoch 134 Batch  200/215   train_loss = 2.905\n",
      "Epoch 134 Batch  210/215   train_loss = 2.827\n",
      "Epoch 135 Batch    5/215   train_loss = 3.073\n",
      "Epoch 135 Batch   15/215   train_loss = 2.886\n",
      "Epoch 135 Batch   25/215   train_loss = 3.055\n",
      "Epoch 135 Batch   35/215   train_loss = 3.070\n",
      "Epoch 135 Batch   45/215   train_loss = 2.999\n",
      "Epoch 135 Batch   55/215   train_loss = 3.059\n",
      "Epoch 135 Batch   65/215   train_loss = 3.008\n",
      "Epoch 135 Batch   75/215   train_loss = 3.085\n",
      "Epoch 135 Batch   85/215   train_loss = 2.996\n",
      "Epoch 135 Batch   95/215   train_loss = 2.922\n",
      "Epoch 135 Batch  105/215   train_loss = 2.985\n",
      "Epoch 135 Batch  115/215   train_loss = 2.861\n",
      "Epoch 135 Batch  125/215   train_loss = 2.894\n",
      "Epoch 135 Batch  135/215   train_loss = 2.915\n",
      "Epoch 135 Batch  145/215   train_loss = 2.975\n",
      "Epoch 135 Batch  155/215   train_loss = 3.090\n",
      "Epoch 135 Batch  165/215   train_loss = 2.931\n",
      "Epoch 135 Batch  175/215   train_loss = 3.240\n",
      "Epoch 135 Batch  185/215   train_loss = 3.180\n",
      "Epoch 135 Batch  195/215   train_loss = 3.021\n",
      "Epoch 135 Batch  205/215   train_loss = 3.072\n",
      "Epoch 136 Batch    0/215   train_loss = 3.052\n",
      "Epoch 136 Batch   10/215   train_loss = 3.264\n",
      "Epoch 136 Batch   20/215   train_loss = 3.013\n",
      "Epoch 136 Batch   30/215   train_loss = 2.986\n",
      "Epoch 136 Batch   40/215   train_loss = 2.999\n",
      "Epoch 136 Batch   50/215   train_loss = 3.013\n",
      "Epoch 136 Batch   60/215   train_loss = 3.130\n",
      "Epoch 136 Batch   70/215   train_loss = 2.925\n",
      "Epoch 136 Batch   80/215   train_loss = 3.028\n",
      "Epoch 136 Batch   90/215   train_loss = 3.126\n",
      "Epoch 136 Batch  100/215   train_loss = 3.031\n",
      "Epoch 136 Batch  110/215   train_loss = 2.913\n",
      "Epoch 136 Batch  120/215   train_loss = 2.952\n",
      "Epoch 136 Batch  130/215   train_loss = 3.155\n",
      "Epoch 136 Batch  140/215   train_loss = 3.155\n",
      "Epoch 136 Batch  150/215   train_loss = 3.045\n",
      "Epoch 136 Batch  160/215   train_loss = 3.056\n",
      "Epoch 136 Batch  170/215   train_loss = 2.958\n",
      "Epoch 136 Batch  180/215   train_loss = 3.160\n",
      "Epoch 136 Batch  190/215   train_loss = 3.119\n",
      "Epoch 136 Batch  200/215   train_loss = 3.047\n",
      "Epoch 136 Batch  210/215   train_loss = 2.953\n",
      "Epoch 137 Batch    5/215   train_loss = 3.095\n",
      "Epoch 137 Batch   15/215   train_loss = 3.019\n",
      "Epoch 137 Batch   25/215   train_loss = 3.049\n",
      "Epoch 137 Batch   35/215   train_loss = 2.994\n",
      "Epoch 137 Batch   45/215   train_loss = 2.968\n",
      "Epoch 137 Batch   55/215   train_loss = 2.926\n",
      "Epoch 137 Batch   65/215   train_loss = 2.960\n",
      "Epoch 137 Batch   75/215   train_loss = 3.006\n",
      "Epoch 137 Batch   85/215   train_loss = 2.941\n",
      "Epoch 137 Batch   95/215   train_loss = 3.110\n",
      "Epoch 137 Batch  105/215   train_loss = 2.908\n",
      "Epoch 137 Batch  115/215   train_loss = 2.933\n",
      "Epoch 137 Batch  125/215   train_loss = 2.941\n",
      "Epoch 137 Batch  135/215   train_loss = 2.929\n",
      "Epoch 137 Batch  145/215   train_loss = 2.882\n",
      "Epoch 137 Batch  155/215   train_loss = 3.121\n",
      "Epoch 137 Batch  165/215   train_loss = 2.998\n",
      "Epoch 137 Batch  175/215   train_loss = 3.201\n",
      "Epoch 137 Batch  185/215   train_loss = 2.981\n",
      "Epoch 137 Batch  195/215   train_loss = 2.783\n",
      "Epoch 137 Batch  205/215   train_loss = 3.089\n",
      "Epoch 138 Batch    0/215   train_loss = 3.177\n",
      "Epoch 138 Batch   10/215   train_loss = 3.091\n",
      "Epoch 138 Batch   20/215   train_loss = 3.174\n",
      "Epoch 138 Batch   30/215   train_loss = 2.889\n",
      "Epoch 138 Batch   40/215   train_loss = 2.858\n",
      "Epoch 138 Batch   50/215   train_loss = 3.055\n",
      "Epoch 138 Batch   60/215   train_loss = 3.094\n",
      "Epoch 138 Batch   70/215   train_loss = 2.928\n",
      "Epoch 138 Batch   80/215   train_loss = 2.998\n",
      "Epoch 138 Batch   90/215   train_loss = 3.010\n",
      "Epoch 138 Batch  100/215   train_loss = 3.132\n",
      "Epoch 138 Batch  110/215   train_loss = 3.007\n",
      "Epoch 138 Batch  120/215   train_loss = 3.122\n",
      "Epoch 138 Batch  130/215   train_loss = 3.214\n",
      "Epoch 138 Batch  140/215   train_loss = 2.997\n",
      "Epoch 138 Batch  150/215   train_loss = 2.943\n",
      "Epoch 138 Batch  160/215   train_loss = 3.189\n",
      "Epoch 138 Batch  170/215   train_loss = 2.892\n",
      "Epoch 138 Batch  180/215   train_loss = 3.142\n",
      "Epoch 138 Batch  190/215   train_loss = 3.019\n",
      "Epoch 138 Batch  200/215   train_loss = 2.974\n",
      "Epoch 138 Batch  210/215   train_loss = 2.885\n",
      "Epoch 139 Batch    5/215   train_loss = 3.133\n",
      "Epoch 139 Batch   15/215   train_loss = 3.005\n",
      "Epoch 139 Batch   25/215   train_loss = 3.004\n",
      "Epoch 139 Batch   35/215   train_loss = 2.979\n",
      "Epoch 139 Batch   45/215   train_loss = 2.937\n",
      "Epoch 139 Batch   55/215   train_loss = 2.904\n",
      "Epoch 139 Batch   65/215   train_loss = 3.014\n",
      "Epoch 139 Batch   75/215   train_loss = 3.004\n",
      "Epoch 139 Batch   85/215   train_loss = 3.041\n",
      "Epoch 139 Batch   95/215   train_loss = 3.055\n",
      "Epoch 139 Batch  105/215   train_loss = 2.888\n",
      "Epoch 139 Batch  115/215   train_loss = 3.027\n",
      "Epoch 139 Batch  125/215   train_loss = 2.915\n",
      "Epoch 139 Batch  135/215   train_loss = 3.104\n",
      "Epoch 139 Batch  145/215   train_loss = 3.138\n",
      "Epoch 139 Batch  155/215   train_loss = 3.116\n",
      "Epoch 139 Batch  165/215   train_loss = 2.948\n",
      "Epoch 139 Batch  175/215   train_loss = 3.190\n",
      "Epoch 139 Batch  185/215   train_loss = 3.145\n",
      "Epoch 139 Batch  195/215   train_loss = 2.960\n",
      "Epoch 139 Batch  205/215   train_loss = 3.083\n",
      "Epoch 140 Batch    0/215   train_loss = 3.034\n",
      "Epoch 140 Batch   10/215   train_loss = 3.120\n",
      "Epoch 140 Batch   20/215   train_loss = 2.997\n",
      "Epoch 140 Batch   30/215   train_loss = 2.868\n",
      "Epoch 140 Batch   40/215   train_loss = 3.042\n",
      "Epoch 140 Batch   50/215   train_loss = 2.997\n",
      "Epoch 140 Batch   60/215   train_loss = 3.025\n",
      "Epoch 140 Batch   70/215   train_loss = 2.972\n",
      "Epoch 140 Batch   80/215   train_loss = 3.048\n",
      "Epoch 140 Batch   90/215   train_loss = 3.043\n",
      "Epoch 140 Batch  100/215   train_loss = 3.002\n",
      "Epoch 140 Batch  110/215   train_loss = 2.969\n",
      "Epoch 140 Batch  120/215   train_loss = 3.124\n",
      "Epoch 140 Batch  130/215   train_loss = 3.239\n",
      "Epoch 140 Batch  140/215   train_loss = 3.005\n",
      "Epoch 140 Batch  150/215   train_loss = 2.836\n",
      "Epoch 140 Batch  160/215   train_loss = 2.899\n",
      "Epoch 140 Batch  170/215   train_loss = 2.962\n",
      "Epoch 140 Batch  180/215   train_loss = 3.015\n",
      "Epoch 140 Batch  190/215   train_loss = 3.199\n",
      "Epoch 140 Batch  200/215   train_loss = 2.874\n",
      "Epoch 140 Batch  210/215   train_loss = 2.841\n",
      "Epoch 141 Batch    5/215   train_loss = 3.170\n",
      "Epoch 141 Batch   15/215   train_loss = 2.961\n",
      "Epoch 141 Batch   25/215   train_loss = 2.973\n",
      "Epoch 141 Batch   35/215   train_loss = 3.055\n",
      "Epoch 141 Batch   45/215   train_loss = 3.062\n",
      "Epoch 141 Batch   55/215   train_loss = 3.015\n",
      "Epoch 141 Batch   65/215   train_loss = 3.183\n",
      "Epoch 141 Batch   75/215   train_loss = 2.980\n",
      "Epoch 141 Batch   85/215   train_loss = 2.922\n",
      "Epoch 141 Batch   95/215   train_loss = 2.946\n",
      "Epoch 141 Batch  105/215   train_loss = 2.988\n",
      "Epoch 141 Batch  115/215   train_loss = 3.007\n",
      "Epoch 141 Batch  125/215   train_loss = 2.919\n",
      "Epoch 141 Batch  135/215   train_loss = 3.041\n",
      "Epoch 141 Batch  145/215   train_loss = 3.061\n",
      "Epoch 141 Batch  155/215   train_loss = 3.061\n",
      "Epoch 141 Batch  165/215   train_loss = 2.909\n",
      "Epoch 141 Batch  175/215   train_loss = 3.053\n",
      "Epoch 141 Batch  185/215   train_loss = 2.884\n",
      "Epoch 141 Batch  195/215   train_loss = 3.042\n",
      "Epoch 141 Batch  205/215   train_loss = 2.869\n",
      "Epoch 142 Batch    0/215   train_loss = 3.103\n",
      "Epoch 142 Batch   10/215   train_loss = 3.127\n",
      "Epoch 142 Batch   20/215   train_loss = 2.944\n",
      "Epoch 142 Batch   30/215   train_loss = 2.945\n",
      "Epoch 142 Batch   40/215   train_loss = 2.914\n",
      "Epoch 142 Batch   50/215   train_loss = 3.024\n",
      "Epoch 142 Batch   60/215   train_loss = 3.045\n",
      "Epoch 142 Batch   70/215   train_loss = 2.893\n",
      "Epoch 142 Batch   80/215   train_loss = 2.821\n",
      "Epoch 142 Batch   90/215   train_loss = 2.962\n",
      "Epoch 142 Batch  100/215   train_loss = 3.063\n",
      "Epoch 142 Batch  110/215   train_loss = 2.921\n",
      "Epoch 142 Batch  120/215   train_loss = 3.220\n",
      "Epoch 142 Batch  130/215   train_loss = 3.386\n",
      "Epoch 142 Batch  140/215   train_loss = 3.161\n",
      "Epoch 142 Batch  150/215   train_loss = 3.133\n",
      "Epoch 142 Batch  160/215   train_loss = 2.876\n",
      "Epoch 142 Batch  170/215   train_loss = 2.993\n",
      "Epoch 142 Batch  180/215   train_loss = 3.036\n",
      "Epoch 142 Batch  190/215   train_loss = 3.207\n",
      "Epoch 142 Batch  200/215   train_loss = 2.888\n",
      "Epoch 142 Batch  210/215   train_loss = 2.832\n",
      "Epoch 143 Batch    5/215   train_loss = 3.023\n",
      "Epoch 143 Batch   15/215   train_loss = 3.020\n",
      "Epoch 143 Batch   25/215   train_loss = 3.018\n",
      "Epoch 143 Batch   35/215   train_loss = 3.031\n",
      "Epoch 143 Batch   45/215   train_loss = 3.020\n",
      "Epoch 143 Batch   55/215   train_loss = 2.998\n",
      "Epoch 143 Batch   65/215   train_loss = 2.886\n",
      "Epoch 143 Batch   75/215   train_loss = 2.988\n",
      "Epoch 143 Batch   85/215   train_loss = 2.893\n",
      "Epoch 143 Batch   95/215   train_loss = 2.815\n",
      "Epoch 143 Batch  105/215   train_loss = 2.977\n",
      "Epoch 143 Batch  115/215   train_loss = 2.884\n",
      "Epoch 143 Batch  125/215   train_loss = 2.943\n",
      "Epoch 143 Batch  135/215   train_loss = 2.905\n",
      "Epoch 143 Batch  145/215   train_loss = 3.009\n",
      "Epoch 143 Batch  155/215   train_loss = 3.084\n",
      "Epoch 143 Batch  165/215   train_loss = 3.095\n",
      "Epoch 143 Batch  175/215   train_loss = 3.238\n",
      "Epoch 143 Batch  185/215   train_loss = 2.985\n",
      "Epoch 143 Batch  195/215   train_loss = 2.752\n",
      "Epoch 143 Batch  205/215   train_loss = 2.917\n",
      "Epoch 144 Batch    0/215   train_loss = 3.021\n",
      "Epoch 144 Batch   10/215   train_loss = 3.094\n",
      "Epoch 144 Batch   20/215   train_loss = 2.955\n",
      "Epoch 144 Batch   30/215   train_loss = 2.912\n",
      "Epoch 144 Batch   40/215   train_loss = 3.095\n",
      "Epoch 144 Batch   50/215   train_loss = 2.961\n",
      "Epoch 144 Batch   60/215   train_loss = 3.217\n",
      "Epoch 144 Batch   70/215   train_loss = 2.996\n",
      "Epoch 144 Batch   80/215   train_loss = 3.036\n",
      "Epoch 144 Batch   90/215   train_loss = 3.023\n",
      "Epoch 144 Batch  100/215   train_loss = 3.200\n",
      "Epoch 144 Batch  110/215   train_loss = 2.952\n",
      "Epoch 144 Batch  120/215   train_loss = 2.901\n",
      "Epoch 144 Batch  130/215   train_loss = 3.166\n",
      "Epoch 144 Batch  140/215   train_loss = 3.233\n",
      "Epoch 144 Batch  150/215   train_loss = 2.922\n",
      "Epoch 144 Batch  160/215   train_loss = 3.224\n",
      "Epoch 144 Batch  170/215   train_loss = 2.951\n",
      "Epoch 144 Batch  180/215   train_loss = 2.892\n",
      "Epoch 144 Batch  190/215   train_loss = 3.161\n",
      "Epoch 144 Batch  200/215   train_loss = 2.911\n",
      "Epoch 144 Batch  210/215   train_loss = 2.905\n",
      "Epoch 145 Batch    5/215   train_loss = 3.148\n",
      "Epoch 145 Batch   15/215   train_loss = 2.972\n",
      "Epoch 145 Batch   25/215   train_loss = 3.064\n",
      "Epoch 145 Batch   35/215   train_loss = 3.089\n",
      "Epoch 145 Batch   45/215   train_loss = 2.936\n",
      "Epoch 145 Batch   55/215   train_loss = 2.858\n",
      "Epoch 145 Batch   65/215   train_loss = 3.190\n",
      "Epoch 145 Batch   75/215   train_loss = 2.895\n",
      "Epoch 145 Batch   85/215   train_loss = 2.984\n",
      "Epoch 145 Batch   95/215   train_loss = 2.939\n",
      "Epoch 145 Batch  105/215   train_loss = 3.058\n",
      "Epoch 145 Batch  115/215   train_loss = 2.783\n",
      "Epoch 145 Batch  125/215   train_loss = 2.913\n",
      "Epoch 145 Batch  135/215   train_loss = 3.072\n",
      "Epoch 145 Batch  145/215   train_loss = 3.095\n",
      "Epoch 145 Batch  155/215   train_loss = 3.096\n",
      "Epoch 145 Batch  165/215   train_loss = 3.057\n",
      "Epoch 145 Batch  175/215   train_loss = 3.183\n",
      "Epoch 145 Batch  185/215   train_loss = 3.019\n",
      "Epoch 145 Batch  195/215   train_loss = 2.760\n",
      "Epoch 145 Batch  205/215   train_loss = 3.008\n",
      "Epoch 146 Batch    0/215   train_loss = 3.125\n",
      "Epoch 146 Batch   10/215   train_loss = 3.120\n",
      "Epoch 146 Batch   20/215   train_loss = 2.987\n",
      "Epoch 146 Batch   30/215   train_loss = 2.834\n",
      "Epoch 146 Batch   40/215   train_loss = 2.723\n",
      "Epoch 146 Batch   50/215   train_loss = 3.016\n",
      "Epoch 146 Batch   60/215   train_loss = 3.161\n",
      "Epoch 146 Batch   70/215   train_loss = 2.811\n",
      "Epoch 146 Batch   80/215   train_loss = 2.952\n",
      "Epoch 146 Batch   90/215   train_loss = 2.972\n",
      "Epoch 146 Batch  100/215   train_loss = 3.024\n",
      "Epoch 146 Batch  110/215   train_loss = 2.871\n",
      "Epoch 146 Batch  120/215   train_loss = 3.166\n",
      "Epoch 146 Batch  130/215   train_loss = 3.172\n",
      "Epoch 146 Batch  140/215   train_loss = 2.982\n",
      "Epoch 146 Batch  150/215   train_loss = 2.856\n",
      "Epoch 146 Batch  160/215   train_loss = 2.968\n",
      "Epoch 146 Batch  170/215   train_loss = 2.980\n",
      "Epoch 146 Batch  180/215   train_loss = 3.029\n",
      "Epoch 146 Batch  190/215   train_loss = 3.047\n",
      "Epoch 146 Batch  200/215   train_loss = 2.962\n",
      "Epoch 146 Batch  210/215   train_loss = 2.836\n",
      "Epoch 147 Batch    5/215   train_loss = 3.053\n",
      "Epoch 147 Batch   15/215   train_loss = 2.881\n",
      "Epoch 147 Batch   25/215   train_loss = 2.908\n",
      "Epoch 147 Batch   35/215   train_loss = 3.069\n",
      "Epoch 147 Batch   45/215   train_loss = 2.881\n",
      "Epoch 147 Batch   55/215   train_loss = 2.977\n",
      "Epoch 147 Batch   65/215   train_loss = 3.185\n",
      "Epoch 147 Batch   75/215   train_loss = 2.969\n",
      "Epoch 147 Batch   85/215   train_loss = 2.850\n",
      "Epoch 147 Batch   95/215   train_loss = 2.968\n",
      "Epoch 147 Batch  105/215   train_loss = 2.986\n",
      "Epoch 147 Batch  115/215   train_loss = 2.785\n",
      "Epoch 147 Batch  125/215   train_loss = 2.897\n",
      "Epoch 147 Batch  135/215   train_loss = 2.899\n",
      "Epoch 147 Batch  145/215   train_loss = 3.043\n",
      "Epoch 147 Batch  155/215   train_loss = 3.002\n",
      "Epoch 147 Batch  165/215   train_loss = 3.029\n",
      "Epoch 147 Batch  175/215   train_loss = 3.200\n",
      "Epoch 147 Batch  185/215   train_loss = 3.004\n",
      "Epoch 147 Batch  195/215   train_loss = 2.904\n",
      "Epoch 147 Batch  205/215   train_loss = 3.049\n",
      "Epoch 148 Batch    0/215   train_loss = 3.014\n",
      "Epoch 148 Batch   10/215   train_loss = 2.958\n",
      "Epoch 148 Batch   20/215   train_loss = 3.042\n",
      "Epoch 148 Batch   30/215   train_loss = 2.858\n",
      "Epoch 148 Batch   40/215   train_loss = 3.047\n",
      "Epoch 148 Batch   50/215   train_loss = 2.794\n",
      "Epoch 148 Batch   60/215   train_loss = 3.117\n",
      "Epoch 148 Batch   70/215   train_loss = 2.851\n",
      "Epoch 148 Batch   80/215   train_loss = 2.951\n",
      "Epoch 148 Batch   90/215   train_loss = 3.079\n",
      "Epoch 148 Batch  100/215   train_loss = 2.965\n",
      "Epoch 148 Batch  110/215   train_loss = 2.885\n",
      "Epoch 148 Batch  120/215   train_loss = 3.054\n",
      "Epoch 148 Batch  130/215   train_loss = 3.104\n",
      "Epoch 148 Batch  140/215   train_loss = 3.228\n",
      "Epoch 148 Batch  150/215   train_loss = 2.890\n",
      "Epoch 148 Batch  160/215   train_loss = 2.977\n",
      "Epoch 148 Batch  170/215   train_loss = 2.975\n",
      "Epoch 148 Batch  180/215   train_loss = 2.967\n",
      "Epoch 148 Batch  190/215   train_loss = 2.987\n",
      "Epoch 148 Batch  200/215   train_loss = 2.939\n",
      "Epoch 148 Batch  210/215   train_loss = 2.752\n",
      "Epoch 149 Batch    5/215   train_loss = 3.187\n",
      "Epoch 149 Batch   15/215   train_loss = 3.083\n",
      "Epoch 149 Batch   25/215   train_loss = 2.952\n",
      "Epoch 149 Batch   35/215   train_loss = 3.144\n",
      "Epoch 149 Batch   45/215   train_loss = 2.941\n",
      "Epoch 149 Batch   55/215   train_loss = 2.992\n",
      "Epoch 149 Batch   65/215   train_loss = 3.205\n",
      "Epoch 149 Batch   75/215   train_loss = 3.087\n",
      "Epoch 149 Batch   85/215   train_loss = 2.935\n",
      "Epoch 149 Batch   95/215   train_loss = 2.809\n",
      "Epoch 149 Batch  105/215   train_loss = 2.753\n",
      "Epoch 149 Batch  115/215   train_loss = 2.992\n",
      "Epoch 149 Batch  125/215   train_loss = 2.791\n",
      "Epoch 149 Batch  135/215   train_loss = 3.010\n",
      "Epoch 149 Batch  145/215   train_loss = 3.062\n",
      "Epoch 149 Batch  155/215   train_loss = 3.110\n",
      "Epoch 149 Batch  165/215   train_loss = 2.975\n",
      "Epoch 149 Batch  175/215   train_loss = 3.060\n",
      "Epoch 149 Batch  185/215   train_loss = 3.109\n",
      "Epoch 149 Batch  195/215   train_loss = 2.839\n",
      "Epoch 149 Batch  205/215   train_loss = 2.885\n",
      "Epoch 150 Batch    0/215   train_loss = 2.927\n",
      "Epoch 150 Batch   10/215   train_loss = 3.074\n",
      "Epoch 150 Batch   20/215   train_loss = 2.971\n",
      "Epoch 150 Batch   30/215   train_loss = 2.945\n",
      "Epoch 150 Batch   40/215   train_loss = 2.919\n",
      "Epoch 150 Batch   50/215   train_loss = 2.942\n",
      "Epoch 150 Batch   60/215   train_loss = 3.027\n",
      "Epoch 150 Batch   70/215   train_loss = 2.944\n",
      "Epoch 150 Batch   80/215   train_loss = 2.996\n",
      "Epoch 150 Batch   90/215   train_loss = 3.076\n",
      "Epoch 150 Batch  100/215   train_loss = 2.883\n",
      "Epoch 150 Batch  110/215   train_loss = 2.865\n",
      "Epoch 150 Batch  120/215   train_loss = 3.023\n",
      "Epoch 150 Batch  130/215   train_loss = 3.112\n",
      "Epoch 150 Batch  140/215   train_loss = 3.171\n",
      "Epoch 150 Batch  150/215   train_loss = 2.928\n",
      "Epoch 150 Batch  160/215   train_loss = 2.986\n",
      "Epoch 150 Batch  170/215   train_loss = 2.883\n",
      "Epoch 150 Batch  180/215   train_loss = 2.995\n",
      "Epoch 150 Batch  190/215   train_loss = 3.031\n",
      "Epoch 150 Batch  200/215   train_loss = 2.950\n",
      "Epoch 150 Batch  210/215   train_loss = 2.916\n",
      "Epoch 151 Batch    5/215   train_loss = 2.944\n",
      "Epoch 151 Batch   15/215   train_loss = 2.918\n",
      "Epoch 151 Batch   25/215   train_loss = 2.930\n",
      "Epoch 151 Batch   35/215   train_loss = 3.126\n",
      "Epoch 151 Batch   45/215   train_loss = 3.259\n",
      "Epoch 151 Batch   55/215   train_loss = 2.834\n",
      "Epoch 151 Batch   65/215   train_loss = 2.997\n",
      "Epoch 151 Batch   75/215   train_loss = 2.990\n",
      "Epoch 151 Batch   85/215   train_loss = 3.097\n",
      "Epoch 151 Batch   95/215   train_loss = 2.865\n",
      "Epoch 151 Batch  105/215   train_loss = 2.887\n",
      "Epoch 151 Batch  115/215   train_loss = 2.799\n",
      "Epoch 151 Batch  125/215   train_loss = 2.904\n",
      "Epoch 151 Batch  135/215   train_loss = 3.004\n",
      "Epoch 151 Batch  145/215   train_loss = 2.831\n",
      "Epoch 151 Batch  155/215   train_loss = 3.177\n",
      "Epoch 151 Batch  165/215   train_loss = 2.975\n",
      "Epoch 151 Batch  175/215   train_loss = 3.304\n",
      "Epoch 151 Batch  185/215   train_loss = 2.884\n",
      "Epoch 151 Batch  195/215   train_loss = 2.766\n",
      "Epoch 151 Batch  205/215   train_loss = 2.930\n",
      "Epoch 152 Batch    0/215   train_loss = 2.955\n",
      "Epoch 152 Batch   10/215   train_loss = 2.984\n",
      "Epoch 152 Batch   20/215   train_loss = 2.956\n",
      "Epoch 152 Batch   30/215   train_loss = 2.806\n",
      "Epoch 152 Batch   40/215   train_loss = 2.996\n",
      "Epoch 152 Batch   50/215   train_loss = 2.698\n",
      "Epoch 152 Batch   60/215   train_loss = 2.986\n",
      "Epoch 152 Batch   70/215   train_loss = 2.806\n",
      "Epoch 152 Batch   80/215   train_loss = 2.996\n",
      "Epoch 152 Batch   90/215   train_loss = 3.036\n",
      "Epoch 152 Batch  100/215   train_loss = 2.972\n",
      "Epoch 152 Batch  110/215   train_loss = 2.906\n",
      "Epoch 152 Batch  120/215   train_loss = 2.978\n",
      "Epoch 152 Batch  130/215   train_loss = 3.075\n",
      "Epoch 152 Batch  140/215   train_loss = 3.130\n",
      "Epoch 152 Batch  150/215   train_loss = 2.875\n",
      "Epoch 152 Batch  160/215   train_loss = 3.142\n",
      "Epoch 152 Batch  170/215   train_loss = 2.989\n",
      "Epoch 152 Batch  180/215   train_loss = 2.983\n",
      "Epoch 152 Batch  190/215   train_loss = 3.079\n",
      "Epoch 152 Batch  200/215   train_loss = 2.929\n",
      "Epoch 152 Batch  210/215   train_loss = 2.915\n",
      "Epoch 153 Batch    5/215   train_loss = 3.102\n",
      "Epoch 153 Batch   15/215   train_loss = 2.795\n",
      "Epoch 153 Batch   25/215   train_loss = 3.093\n",
      "Epoch 153 Batch   35/215   train_loss = 3.225\n",
      "Epoch 153 Batch   45/215   train_loss = 2.867\n",
      "Epoch 153 Batch   55/215   train_loss = 2.845\n",
      "Epoch 153 Batch   65/215   train_loss = 3.019\n",
      "Epoch 153 Batch   75/215   train_loss = 2.959\n",
      "Epoch 153 Batch   85/215   train_loss = 2.817\n",
      "Epoch 153 Batch   95/215   train_loss = 2.869\n",
      "Epoch 153 Batch  105/215   train_loss = 2.824\n",
      "Epoch 153 Batch  115/215   train_loss = 2.749\n",
      "Epoch 153 Batch  125/215   train_loss = 3.042\n",
      "Epoch 153 Batch  135/215   train_loss = 3.025\n",
      "Epoch 153 Batch  145/215   train_loss = 3.052\n",
      "Epoch 153 Batch  155/215   train_loss = 3.128\n",
      "Epoch 153 Batch  165/215   train_loss = 2.752\n",
      "Epoch 153 Batch  175/215   train_loss = 3.344\n",
      "Epoch 153 Batch  185/215   train_loss = 3.047\n",
      "Epoch 153 Batch  195/215   train_loss = 2.699\n",
      "Epoch 153 Batch  205/215   train_loss = 2.841\n",
      "Epoch 154 Batch    0/215   train_loss = 3.040\n",
      "Epoch 154 Batch   10/215   train_loss = 2.950\n",
      "Epoch 154 Batch   20/215   train_loss = 2.930\n",
      "Epoch 154 Batch   30/215   train_loss = 2.733\n",
      "Epoch 154 Batch   40/215   train_loss = 2.878\n",
      "Epoch 154 Batch   50/215   train_loss = 2.897\n",
      "Epoch 154 Batch   60/215   train_loss = 3.089\n",
      "Epoch 154 Batch   70/215   train_loss = 2.839\n",
      "Epoch 154 Batch   80/215   train_loss = 2.903\n",
      "Epoch 154 Batch   90/215   train_loss = 3.070\n",
      "Epoch 154 Batch  100/215   train_loss = 3.040\n",
      "Epoch 154 Batch  110/215   train_loss = 2.963\n",
      "Epoch 154 Batch  120/215   train_loss = 3.045\n",
      "Epoch 154 Batch  130/215   train_loss = 3.123\n",
      "Epoch 154 Batch  140/215   train_loss = 2.905\n",
      "Epoch 154 Batch  150/215   train_loss = 2.956\n",
      "Epoch 154 Batch  160/215   train_loss = 3.201\n",
      "Epoch 154 Batch  170/215   train_loss = 2.924\n",
      "Epoch 154 Batch  180/215   train_loss = 2.838\n",
      "Epoch 154 Batch  190/215   train_loss = 3.127\n",
      "Epoch 154 Batch  200/215   train_loss = 2.880\n",
      "Epoch 154 Batch  210/215   train_loss = 2.795\n",
      "Epoch 155 Batch    5/215   train_loss = 2.989\n",
      "Epoch 155 Batch   15/215   train_loss = 2.866\n",
      "Epoch 155 Batch   25/215   train_loss = 2.942\n",
      "Epoch 155 Batch   35/215   train_loss = 3.209\n",
      "Epoch 155 Batch   45/215   train_loss = 2.896\n",
      "Epoch 155 Batch   55/215   train_loss = 3.107\n",
      "Epoch 155 Batch   65/215   train_loss = 3.248\n",
      "Epoch 155 Batch   75/215   train_loss = 2.914\n",
      "Epoch 155 Batch   85/215   train_loss = 2.886\n",
      "Epoch 155 Batch   95/215   train_loss = 2.954\n",
      "Epoch 155 Batch  105/215   train_loss = 2.806\n",
      "Epoch 155 Batch  115/215   train_loss = 2.690\n",
      "Epoch 155 Batch  125/215   train_loss = 2.925\n",
      "Epoch 155 Batch  135/215   train_loss = 2.888\n",
      "Epoch 155 Batch  145/215   train_loss = 3.019\n",
      "Epoch 155 Batch  155/215   train_loss = 3.125\n",
      "Epoch 155 Batch  165/215   train_loss = 2.884\n",
      "Epoch 155 Batch  175/215   train_loss = 3.093\n",
      "Epoch 155 Batch  185/215   train_loss = 2.824\n",
      "Epoch 155 Batch  195/215   train_loss = 2.796\n",
      "Epoch 155 Batch  205/215   train_loss = 2.945\n",
      "Epoch 156 Batch    0/215   train_loss = 3.062\n",
      "Epoch 156 Batch   10/215   train_loss = 2.915\n",
      "Epoch 156 Batch   20/215   train_loss = 2.997\n",
      "Epoch 156 Batch   30/215   train_loss = 2.968\n",
      "Epoch 156 Batch   40/215   train_loss = 3.007\n",
      "Epoch 156 Batch   50/215   train_loss = 2.757\n",
      "Epoch 156 Batch   60/215   train_loss = 3.083\n",
      "Epoch 156 Batch   70/215   train_loss = 2.839\n",
      "Epoch 156 Batch   80/215   train_loss = 2.992\n",
      "Epoch 156 Batch   90/215   train_loss = 3.156\n",
      "Epoch 156 Batch  100/215   train_loss = 2.967\n",
      "Epoch 156 Batch  110/215   train_loss = 2.805\n",
      "Epoch 156 Batch  120/215   train_loss = 2.984\n",
      "Epoch 156 Batch  130/215   train_loss = 3.155\n",
      "Epoch 156 Batch  140/215   train_loss = 3.122\n",
      "Epoch 156 Batch  150/215   train_loss = 2.896\n",
      "Epoch 156 Batch  160/215   train_loss = 2.798\n",
      "Epoch 156 Batch  170/215   train_loss = 2.843\n",
      "Epoch 156 Batch  180/215   train_loss = 3.016\n",
      "Epoch 156 Batch  190/215   train_loss = 3.090\n",
      "Epoch 156 Batch  200/215   train_loss = 2.816\n",
      "Epoch 156 Batch  210/215   train_loss = 2.729\n",
      "Epoch 157 Batch    5/215   train_loss = 3.014\n",
      "Epoch 157 Batch   15/215   train_loss = 2.927\n",
      "Epoch 157 Batch   25/215   train_loss = 2.927\n",
      "Epoch 157 Batch   35/215   train_loss = 2.946\n",
      "Epoch 157 Batch   45/215   train_loss = 2.903\n",
      "Epoch 157 Batch   55/215   train_loss = 2.854\n",
      "Epoch 157 Batch   65/215   train_loss = 3.200\n",
      "Epoch 157 Batch   75/215   train_loss = 2.855\n",
      "Epoch 157 Batch   85/215   train_loss = 2.920\n",
      "Epoch 157 Batch   95/215   train_loss = 3.023\n",
      "Epoch 157 Batch  105/215   train_loss = 2.746\n",
      "Epoch 157 Batch  115/215   train_loss = 2.842\n",
      "Epoch 157 Batch  125/215   train_loss = 2.706\n",
      "Epoch 157 Batch  135/215   train_loss = 2.887\n",
      "Epoch 157 Batch  145/215   train_loss = 2.877\n",
      "Epoch 157 Batch  155/215   train_loss = 2.953\n",
      "Epoch 157 Batch  165/215   train_loss = 3.108\n",
      "Epoch 157 Batch  175/215   train_loss = 3.017\n",
      "Epoch 157 Batch  185/215   train_loss = 3.028\n",
      "Epoch 157 Batch  195/215   train_loss = 2.878\n",
      "Epoch 157 Batch  205/215   train_loss = 2.908\n",
      "Epoch 158 Batch    0/215   train_loss = 2.934\n",
      "Epoch 158 Batch   10/215   train_loss = 3.075\n",
      "Epoch 158 Batch   20/215   train_loss = 2.966\n",
      "Epoch 158 Batch   30/215   train_loss = 2.888\n",
      "Epoch 158 Batch   40/215   train_loss = 2.805\n",
      "Epoch 158 Batch   50/215   train_loss = 2.992\n",
      "Epoch 158 Batch   60/215   train_loss = 3.173\n",
      "Epoch 158 Batch   70/215   train_loss = 2.766\n",
      "Epoch 158 Batch   80/215   train_loss = 3.026\n",
      "Epoch 158 Batch   90/215   train_loss = 3.149\n",
      "Epoch 158 Batch  100/215   train_loss = 2.883\n",
      "Epoch 158 Batch  110/215   train_loss = 2.885\n",
      "Epoch 158 Batch  120/215   train_loss = 3.020\n",
      "Epoch 158 Batch  130/215   train_loss = 3.101\n",
      "Epoch 158 Batch  140/215   train_loss = 3.095\n",
      "Epoch 158 Batch  150/215   train_loss = 3.005\n",
      "Epoch 158 Batch  160/215   train_loss = 2.845\n",
      "Epoch 158 Batch  170/215   train_loss = 2.850\n",
      "Epoch 158 Batch  180/215   train_loss = 3.158\n",
      "Epoch 158 Batch  190/215   train_loss = 2.944\n",
      "Epoch 158 Batch  200/215   train_loss = 2.859\n",
      "Epoch 158 Batch  210/215   train_loss = 2.832\n",
      "Epoch 159 Batch    5/215   train_loss = 2.993\n",
      "Epoch 159 Batch   15/215   train_loss = 2.957\n",
      "Epoch 159 Batch   25/215   train_loss = 2.729\n",
      "Epoch 159 Batch   35/215   train_loss = 3.041\n",
      "Epoch 159 Batch   45/215   train_loss = 2.997\n",
      "Epoch 159 Batch   55/215   train_loss = 2.939\n",
      "Epoch 159 Batch   65/215   train_loss = 2.799\n",
      "Epoch 159 Batch   75/215   train_loss = 2.943\n",
      "Epoch 159 Batch   85/215   train_loss = 2.700\n",
      "Epoch 159 Batch   95/215   train_loss = 2.926\n",
      "Epoch 159 Batch  105/215   train_loss = 2.837\n",
      "Epoch 159 Batch  115/215   train_loss = 2.933\n",
      "Epoch 159 Batch  125/215   train_loss = 2.741\n",
      "Epoch 159 Batch  135/215   train_loss = 2.882\n",
      "Epoch 159 Batch  145/215   train_loss = 2.791\n",
      "Epoch 159 Batch  155/215   train_loss = 3.081\n",
      "Epoch 159 Batch  165/215   train_loss = 2.909\n",
      "Epoch 159 Batch  175/215   train_loss = 3.125\n",
      "Epoch 159 Batch  185/215   train_loss = 3.040\n",
      "Epoch 159 Batch  195/215   train_loss = 2.800\n",
      "Epoch 159 Batch  205/215   train_loss = 2.897\n",
      "Epoch 160 Batch    0/215   train_loss = 3.228\n",
      "Epoch 160 Batch   10/215   train_loss = 3.097\n",
      "Epoch 160 Batch   20/215   train_loss = 2.968\n",
      "Epoch 160 Batch   30/215   train_loss = 2.774\n",
      "Epoch 160 Batch   40/215   train_loss = 2.917\n",
      "Epoch 160 Batch   50/215   train_loss = 2.835\n",
      "Epoch 160 Batch   60/215   train_loss = 3.095\n",
      "Epoch 160 Batch   70/215   train_loss = 2.925\n",
      "Epoch 160 Batch   80/215   train_loss = 2.980\n",
      "Epoch 160 Batch   90/215   train_loss = 2.955\n",
      "Epoch 160 Batch  100/215   train_loss = 3.016\n",
      "Epoch 160 Batch  110/215   train_loss = 2.857\n",
      "Epoch 160 Batch  120/215   train_loss = 2.988\n",
      "Epoch 160 Batch  130/215   train_loss = 3.171\n",
      "Epoch 160 Batch  140/215   train_loss = 3.005\n",
      "Epoch 160 Batch  150/215   train_loss = 3.069\n",
      "Epoch 160 Batch  160/215   train_loss = 3.094\n",
      "Epoch 160 Batch  170/215   train_loss = 2.912\n",
      "Epoch 160 Batch  180/215   train_loss = 2.933\n",
      "Epoch 160 Batch  190/215   train_loss = 3.043\n",
      "Epoch 160 Batch  200/215   train_loss = 2.876\n",
      "Epoch 160 Batch  210/215   train_loss = 2.760\n",
      "Epoch 161 Batch    5/215   train_loss = 3.022\n",
      "Epoch 161 Batch   15/215   train_loss = 2.848\n",
      "Epoch 161 Batch   25/215   train_loss = 2.910\n",
      "Epoch 161 Batch   35/215   train_loss = 3.282\n",
      "Epoch 161 Batch   45/215   train_loss = 3.133\n",
      "Epoch 161 Batch   55/215   train_loss = 2.814\n",
      "Epoch 161 Batch   65/215   train_loss = 3.044\n",
      "Epoch 161 Batch   75/215   train_loss = 2.894\n",
      "Epoch 161 Batch   85/215   train_loss = 2.905\n",
      "Epoch 161 Batch   95/215   train_loss = 2.818\n",
      "Epoch 161 Batch  105/215   train_loss = 2.792\n",
      "Epoch 161 Batch  115/215   train_loss = 2.816\n",
      "Epoch 161 Batch  125/215   train_loss = 2.899\n",
      "Epoch 161 Batch  135/215   train_loss = 2.928\n",
      "Epoch 161 Batch  145/215   train_loss = 2.974\n",
      "Epoch 161 Batch  155/215   train_loss = 3.067\n",
      "Epoch 161 Batch  165/215   train_loss = 3.079\n",
      "Epoch 161 Batch  175/215   train_loss = 3.177\n",
      "Epoch 161 Batch  185/215   train_loss = 3.013\n",
      "Epoch 161 Batch  195/215   train_loss = 3.003\n",
      "Epoch 161 Batch  205/215   train_loss = 2.883\n",
      "Epoch 162 Batch    0/215   train_loss = 2.947\n",
      "Epoch 162 Batch   10/215   train_loss = 3.284\n",
      "Epoch 162 Batch   20/215   train_loss = 2.843\n",
      "Epoch 162 Batch   30/215   train_loss = 2.965\n",
      "Epoch 162 Batch   40/215   train_loss = 2.867\n",
      "Epoch 162 Batch   50/215   train_loss = 2.786\n",
      "Epoch 162 Batch   60/215   train_loss = 2.945\n",
      "Epoch 162 Batch   70/215   train_loss = 2.697\n",
      "Epoch 162 Batch   80/215   train_loss = 2.823\n",
      "Epoch 162 Batch   90/215   train_loss = 2.928\n",
      "Epoch 162 Batch  100/215   train_loss = 2.978\n",
      "Epoch 162 Batch  110/215   train_loss = 3.080\n",
      "Epoch 162 Batch  120/215   train_loss = 3.068\n",
      "Epoch 162 Batch  130/215   train_loss = 3.143\n",
      "Epoch 162 Batch  140/215   train_loss = 2.983\n",
      "Epoch 162 Batch  150/215   train_loss = 2.975\n",
      "Epoch 162 Batch  160/215   train_loss = 2.922\n",
      "Epoch 162 Batch  170/215   train_loss = 3.022\n",
      "Epoch 162 Batch  180/215   train_loss = 3.126\n",
      "Epoch 162 Batch  190/215   train_loss = 2.936\n",
      "Epoch 162 Batch  200/215   train_loss = 3.052\n",
      "Epoch 162 Batch  210/215   train_loss = 2.878\n",
      "Epoch 163 Batch    5/215   train_loss = 2.906\n",
      "Epoch 163 Batch   15/215   train_loss = 2.981\n",
      "Epoch 163 Batch   25/215   train_loss = 2.923\n",
      "Epoch 163 Batch   35/215   train_loss = 3.030\n",
      "Epoch 163 Batch   45/215   train_loss = 2.860\n",
      "Epoch 163 Batch   55/215   train_loss = 3.020\n",
      "Epoch 163 Batch   65/215   train_loss = 3.017\n",
      "Epoch 163 Batch   75/215   train_loss = 2.929\n",
      "Epoch 163 Batch   85/215   train_loss = 2.779\n",
      "Epoch 163 Batch   95/215   train_loss = 2.926\n",
      "Epoch 163 Batch  105/215   train_loss = 2.950\n",
      "Epoch 163 Batch  115/215   train_loss = 3.004\n",
      "Epoch 163 Batch  125/215   train_loss = 2.836\n",
      "Epoch 163 Batch  135/215   train_loss = 2.947\n",
      "Epoch 163 Batch  145/215   train_loss = 2.892\n",
      "Epoch 163 Batch  155/215   train_loss = 3.007\n",
      "Epoch 163 Batch  165/215   train_loss = 2.947\n",
      "Epoch 163 Batch  175/215   train_loss = 3.211\n",
      "Epoch 163 Batch  185/215   train_loss = 3.070\n",
      "Epoch 163 Batch  195/215   train_loss = 2.806\n",
      "Epoch 163 Batch  205/215   train_loss = 2.939\n",
      "Epoch 164 Batch    0/215   train_loss = 3.030\n",
      "Epoch 164 Batch   10/215   train_loss = 2.959\n",
      "Epoch 164 Batch   20/215   train_loss = 2.862\n",
      "Epoch 164 Batch   30/215   train_loss = 2.852\n",
      "Epoch 164 Batch   40/215   train_loss = 2.853\n",
      "Epoch 164 Batch   50/215   train_loss = 2.890\n",
      "Epoch 164 Batch   60/215   train_loss = 2.963\n",
      "Epoch 164 Batch   70/215   train_loss = 2.752\n",
      "Epoch 164 Batch   80/215   train_loss = 2.964\n",
      "Epoch 164 Batch   90/215   train_loss = 3.068\n",
      "Epoch 164 Batch  100/215   train_loss = 3.078\n",
      "Epoch 164 Batch  110/215   train_loss = 3.025\n",
      "Epoch 164 Batch  120/215   train_loss = 2.931\n",
      "Epoch 164 Batch  130/215   train_loss = 3.058\n",
      "Epoch 164 Batch  140/215   train_loss = 2.987\n",
      "Epoch 164 Batch  150/215   train_loss = 2.932\n",
      "Epoch 164 Batch  160/215   train_loss = 2.932\n",
      "Epoch 164 Batch  170/215   train_loss = 2.944\n",
      "Epoch 164 Batch  180/215   train_loss = 2.733\n",
      "Epoch 164 Batch  190/215   train_loss = 3.000\n",
      "Epoch 164 Batch  200/215   train_loss = 2.916\n",
      "Epoch 164 Batch  210/215   train_loss = 2.855\n",
      "Epoch 165 Batch    5/215   train_loss = 2.935\n",
      "Epoch 165 Batch   15/215   train_loss = 2.954\n",
      "Epoch 165 Batch   25/215   train_loss = 2.820\n",
      "Epoch 165 Batch   35/215   train_loss = 3.019\n",
      "Epoch 165 Batch   45/215   train_loss = 2.991\n",
      "Epoch 165 Batch   55/215   train_loss = 2.878\n",
      "Epoch 165 Batch   65/215   train_loss = 3.113\n",
      "Epoch 165 Batch   75/215   train_loss = 2.859\n",
      "Epoch 165 Batch   85/215   train_loss = 2.752\n",
      "Epoch 165 Batch   95/215   train_loss = 2.995\n",
      "Epoch 165 Batch  105/215   train_loss = 2.827\n",
      "Epoch 165 Batch  115/215   train_loss = 2.684\n",
      "Epoch 165 Batch  125/215   train_loss = 2.849\n",
      "Epoch 165 Batch  135/215   train_loss = 2.934\n",
      "Epoch 165 Batch  145/215   train_loss = 2.912\n",
      "Epoch 165 Batch  155/215   train_loss = 3.138\n",
      "Epoch 165 Batch  165/215   train_loss = 2.950\n",
      "Epoch 165 Batch  175/215   train_loss = 3.104\n",
      "Epoch 165 Batch  185/215   train_loss = 2.989\n",
      "Epoch 165 Batch  195/215   train_loss = 2.847\n",
      "Epoch 165 Batch  205/215   train_loss = 2.830\n",
      "Epoch 166 Batch    0/215   train_loss = 3.003\n",
      "Epoch 166 Batch   10/215   train_loss = 3.023\n",
      "Epoch 166 Batch   20/215   train_loss = 2.922\n",
      "Epoch 166 Batch   30/215   train_loss = 2.920\n",
      "Epoch 166 Batch   40/215   train_loss = 2.885\n",
      "Epoch 166 Batch   50/215   train_loss = 2.980\n",
      "Epoch 166 Batch   60/215   train_loss = 2.892\n",
      "Epoch 166 Batch   70/215   train_loss = 2.723\n",
      "Epoch 166 Batch   80/215   train_loss = 2.921\n",
      "Epoch 166 Batch   90/215   train_loss = 3.079\n",
      "Epoch 166 Batch  100/215   train_loss = 3.035\n",
      "Epoch 166 Batch  110/215   train_loss = 3.024\n",
      "Epoch 166 Batch  120/215   train_loss = 3.096\n",
      "Epoch 166 Batch  130/215   train_loss = 3.082\n",
      "Epoch 166 Batch  140/215   train_loss = 2.930\n",
      "Epoch 166 Batch  150/215   train_loss = 2.978\n",
      "Epoch 166 Batch  160/215   train_loss = 2.965\n",
      "Epoch 166 Batch  170/215   train_loss = 3.001\n",
      "Epoch 166 Batch  180/215   train_loss = 2.943\n",
      "Epoch 166 Batch  190/215   train_loss = 2.885\n",
      "Epoch 166 Batch  200/215   train_loss = 2.757\n",
      "Epoch 166 Batch  210/215   train_loss = 2.741\n",
      "Epoch 167 Batch    5/215   train_loss = 2.969\n",
      "Epoch 167 Batch   15/215   train_loss = 2.821\n",
      "Epoch 167 Batch   25/215   train_loss = 2.833\n",
      "Epoch 167 Batch   35/215   train_loss = 3.025\n",
      "Epoch 167 Batch   45/215   train_loss = 2.842\n",
      "Epoch 167 Batch   55/215   train_loss = 2.885\n",
      "Epoch 167 Batch   65/215   train_loss = 2.958\n",
      "Epoch 167 Batch   75/215   train_loss = 2.829\n",
      "Epoch 167 Batch   85/215   train_loss = 2.986\n",
      "Epoch 167 Batch   95/215   train_loss = 2.829\n",
      "Epoch 167 Batch  105/215   train_loss = 2.734\n",
      "Epoch 167 Batch  115/215   train_loss = 2.619\n",
      "Epoch 167 Batch  125/215   train_loss = 2.676\n",
      "Epoch 167 Batch  135/215   train_loss = 3.081\n",
      "Epoch 167 Batch  145/215   train_loss = 2.926\n",
      "Epoch 167 Batch  155/215   train_loss = 3.034\n",
      "Epoch 167 Batch  165/215   train_loss = 2.779\n",
      "Epoch 167 Batch  175/215   train_loss = 3.126\n",
      "Epoch 167 Batch  185/215   train_loss = 3.169\n",
      "Epoch 167 Batch  195/215   train_loss = 2.737\n",
      "Epoch 167 Batch  205/215   train_loss = 2.922\n",
      "Epoch 168 Batch    0/215   train_loss = 2.936\n",
      "Epoch 168 Batch   10/215   train_loss = 3.019\n",
      "Epoch 168 Batch   20/215   train_loss = 2.921\n",
      "Epoch 168 Batch   30/215   train_loss = 2.800\n",
      "Epoch 168 Batch   40/215   train_loss = 2.999\n",
      "Epoch 168 Batch   50/215   train_loss = 2.913\n",
      "Epoch 168 Batch   60/215   train_loss = 2.918\n",
      "Epoch 168 Batch   70/215   train_loss = 3.116\n",
      "Epoch 168 Batch   80/215   train_loss = 2.871\n",
      "Epoch 168 Batch   90/215   train_loss = 3.208\n",
      "Epoch 168 Batch  100/215   train_loss = 3.042\n",
      "Epoch 168 Batch  110/215   train_loss = 2.846\n",
      "Epoch 168 Batch  120/215   train_loss = 2.837\n",
      "Epoch 168 Batch  130/215   train_loss = 3.081\n",
      "Epoch 168 Batch  140/215   train_loss = 3.090\n",
      "Epoch 168 Batch  150/215   train_loss = 3.004\n",
      "Epoch 168 Batch  160/215   train_loss = 2.878\n",
      "Epoch 168 Batch  170/215   train_loss = 2.876\n",
      "Epoch 168 Batch  180/215   train_loss = 2.981\n",
      "Epoch 168 Batch  190/215   train_loss = 2.931\n",
      "Epoch 168 Batch  200/215   train_loss = 2.702\n",
      "Epoch 168 Batch  210/215   train_loss = 2.920\n",
      "Epoch 169 Batch    5/215   train_loss = 3.207\n",
      "Epoch 169 Batch   15/215   train_loss = 2.916\n",
      "Epoch 169 Batch   25/215   train_loss = 2.850\n",
      "Epoch 169 Batch   35/215   train_loss = 2.906\n",
      "Epoch 169 Batch   45/215   train_loss = 2.999\n",
      "Epoch 169 Batch   55/215   train_loss = 2.955\n",
      "Epoch 169 Batch   65/215   train_loss = 2.902\n",
      "Epoch 169 Batch   75/215   train_loss = 2.877\n",
      "Epoch 169 Batch   85/215   train_loss = 2.926\n",
      "Epoch 169 Batch   95/215   train_loss = 2.999\n",
      "Epoch 169 Batch  105/215   train_loss = 2.725\n",
      "Epoch 169 Batch  115/215   train_loss = 2.690\n",
      "Epoch 169 Batch  125/215   train_loss = 2.917\n",
      "Epoch 169 Batch  135/215   train_loss = 2.905\n",
      "Epoch 169 Batch  145/215   train_loss = 2.902\n",
      "Epoch 169 Batch  155/215   train_loss = 3.018\n",
      "Epoch 169 Batch  165/215   train_loss = 3.109\n",
      "Epoch 169 Batch  175/215   train_loss = 3.113\n",
      "Epoch 169 Batch  185/215   train_loss = 3.122\n",
      "Epoch 169 Batch  195/215   train_loss = 2.849\n",
      "Epoch 169 Batch  205/215   train_loss = 2.982\n",
      "Epoch 170 Batch    0/215   train_loss = 2.993\n",
      "Epoch 170 Batch   10/215   train_loss = 3.056\n",
      "Epoch 170 Batch   20/215   train_loss = 2.846\n",
      "Epoch 170 Batch   30/215   train_loss = 2.687\n",
      "Epoch 170 Batch   40/215   train_loss = 2.813\n",
      "Epoch 170 Batch   50/215   train_loss = 2.713\n",
      "Epoch 170 Batch   60/215   train_loss = 3.154\n",
      "Epoch 170 Batch   70/215   train_loss = 2.776\n",
      "Epoch 170 Batch   80/215   train_loss = 2.934\n",
      "Epoch 170 Batch   90/215   train_loss = 3.141\n",
      "Epoch 170 Batch  100/215   train_loss = 2.828\n",
      "Epoch 170 Batch  110/215   train_loss = 2.733\n",
      "Epoch 170 Batch  120/215   train_loss = 2.934\n",
      "Epoch 170 Batch  130/215   train_loss = 3.144\n",
      "Epoch 170 Batch  140/215   train_loss = 3.146\n",
      "Epoch 170 Batch  150/215   train_loss = 3.034\n",
      "Epoch 170 Batch  160/215   train_loss = 2.928\n",
      "Epoch 170 Batch  170/215   train_loss = 2.896\n",
      "Epoch 170 Batch  180/215   train_loss = 2.903\n",
      "Epoch 170 Batch  190/215   train_loss = 3.044\n",
      "Epoch 170 Batch  200/215   train_loss = 3.057\n",
      "Epoch 170 Batch  210/215   train_loss = 2.826\n",
      "Epoch 171 Batch    5/215   train_loss = 2.991\n",
      "Epoch 171 Batch   15/215   train_loss = 2.702\n",
      "Epoch 171 Batch   25/215   train_loss = 2.865\n",
      "Epoch 171 Batch   35/215   train_loss = 3.000\n",
      "Epoch 171 Batch   45/215   train_loss = 3.136\n",
      "Epoch 171 Batch   55/215   train_loss = 2.890\n",
      "Epoch 171 Batch   65/215   train_loss = 3.000\n",
      "Epoch 171 Batch   75/215   train_loss = 2.796\n",
      "Epoch 171 Batch   85/215   train_loss = 2.820\n",
      "Epoch 171 Batch   95/215   train_loss = 2.802\n",
      "Epoch 171 Batch  105/215   train_loss = 2.749\n",
      "Epoch 171 Batch  115/215   train_loss = 2.723\n",
      "Epoch 171 Batch  125/215   train_loss = 2.895\n",
      "Epoch 171 Batch  135/215   train_loss = 2.730\n",
      "Epoch 171 Batch  145/215   train_loss = 2.943\n",
      "Epoch 171 Batch  155/215   train_loss = 2.981\n",
      "Epoch 171 Batch  165/215   train_loss = 2.682\n",
      "Epoch 171 Batch  175/215   train_loss = 3.078\n",
      "Epoch 171 Batch  185/215   train_loss = 2.977\n",
      "Epoch 171 Batch  195/215   train_loss = 2.763\n",
      "Epoch 171 Batch  205/215   train_loss = 2.766\n",
      "Epoch 172 Batch    0/215   train_loss = 3.071\n",
      "Epoch 172 Batch   10/215   train_loss = 3.015\n",
      "Epoch 172 Batch   20/215   train_loss = 2.902\n",
      "Epoch 172 Batch   30/215   train_loss = 2.722\n",
      "Epoch 172 Batch   40/215   train_loss = 2.883\n",
      "Epoch 172 Batch   50/215   train_loss = 2.900\n",
      "Epoch 172 Batch   60/215   train_loss = 2.960\n",
      "Epoch 172 Batch   70/215   train_loss = 2.715\n",
      "Epoch 172 Batch   80/215   train_loss = 2.886\n",
      "Epoch 172 Batch   90/215   train_loss = 3.190\n",
      "Epoch 172 Batch  100/215   train_loss = 3.065\n",
      "Epoch 172 Batch  110/215   train_loss = 2.817\n",
      "Epoch 172 Batch  120/215   train_loss = 2.952\n",
      "Epoch 172 Batch  130/215   train_loss = 3.036\n",
      "Epoch 172 Batch  140/215   train_loss = 3.015\n",
      "Epoch 172 Batch  150/215   train_loss = 2.959\n",
      "Epoch 172 Batch  160/215   train_loss = 2.843\n",
      "Epoch 172 Batch  170/215   train_loss = 2.870\n",
      "Epoch 172 Batch  180/215   train_loss = 2.793\n",
      "Epoch 172 Batch  190/215   train_loss = 3.022\n",
      "Epoch 172 Batch  200/215   train_loss = 2.748\n",
      "Epoch 172 Batch  210/215   train_loss = 2.712\n",
      "Epoch 173 Batch    5/215   train_loss = 3.095\n",
      "Epoch 173 Batch   15/215   train_loss = 2.715\n",
      "Epoch 173 Batch   25/215   train_loss = 2.810\n",
      "Epoch 173 Batch   35/215   train_loss = 3.014\n",
      "Epoch 173 Batch   45/215   train_loss = 2.881\n",
      "Epoch 173 Batch   55/215   train_loss = 2.811\n",
      "Epoch 173 Batch   65/215   train_loss = 3.024\n",
      "Epoch 173 Batch   75/215   train_loss = 2.899\n",
      "Epoch 173 Batch   85/215   train_loss = 2.833\n",
      "Epoch 173 Batch   95/215   train_loss = 2.887\n",
      "Epoch 173 Batch  105/215   train_loss = 2.914\n",
      "Epoch 173 Batch  115/215   train_loss = 2.821\n",
      "Epoch 173 Batch  125/215   train_loss = 2.916\n",
      "Epoch 173 Batch  135/215   train_loss = 2.774\n",
      "Epoch 173 Batch  145/215   train_loss = 2.839\n",
      "Epoch 173 Batch  155/215   train_loss = 2.836\n",
      "Epoch 173 Batch  165/215   train_loss = 3.126\n",
      "Epoch 173 Batch  175/215   train_loss = 3.175\n",
      "Epoch 173 Batch  185/215   train_loss = 2.914\n",
      "Epoch 173 Batch  195/215   train_loss = 2.750\n",
      "Epoch 173 Batch  205/215   train_loss = 2.859\n",
      "Epoch 174 Batch    0/215   train_loss = 2.948\n",
      "Epoch 174 Batch   10/215   train_loss = 3.065\n",
      "Epoch 174 Batch   20/215   train_loss = 3.043\n",
      "Epoch 174 Batch   30/215   train_loss = 2.726\n",
      "Epoch 174 Batch   40/215   train_loss = 3.023\n",
      "Epoch 174 Batch   50/215   train_loss = 2.968\n",
      "Epoch 174 Batch   60/215   train_loss = 2.994\n",
      "Epoch 174 Batch   70/215   train_loss = 3.004\n",
      "Epoch 174 Batch   80/215   train_loss = 2.905\n",
      "Epoch 174 Batch   90/215   train_loss = 3.048\n",
      "Epoch 174 Batch  100/215   train_loss = 3.021\n",
      "Epoch 174 Batch  110/215   train_loss = 2.917\n",
      "Epoch 174 Batch  120/215   train_loss = 3.010\n",
      "Epoch 174 Batch  130/215   train_loss = 3.051\n",
      "Epoch 174 Batch  140/215   train_loss = 2.874\n",
      "Epoch 174 Batch  150/215   train_loss = 2.987\n",
      "Epoch 174 Batch  160/215   train_loss = 2.842\n",
      "Epoch 174 Batch  170/215   train_loss = 2.785\n",
      "Epoch 174 Batch  180/215   train_loss = 3.101\n",
      "Epoch 174 Batch  190/215   train_loss = 3.033\n",
      "Epoch 174 Batch  200/215   train_loss = 2.929\n",
      "Epoch 174 Batch  210/215   train_loss = 2.727\n",
      "Epoch 175 Batch    5/215   train_loss = 2.956\n",
      "Epoch 175 Batch   15/215   train_loss = 3.094\n",
      "Epoch 175 Batch   25/215   train_loss = 2.890\n",
      "Epoch 175 Batch   35/215   train_loss = 2.851\n",
      "Epoch 175 Batch   45/215   train_loss = 3.141\n",
      "Epoch 175 Batch   55/215   train_loss = 2.936\n",
      "Epoch 175 Batch   65/215   train_loss = 3.173\n",
      "Epoch 175 Batch   75/215   train_loss = 2.919\n",
      "Epoch 175 Batch   85/215   train_loss = 2.833\n",
      "Epoch 175 Batch   95/215   train_loss = 2.849\n",
      "Epoch 175 Batch  105/215   train_loss = 2.833\n",
      "Epoch 175 Batch  115/215   train_loss = 2.602\n",
      "Epoch 175 Batch  125/215   train_loss = 2.874\n",
      "Epoch 175 Batch  135/215   train_loss = 2.707\n",
      "Epoch 175 Batch  145/215   train_loss = 2.885\n",
      "Epoch 175 Batch  155/215   train_loss = 2.901\n",
      "Epoch 175 Batch  165/215   train_loss = 2.759\n",
      "Epoch 175 Batch  175/215   train_loss = 3.050\n",
      "Epoch 175 Batch  185/215   train_loss = 2.955\n",
      "Epoch 175 Batch  195/215   train_loss = 2.834\n",
      "Epoch 175 Batch  205/215   train_loss = 2.953\n",
      "Epoch 176 Batch    0/215   train_loss = 2.980\n",
      "Epoch 176 Batch   10/215   train_loss = 2.986\n",
      "Epoch 176 Batch   20/215   train_loss = 2.878\n",
      "Epoch 176 Batch   30/215   train_loss = 2.797\n",
      "Epoch 176 Batch   40/215   train_loss = 3.033\n",
      "Epoch 176 Batch   50/215   train_loss = 2.874\n",
      "Epoch 176 Batch   60/215   train_loss = 3.038\n",
      "Epoch 176 Batch   70/215   train_loss = 2.788\n",
      "Epoch 176 Batch   80/215   train_loss = 2.915\n",
      "Epoch 176 Batch   90/215   train_loss = 2.985\n",
      "Epoch 176 Batch  100/215   train_loss = 2.927\n",
      "Epoch 176 Batch  110/215   train_loss = 2.801\n",
      "Epoch 176 Batch  120/215   train_loss = 2.925\n",
      "Epoch 176 Batch  130/215   train_loss = 3.119\n",
      "Epoch 176 Batch  140/215   train_loss = 2.841\n",
      "Epoch 176 Batch  150/215   train_loss = 2.952\n",
      "Epoch 176 Batch  160/215   train_loss = 2.827\n",
      "Epoch 176 Batch  170/215   train_loss = 2.860\n",
      "Epoch 176 Batch  180/215   train_loss = 2.923\n",
      "Epoch 176 Batch  190/215   train_loss = 3.002\n",
      "Epoch 176 Batch  200/215   train_loss = 2.854\n",
      "Epoch 176 Batch  210/215   train_loss = 2.749\n",
      "Epoch 177 Batch    5/215   train_loss = 3.127\n",
      "Epoch 177 Batch   15/215   train_loss = 2.756\n",
      "Epoch 177 Batch   25/215   train_loss = 2.791\n",
      "Epoch 177 Batch   35/215   train_loss = 3.062\n",
      "Epoch 177 Batch   45/215   train_loss = 2.849\n",
      "Epoch 177 Batch   55/215   train_loss = 2.828\n",
      "Epoch 177 Batch   65/215   train_loss = 2.967\n",
      "Epoch 177 Batch   75/215   train_loss = 2.813\n",
      "Epoch 177 Batch   85/215   train_loss = 2.816\n",
      "Epoch 177 Batch   95/215   train_loss = 2.865\n",
      "Epoch 177 Batch  105/215   train_loss = 2.836\n",
      "Epoch 177 Batch  115/215   train_loss = 2.708\n",
      "Epoch 177 Batch  125/215   train_loss = 2.797\n",
      "Epoch 177 Batch  135/215   train_loss = 2.769\n",
      "Epoch 177 Batch  145/215   train_loss = 2.917\n",
      "Epoch 177 Batch  155/215   train_loss = 2.886\n",
      "Epoch 177 Batch  165/215   train_loss = 2.941\n",
      "Epoch 177 Batch  175/215   train_loss = 3.011\n",
      "Epoch 177 Batch  185/215   train_loss = 2.891\n",
      "Epoch 177 Batch  195/215   train_loss = 2.767\n",
      "Epoch 177 Batch  205/215   train_loss = 3.001\n",
      "Epoch 178 Batch    0/215   train_loss = 2.970\n",
      "Epoch 178 Batch   10/215   train_loss = 3.171\n",
      "Epoch 178 Batch   20/215   train_loss = 2.932\n",
      "Epoch 178 Batch   30/215   train_loss = 2.705\n",
      "Epoch 178 Batch   40/215   train_loss = 2.916\n",
      "Epoch 178 Batch   50/215   train_loss = 2.943\n",
      "Epoch 178 Batch   60/215   train_loss = 3.010\n",
      "Epoch 178 Batch   70/215   train_loss = 2.644\n",
      "Epoch 178 Batch   80/215   train_loss = 2.835\n",
      "Epoch 178 Batch   90/215   train_loss = 2.950\n",
      "Epoch 178 Batch  100/215   train_loss = 2.919\n",
      "Epoch 178 Batch  110/215   train_loss = 2.922\n",
      "Epoch 178 Batch  120/215   train_loss = 2.886\n",
      "Epoch 178 Batch  130/215   train_loss = 3.069\n",
      "Epoch 178 Batch  140/215   train_loss = 3.065\n",
      "Epoch 178 Batch  150/215   train_loss = 2.883\n",
      "Epoch 178 Batch  160/215   train_loss = 2.907\n",
      "Epoch 178 Batch  170/215   train_loss = 2.857\n",
      "Epoch 178 Batch  180/215   train_loss = 3.040\n",
      "Epoch 178 Batch  190/215   train_loss = 3.169\n",
      "Epoch 178 Batch  200/215   train_loss = 2.891\n",
      "Epoch 178 Batch  210/215   train_loss = 2.604\n",
      "Epoch 179 Batch    5/215   train_loss = 3.006\n",
      "Epoch 179 Batch   15/215   train_loss = 2.807\n",
      "Epoch 179 Batch   25/215   train_loss = 2.736\n",
      "Epoch 179 Batch   35/215   train_loss = 3.027\n",
      "Epoch 179 Batch   45/215   train_loss = 2.822\n",
      "Epoch 179 Batch   55/215   train_loss = 2.750\n",
      "Epoch 179 Batch   65/215   train_loss = 3.019\n",
      "Epoch 179 Batch   75/215   train_loss = 3.280\n",
      "Epoch 179 Batch   85/215   train_loss = 2.828\n",
      "Epoch 179 Batch   95/215   train_loss = 2.948\n",
      "Epoch 179 Batch  105/215   train_loss = 2.789\n",
      "Epoch 179 Batch  115/215   train_loss = 2.768\n",
      "Epoch 179 Batch  125/215   train_loss = 2.783\n",
      "Epoch 179 Batch  135/215   train_loss = 2.924\n",
      "Epoch 179 Batch  145/215   train_loss = 2.920\n",
      "Epoch 179 Batch  155/215   train_loss = 2.891\n",
      "Epoch 179 Batch  165/215   train_loss = 2.948\n",
      "Epoch 179 Batch  175/215   train_loss = 3.163\n",
      "Epoch 179 Batch  185/215   train_loss = 3.065\n",
      "Epoch 179 Batch  195/215   train_loss = 2.723\n",
      "Epoch 179 Batch  205/215   train_loss = 2.875\n",
      "Epoch 180 Batch    0/215   train_loss = 2.910\n",
      "Epoch 180 Batch   10/215   train_loss = 2.868\n",
      "Epoch 180 Batch   20/215   train_loss = 2.865\n",
      "Epoch 180 Batch   30/215   train_loss = 2.887\n",
      "Epoch 180 Batch   40/215   train_loss = 2.943\n",
      "Epoch 180 Batch   50/215   train_loss = 2.743\n",
      "Epoch 180 Batch   60/215   train_loss = 3.090\n",
      "Epoch 180 Batch   70/215   train_loss = 2.890\n",
      "Epoch 180 Batch   80/215   train_loss = 3.008\n",
      "Epoch 180 Batch   90/215   train_loss = 2.957\n",
      "Epoch 180 Batch  100/215   train_loss = 2.927\n",
      "Epoch 180 Batch  110/215   train_loss = 2.830\n",
      "Epoch 180 Batch  120/215   train_loss = 2.992\n",
      "Epoch 180 Batch  130/215   train_loss = 2.932\n",
      "Epoch 180 Batch  140/215   train_loss = 2.885\n",
      "Epoch 180 Batch  150/215   train_loss = 3.013\n",
      "Epoch 180 Batch  160/215   train_loss = 3.012\n",
      "Epoch 180 Batch  170/215   train_loss = 2.999\n",
      "Epoch 180 Batch  180/215   train_loss = 2.879\n",
      "Epoch 180 Batch  190/215   train_loss = 3.051\n",
      "Epoch 180 Batch  200/215   train_loss = 2.809\n",
      "Epoch 180 Batch  210/215   train_loss = 2.676\n",
      "Epoch 181 Batch    5/215   train_loss = 2.840\n",
      "Epoch 181 Batch   15/215   train_loss = 2.777\n",
      "Epoch 181 Batch   25/215   train_loss = 2.763\n",
      "Epoch 181 Batch   35/215   train_loss = 2.914\n",
      "Epoch 181 Batch   45/215   train_loss = 2.914\n",
      "Epoch 181 Batch   55/215   train_loss = 2.710\n",
      "Epoch 181 Batch   65/215   train_loss = 2.923\n",
      "Epoch 181 Batch   75/215   train_loss = 2.734\n",
      "Epoch 181 Batch   85/215   train_loss = 2.718\n",
      "Epoch 181 Batch   95/215   train_loss = 2.957\n",
      "Epoch 181 Batch  105/215   train_loss = 2.747\n",
      "Epoch 181 Batch  115/215   train_loss = 2.843\n",
      "Epoch 181 Batch  125/215   train_loss = 2.754\n",
      "Epoch 181 Batch  135/215   train_loss = 2.752\n",
      "Epoch 181 Batch  145/215   train_loss = 2.915\n",
      "Epoch 181 Batch  155/215   train_loss = 2.951\n",
      "Epoch 181 Batch  165/215   train_loss = 3.031\n",
      "Epoch 181 Batch  175/215   train_loss = 2.982\n",
      "Epoch 181 Batch  185/215   train_loss = 2.955\n",
      "Epoch 181 Batch  195/215   train_loss = 2.705\n",
      "Epoch 181 Batch  205/215   train_loss = 2.900\n",
      "Epoch 182 Batch    0/215   train_loss = 3.130\n",
      "Epoch 182 Batch   10/215   train_loss = 3.052\n",
      "Epoch 182 Batch   20/215   train_loss = 2.878\n",
      "Epoch 182 Batch   30/215   train_loss = 2.845\n",
      "Epoch 182 Batch   40/215   train_loss = 2.771\n",
      "Epoch 182 Batch   50/215   train_loss = 2.753\n",
      "Epoch 182 Batch   60/215   train_loss = 2.870\n",
      "Epoch 182 Batch   70/215   train_loss = 2.879\n",
      "Epoch 182 Batch   80/215   train_loss = 2.868\n",
      "Epoch 182 Batch   90/215   train_loss = 2.855\n",
      "Epoch 182 Batch  100/215   train_loss = 3.013\n",
      "Epoch 182 Batch  110/215   train_loss = 2.801\n",
      "Epoch 182 Batch  120/215   train_loss = 2.731\n",
      "Epoch 182 Batch  130/215   train_loss = 3.151\n",
      "Epoch 182 Batch  140/215   train_loss = 2.853\n",
      "Epoch 182 Batch  150/215   train_loss = 3.014\n",
      "Epoch 182 Batch  160/215   train_loss = 3.076\n",
      "Epoch 182 Batch  170/215   train_loss = 2.868\n",
      "Epoch 182 Batch  180/215   train_loss = 2.914\n",
      "Epoch 182 Batch  190/215   train_loss = 2.976\n",
      "Epoch 182 Batch  200/215   train_loss = 2.859\n",
      "Epoch 182 Batch  210/215   train_loss = 2.749\n",
      "Epoch 183 Batch    5/215   train_loss = 2.833\n",
      "Epoch 183 Batch   15/215   train_loss = 2.790\n",
      "Epoch 183 Batch   25/215   train_loss = 2.806\n",
      "Epoch 183 Batch   35/215   train_loss = 2.904\n",
      "Epoch 183 Batch   45/215   train_loss = 2.890\n",
      "Epoch 183 Batch   55/215   train_loss = 2.896\n",
      "Epoch 183 Batch   65/215   train_loss = 3.086\n",
      "Epoch 183 Batch   75/215   train_loss = 2.883\n",
      "Epoch 183 Batch   85/215   train_loss = 2.973\n",
      "Epoch 183 Batch   95/215   train_loss = 2.885\n",
      "Epoch 183 Batch  105/215   train_loss = 2.700\n",
      "Epoch 183 Batch  115/215   train_loss = 2.774\n",
      "Epoch 183 Batch  125/215   train_loss = 2.693\n",
      "Epoch 183 Batch  135/215   train_loss = 2.951\n",
      "Epoch 183 Batch  145/215   train_loss = 2.870\n",
      "Epoch 183 Batch  155/215   train_loss = 2.895\n",
      "Epoch 183 Batch  165/215   train_loss = 2.929\n",
      "Epoch 183 Batch  175/215   train_loss = 3.077\n",
      "Epoch 183 Batch  185/215   train_loss = 2.850\n",
      "Epoch 183 Batch  195/215   train_loss = 2.778\n",
      "Epoch 183 Batch  205/215   train_loss = 2.801\n",
      "Epoch 184 Batch    0/215   train_loss = 3.043\n",
      "Epoch 184 Batch   10/215   train_loss = 2.998\n",
      "Epoch 184 Batch   20/215   train_loss = 2.979\n",
      "Epoch 184 Batch   30/215   train_loss = 2.919\n",
      "Epoch 184 Batch   40/215   train_loss = 3.022\n",
      "Epoch 184 Batch   50/215   train_loss = 2.748\n",
      "Epoch 184 Batch   60/215   train_loss = 3.071\n",
      "Epoch 184 Batch   70/215   train_loss = 2.770\n",
      "Epoch 184 Batch   80/215   train_loss = 2.918\n",
      "Epoch 184 Batch   90/215   train_loss = 2.953\n",
      "Epoch 184 Batch  100/215   train_loss = 2.757\n",
      "Epoch 184 Batch  110/215   train_loss = 2.872\n",
      "Epoch 184 Batch  120/215   train_loss = 3.120\n",
      "Epoch 184 Batch  130/215   train_loss = 3.241\n",
      "Epoch 184 Batch  140/215   train_loss = 2.810\n",
      "Epoch 184 Batch  150/215   train_loss = 2.817\n",
      "Epoch 184 Batch  160/215   train_loss = 2.872\n",
      "Epoch 184 Batch  170/215   train_loss = 3.104\n",
      "Epoch 184 Batch  180/215   train_loss = 2.794\n",
      "Epoch 184 Batch  190/215   train_loss = 3.022\n",
      "Epoch 184 Batch  200/215   train_loss = 2.876\n",
      "Epoch 184 Batch  210/215   train_loss = 2.763\n",
      "Epoch 185 Batch    5/215   train_loss = 2.971\n",
      "Epoch 185 Batch   15/215   train_loss = 2.893\n",
      "Epoch 185 Batch   25/215   train_loss = 2.783\n",
      "Epoch 185 Batch   35/215   train_loss = 2.963\n",
      "Epoch 185 Batch   45/215   train_loss = 2.765\n",
      "Epoch 185 Batch   55/215   train_loss = 2.787\n",
      "Epoch 185 Batch   65/215   train_loss = 2.931\n",
      "Epoch 185 Batch   75/215   train_loss = 2.872\n",
      "Epoch 185 Batch   85/215   train_loss = 2.803\n",
      "Epoch 185 Batch   95/215   train_loss = 2.821\n",
      "Epoch 185 Batch  105/215   train_loss = 2.694\n",
      "Epoch 185 Batch  115/215   train_loss = 2.735\n",
      "Epoch 185 Batch  125/215   train_loss = 2.766\n",
      "Epoch 185 Batch  135/215   train_loss = 2.900\n",
      "Epoch 185 Batch  145/215   train_loss = 3.003\n",
      "Epoch 185 Batch  155/215   train_loss = 2.861\n",
      "Epoch 185 Batch  165/215   train_loss = 3.054\n",
      "Epoch 185 Batch  175/215   train_loss = 3.181\n",
      "Epoch 185 Batch  185/215   train_loss = 2.947\n",
      "Epoch 185 Batch  195/215   train_loss = 2.683\n",
      "Epoch 185 Batch  205/215   train_loss = 2.875\n",
      "Epoch 186 Batch    0/215   train_loss = 2.955\n",
      "Epoch 186 Batch   10/215   train_loss = 3.115\n",
      "Epoch 186 Batch   20/215   train_loss = 3.020\n",
      "Epoch 186 Batch   30/215   train_loss = 2.849\n",
      "Epoch 186 Batch   40/215   train_loss = 2.713\n",
      "Epoch 186 Batch   50/215   train_loss = 3.029\n",
      "Epoch 186 Batch   60/215   train_loss = 2.912\n",
      "Epoch 186 Batch   70/215   train_loss = 2.728\n",
      "Epoch 186 Batch   80/215   train_loss = 2.764\n",
      "Epoch 186 Batch   90/215   train_loss = 2.903\n",
      "Epoch 186 Batch  100/215   train_loss = 3.107\n",
      "Epoch 186 Batch  110/215   train_loss = 2.818\n",
      "Epoch 186 Batch  120/215   train_loss = 3.027\n",
      "Epoch 186 Batch  130/215   train_loss = 3.052\n",
      "Epoch 186 Batch  140/215   train_loss = 2.764\n",
      "Epoch 186 Batch  150/215   train_loss = 3.049\n",
      "Epoch 186 Batch  160/215   train_loss = 2.805\n",
      "Epoch 186 Batch  170/215   train_loss = 2.926\n",
      "Epoch 186 Batch  180/215   train_loss = 3.001\n",
      "Epoch 186 Batch  190/215   train_loss = 3.039\n",
      "Epoch 186 Batch  200/215   train_loss = 2.848\n",
      "Epoch 186 Batch  210/215   train_loss = 2.788\n",
      "Epoch 187 Batch    5/215   train_loss = 3.079\n",
      "Epoch 187 Batch   15/215   train_loss = 2.855\n",
      "Epoch 187 Batch   25/215   train_loss = 2.845\n",
      "Epoch 187 Batch   35/215   train_loss = 2.926\n",
      "Epoch 187 Batch   45/215   train_loss = 2.811\n",
      "Epoch 187 Batch   55/215   train_loss = 2.696\n",
      "Epoch 187 Batch   65/215   train_loss = 3.067\n",
      "Epoch 187 Batch   75/215   train_loss = 2.740\n",
      "Epoch 187 Batch   85/215   train_loss = 2.666\n",
      "Epoch 187 Batch   95/215   train_loss = 2.695\n",
      "Epoch 187 Batch  105/215   train_loss = 2.783\n",
      "Epoch 187 Batch  115/215   train_loss = 2.795\n",
      "Epoch 187 Batch  125/215   train_loss = 2.879\n",
      "Epoch 187 Batch  135/215   train_loss = 2.900\n",
      "Epoch 187 Batch  145/215   train_loss = 2.904\n",
      "Epoch 187 Batch  155/215   train_loss = 3.040\n",
      "Epoch 187 Batch  165/215   train_loss = 2.793\n",
      "Epoch 187 Batch  175/215   train_loss = 2.940\n",
      "Epoch 187 Batch  185/215   train_loss = 2.924\n",
      "Epoch 187 Batch  195/215   train_loss = 2.768\n",
      "Epoch 187 Batch  205/215   train_loss = 2.735\n",
      "Epoch 188 Batch    0/215   train_loss = 2.979\n",
      "Epoch 188 Batch   10/215   train_loss = 2.904\n",
      "Epoch 188 Batch   20/215   train_loss = 2.912\n",
      "Epoch 188 Batch   30/215   train_loss = 2.749\n",
      "Epoch 188 Batch   40/215   train_loss = 2.938\n",
      "Epoch 188 Batch   50/215   train_loss = 2.975\n",
      "Epoch 188 Batch   60/215   train_loss = 3.035\n",
      "Epoch 188 Batch   70/215   train_loss = 2.896\n",
      "Epoch 188 Batch   80/215   train_loss = 2.710\n",
      "Epoch 188 Batch   90/215   train_loss = 2.828\n",
      "Epoch 188 Batch  100/215   train_loss = 2.969\n",
      "Epoch 188 Batch  110/215   train_loss = 3.156\n",
      "Epoch 188 Batch  120/215   train_loss = 2.914\n",
      "Epoch 188 Batch  130/215   train_loss = 3.187\n",
      "Epoch 188 Batch  140/215   train_loss = 2.981\n",
      "Epoch 188 Batch  150/215   train_loss = 2.750\n",
      "Epoch 188 Batch  160/215   train_loss = 2.966\n",
      "Epoch 188 Batch  170/215   train_loss = 2.867\n",
      "Epoch 188 Batch  180/215   train_loss = 3.032\n",
      "Epoch 188 Batch  190/215   train_loss = 2.925\n",
      "Epoch 188 Batch  200/215   train_loss = 2.791\n",
      "Epoch 188 Batch  210/215   train_loss = 2.600\n",
      "Epoch 189 Batch    5/215   train_loss = 2.979\n",
      "Epoch 189 Batch   15/215   train_loss = 2.663\n",
      "Epoch 189 Batch   25/215   train_loss = 2.834\n",
      "Epoch 189 Batch   35/215   train_loss = 2.814\n",
      "Epoch 189 Batch   45/215   train_loss = 2.838\n",
      "Epoch 189 Batch   55/215   train_loss = 2.793\n",
      "Epoch 189 Batch   65/215   train_loss = 2.881\n",
      "Epoch 189 Batch   75/215   train_loss = 2.780\n",
      "Epoch 189 Batch   85/215   train_loss = 2.732\n",
      "Epoch 189 Batch   95/215   train_loss = 2.806\n",
      "Epoch 189 Batch  105/215   train_loss = 2.679\n",
      "Epoch 189 Batch  115/215   train_loss = 2.781\n",
      "Epoch 189 Batch  125/215   train_loss = 2.867\n",
      "Epoch 189 Batch  135/215   train_loss = 2.798\n",
      "Epoch 189 Batch  145/215   train_loss = 2.919\n",
      "Epoch 189 Batch  155/215   train_loss = 2.896\n",
      "Epoch 189 Batch  165/215   train_loss = 2.723\n",
      "Epoch 189 Batch  175/215   train_loss = 3.087\n",
      "Epoch 189 Batch  185/215   train_loss = 2.914\n",
      "Epoch 189 Batch  195/215   train_loss = 2.882\n",
      "Epoch 189 Batch  205/215   train_loss = 2.847\n",
      "Epoch 190 Batch    0/215   train_loss = 3.022\n",
      "Epoch 190 Batch   10/215   train_loss = 2.993\n",
      "Epoch 190 Batch   20/215   train_loss = 2.957\n",
      "Epoch 190 Batch   30/215   train_loss = 2.772\n",
      "Epoch 190 Batch   40/215   train_loss = 2.761\n",
      "Epoch 190 Batch   50/215   train_loss = 2.872\n",
      "Epoch 190 Batch   60/215   train_loss = 2.926\n",
      "Epoch 190 Batch   70/215   train_loss = 2.720\n",
      "Epoch 190 Batch   80/215   train_loss = 2.883\n",
      "Epoch 190 Batch   90/215   train_loss = 2.961\n",
      "Epoch 190 Batch  100/215   train_loss = 2.875\n",
      "Epoch 190 Batch  110/215   train_loss = 2.784\n",
      "Epoch 190 Batch  120/215   train_loss = 2.837\n",
      "Epoch 190 Batch  130/215   train_loss = 2.924\n",
      "Epoch 190 Batch  140/215   train_loss = 3.060\n",
      "Epoch 190 Batch  150/215   train_loss = 2.876\n",
      "Epoch 190 Batch  160/215   train_loss = 2.935\n",
      "Epoch 190 Batch  170/215   train_loss = 2.776\n",
      "Epoch 190 Batch  180/215   train_loss = 2.871\n",
      "Epoch 190 Batch  190/215   train_loss = 2.867\n",
      "Epoch 190 Batch  200/215   train_loss = 2.847\n",
      "Epoch 190 Batch  210/215   train_loss = 2.640\n",
      "Epoch 191 Batch    5/215   train_loss = 3.007\n",
      "Epoch 191 Batch   15/215   train_loss = 2.778\n",
      "Epoch 191 Batch   25/215   train_loss = 2.707\n",
      "Epoch 191 Batch   35/215   train_loss = 2.934\n",
      "Epoch 191 Batch   45/215   train_loss = 3.022\n",
      "Epoch 191 Batch   55/215   train_loss = 2.839\n",
      "Epoch 191 Batch   65/215   train_loss = 3.008\n",
      "Epoch 191 Batch   75/215   train_loss = 2.823\n",
      "Epoch 191 Batch   85/215   train_loss = 2.788\n",
      "Epoch 191 Batch   95/215   train_loss = 2.774\n",
      "Epoch 191 Batch  105/215   train_loss = 2.784\n",
      "Epoch 191 Batch  115/215   train_loss = 2.751\n",
      "Epoch 191 Batch  125/215   train_loss = 3.001\n",
      "Epoch 191 Batch  135/215   train_loss = 2.931\n",
      "Epoch 191 Batch  145/215   train_loss = 2.927\n",
      "Epoch 191 Batch  155/215   train_loss = 3.077\n",
      "Epoch 191 Batch  165/215   train_loss = 2.812\n",
      "Epoch 191 Batch  175/215   train_loss = 3.207\n",
      "Epoch 191 Batch  185/215   train_loss = 2.999\n",
      "Epoch 191 Batch  195/215   train_loss = 2.839\n",
      "Epoch 191 Batch  205/215   train_loss = 2.920\n",
      "Epoch 192 Batch    0/215   train_loss = 2.885\n",
      "Epoch 192 Batch   10/215   train_loss = 3.041\n",
      "Epoch 192 Batch   20/215   train_loss = 2.805\n",
      "Epoch 192 Batch   30/215   train_loss = 2.708\n",
      "Epoch 192 Batch   40/215   train_loss = 2.878\n",
      "Epoch 192 Batch   50/215   train_loss = 2.654\n",
      "Epoch 192 Batch   60/215   train_loss = 3.051\n",
      "Epoch 192 Batch   70/215   train_loss = 2.651\n",
      "Epoch 192 Batch   80/215   train_loss = 2.792\n",
      "Epoch 192 Batch   90/215   train_loss = 2.863\n",
      "Epoch 192 Batch  100/215   train_loss = 3.015\n",
      "Epoch 192 Batch  110/215   train_loss = 2.827\n",
      "Epoch 192 Batch  120/215   train_loss = 2.833\n",
      "Epoch 192 Batch  130/215   train_loss = 2.908\n",
      "Epoch 192 Batch  140/215   train_loss = 2.862\n",
      "Epoch 192 Batch  150/215   train_loss = 2.998\n",
      "Epoch 192 Batch  160/215   train_loss = 2.903\n",
      "Epoch 192 Batch  170/215   train_loss = 2.698\n",
      "Epoch 192 Batch  180/215   train_loss = 2.889\n",
      "Epoch 192 Batch  190/215   train_loss = 2.823\n",
      "Epoch 192 Batch  200/215   train_loss = 2.586\n",
      "Epoch 192 Batch  210/215   train_loss = 2.730\n",
      "Epoch 193 Batch    5/215   train_loss = 3.055\n",
      "Epoch 193 Batch   15/215   train_loss = 2.791\n",
      "Epoch 193 Batch   25/215   train_loss = 2.964\n",
      "Epoch 193 Batch   35/215   train_loss = 3.043\n",
      "Epoch 193 Batch   45/215   train_loss = 2.798\n",
      "Epoch 193 Batch   55/215   train_loss = 2.780\n",
      "Epoch 193 Batch   65/215   train_loss = 3.002\n",
      "Epoch 193 Batch   75/215   train_loss = 2.812\n",
      "Epoch 193 Batch   85/215   train_loss = 2.735\n",
      "Epoch 193 Batch   95/215   train_loss = 2.796\n",
      "Epoch 193 Batch  105/215   train_loss = 2.851\n",
      "Epoch 193 Batch  115/215   train_loss = 2.867\n",
      "Epoch 193 Batch  125/215   train_loss = 2.809\n",
      "Epoch 193 Batch  135/215   train_loss = 2.946\n",
      "Epoch 193 Batch  145/215   train_loss = 2.930\n",
      "Epoch 193 Batch  155/215   train_loss = 2.947\n",
      "Epoch 193 Batch  165/215   train_loss = 2.725\n",
      "Epoch 193 Batch  175/215   train_loss = 3.046\n",
      "Epoch 193 Batch  185/215   train_loss = 2.884\n",
      "Epoch 193 Batch  195/215   train_loss = 2.798\n",
      "Epoch 193 Batch  205/215   train_loss = 2.719\n",
      "Epoch 194 Batch    0/215   train_loss = 3.095\n",
      "Epoch 194 Batch   10/215   train_loss = 2.966\n",
      "Epoch 194 Batch   20/215   train_loss = 2.778\n",
      "Epoch 194 Batch   30/215   train_loss = 2.765\n",
      "Epoch 194 Batch   40/215   train_loss = 2.830\n",
      "Epoch 194 Batch   50/215   train_loss = 2.701\n",
      "Epoch 194 Batch   60/215   train_loss = 3.024\n",
      "Epoch 194 Batch   70/215   train_loss = 2.710\n",
      "Epoch 194 Batch   80/215   train_loss = 3.048\n",
      "Epoch 194 Batch   90/215   train_loss = 3.009\n",
      "Epoch 194 Batch  100/215   train_loss = 2.977\n",
      "Epoch 194 Batch  110/215   train_loss = 2.786\n",
      "Epoch 194 Batch  120/215   train_loss = 2.918\n",
      "Epoch 194 Batch  130/215   train_loss = 3.128\n",
      "Epoch 194 Batch  140/215   train_loss = 2.772\n",
      "Epoch 194 Batch  150/215   train_loss = 2.898\n",
      "Epoch 194 Batch  160/215   train_loss = 2.909\n",
      "Epoch 194 Batch  170/215   train_loss = 2.847\n",
      "Epoch 194 Batch  180/215   train_loss = 2.897\n",
      "Epoch 194 Batch  190/215   train_loss = 2.967\n",
      "Epoch 194 Batch  200/215   train_loss = 2.788\n",
      "Epoch 194 Batch  210/215   train_loss = 2.703\n",
      "Epoch 195 Batch    5/215   train_loss = 3.042\n",
      "Epoch 195 Batch   15/215   train_loss = 2.721\n",
      "Epoch 195 Batch   25/215   train_loss = 2.860\n",
      "Epoch 195 Batch   35/215   train_loss = 3.119\n",
      "Epoch 195 Batch   45/215   train_loss = 2.905\n",
      "Epoch 195 Batch   55/215   train_loss = 2.862\n",
      "Epoch 195 Batch   65/215   train_loss = 2.894\n",
      "Epoch 195 Batch   75/215   train_loss = 2.824\n",
      "Epoch 195 Batch   85/215   train_loss = 2.747\n",
      "Epoch 195 Batch   95/215   train_loss = 2.829\n",
      "Epoch 195 Batch  105/215   train_loss = 2.647\n",
      "Epoch 195 Batch  115/215   train_loss = 2.670\n",
      "Epoch 195 Batch  125/215   train_loss = 2.799\n",
      "Epoch 195 Batch  135/215   train_loss = 2.799\n",
      "Epoch 195 Batch  145/215   train_loss = 2.850\n",
      "Epoch 195 Batch  155/215   train_loss = 2.848\n",
      "Epoch 195 Batch  165/215   train_loss = 2.783\n",
      "Epoch 195 Batch  175/215   train_loss = 3.108\n",
      "Epoch 195 Batch  185/215   train_loss = 2.854\n",
      "Epoch 195 Batch  195/215   train_loss = 2.832\n",
      "Epoch 195 Batch  205/215   train_loss = 3.005\n",
      "Epoch 196 Batch    0/215   train_loss = 2.903\n",
      "Epoch 196 Batch   10/215   train_loss = 3.061\n",
      "Epoch 196 Batch   20/215   train_loss = 2.955\n",
      "Epoch 196 Batch   30/215   train_loss = 2.788\n",
      "Epoch 196 Batch   40/215   train_loss = 2.916\n",
      "Epoch 196 Batch   50/215   train_loss = 2.716\n",
      "Epoch 196 Batch   60/215   train_loss = 2.892\n",
      "Epoch 196 Batch   70/215   train_loss = 2.926\n",
      "Epoch 196 Batch   80/215   train_loss = 3.164\n",
      "Epoch 196 Batch   90/215   train_loss = 2.808\n",
      "Epoch 196 Batch  100/215   train_loss = 2.829\n",
      "Epoch 196 Batch  110/215   train_loss = 2.885\n",
      "Epoch 196 Batch  120/215   train_loss = 3.035\n",
      "Epoch 196 Batch  130/215   train_loss = 3.066\n",
      "Epoch 196 Batch  140/215   train_loss = 2.813\n",
      "Epoch 196 Batch  150/215   train_loss = 2.852\n",
      "Epoch 196 Batch  160/215   train_loss = 2.917\n",
      "Epoch 196 Batch  170/215   train_loss = 2.767\n",
      "Epoch 196 Batch  180/215   train_loss = 2.953\n",
      "Epoch 196 Batch  190/215   train_loss = 2.979\n",
      "Epoch 196 Batch  200/215   train_loss = 2.644\n",
      "Epoch 196 Batch  210/215   train_loss = 2.715\n",
      "Epoch 197 Batch    5/215   train_loss = 3.018\n",
      "Epoch 197 Batch   15/215   train_loss = 2.698\n",
      "Epoch 197 Batch   25/215   train_loss = 2.985\n",
      "Epoch 197 Batch   35/215   train_loss = 2.992\n",
      "Epoch 197 Batch   45/215   train_loss = 2.887\n",
      "Epoch 197 Batch   55/215   train_loss = 3.072\n",
      "Epoch 197 Batch   65/215   train_loss = 2.985\n",
      "Epoch 197 Batch   75/215   train_loss = 2.745\n",
      "Epoch 197 Batch   85/215   train_loss = 2.889\n",
      "Epoch 197 Batch   95/215   train_loss = 2.669\n",
      "Epoch 197 Batch  105/215   train_loss = 2.716\n",
      "Epoch 197 Batch  115/215   train_loss = 2.817\n",
      "Epoch 197 Batch  125/215   train_loss = 2.827\n",
      "Epoch 197 Batch  135/215   train_loss = 2.820\n",
      "Epoch 197 Batch  145/215   train_loss = 3.158\n",
      "Epoch 197 Batch  155/215   train_loss = 2.823\n",
      "Epoch 197 Batch  165/215   train_loss = 3.014\n",
      "Epoch 197 Batch  175/215   train_loss = 3.104\n",
      "Epoch 197 Batch  185/215   train_loss = 2.919\n",
      "Epoch 197 Batch  195/215   train_loss = 2.863\n",
      "Epoch 197 Batch  205/215   train_loss = 2.862\n",
      "Epoch 198 Batch    0/215   train_loss = 2.951\n",
      "Epoch 198 Batch   10/215   train_loss = 2.901\n",
      "Epoch 198 Batch   20/215   train_loss = 2.977\n",
      "Epoch 198 Batch   30/215   train_loss = 2.719\n",
      "Epoch 198 Batch   40/215   train_loss = 2.879\n",
      "Epoch 198 Batch   50/215   train_loss = 2.776\n",
      "Epoch 198 Batch   60/215   train_loss = 2.944\n",
      "Epoch 198 Batch   70/215   train_loss = 3.120\n",
      "Epoch 198 Batch   80/215   train_loss = 2.869\n",
      "Epoch 198 Batch   90/215   train_loss = 3.040\n",
      "Epoch 198 Batch  100/215   train_loss = 3.145\n",
      "Epoch 198 Batch  110/215   train_loss = 2.841\n",
      "Epoch 198 Batch  120/215   train_loss = 2.901\n",
      "Epoch 198 Batch  130/215   train_loss = 3.123\n",
      "Epoch 198 Batch  140/215   train_loss = 2.903\n",
      "Epoch 198 Batch  150/215   train_loss = 2.879\n",
      "Epoch 198 Batch  160/215   train_loss = 2.927\n",
      "Epoch 198 Batch  170/215   train_loss = 2.723\n",
      "Epoch 198 Batch  180/215   train_loss = 2.976\n",
      "Epoch 198 Batch  190/215   train_loss = 2.971\n",
      "Epoch 198 Batch  200/215   train_loss = 2.692\n",
      "Epoch 198 Batch  210/215   train_loss = 2.904\n",
      "Epoch 199 Batch    5/215   train_loss = 2.922\n",
      "Epoch 199 Batch   15/215   train_loss = 2.727\n",
      "Epoch 199 Batch   25/215   train_loss = 2.793\n",
      "Epoch 199 Batch   35/215   train_loss = 2.969\n",
      "Epoch 199 Batch   45/215   train_loss = 2.808\n",
      "Epoch 199 Batch   55/215   train_loss = 2.778\n",
      "Epoch 199 Batch   65/215   train_loss = 3.290\n",
      "Epoch 199 Batch   75/215   train_loss = 2.842\n",
      "Epoch 199 Batch   85/215   train_loss = 2.904\n",
      "Epoch 199 Batch   95/215   train_loss = 2.976\n",
      "Epoch 199 Batch  105/215   train_loss = 2.807\n",
      "Epoch 199 Batch  115/215   train_loss = 2.707\n",
      "Epoch 199 Batch  125/215   train_loss = 2.814\n",
      "Epoch 199 Batch  135/215   train_loss = 2.868\n",
      "Epoch 199 Batch  145/215   train_loss = 2.915\n",
      "Epoch 199 Batch  155/215   train_loss = 2.964\n",
      "Epoch 199 Batch  165/215   train_loss = 2.918\n",
      "Epoch 199 Batch  175/215   train_loss = 2.898\n",
      "Epoch 199 Batch  185/215   train_loss = 2.905\n",
      "Epoch 199 Batch  195/215   train_loss = 2.922\n",
      "Epoch 199 Batch  205/215   train_loss = 2.766\n",
      "Epoch 200 Batch    0/215   train_loss = 3.088\n",
      "Epoch 200 Batch   10/215   train_loss = 2.903\n",
      "Epoch 200 Batch   20/215   train_loss = 2.808\n",
      "Epoch 200 Batch   30/215   train_loss = 2.805\n",
      "Epoch 200 Batch   40/215   train_loss = 2.824\n",
      "Epoch 200 Batch   50/215   train_loss = 2.864\n",
      "Epoch 200 Batch   60/215   train_loss = 2.950\n",
      "Epoch 200 Batch   70/215   train_loss = 2.749\n",
      "Epoch 200 Batch   80/215   train_loss = 2.940\n",
      "Epoch 200 Batch   90/215   train_loss = 2.811\n",
      "Epoch 200 Batch  100/215   train_loss = 3.175\n",
      "Epoch 200 Batch  110/215   train_loss = 2.829\n",
      "Epoch 200 Batch  120/215   train_loss = 2.924\n",
      "Epoch 200 Batch  130/215   train_loss = 3.081\n",
      "Epoch 200 Batch  140/215   train_loss = 2.761\n",
      "Epoch 200 Batch  150/215   train_loss = 2.985\n",
      "Epoch 200 Batch  160/215   train_loss = 2.846\n",
      "Epoch 200 Batch  170/215   train_loss = 2.893\n",
      "Epoch 200 Batch  180/215   train_loss = 2.811\n",
      "Epoch 200 Batch  190/215   train_loss = 2.763\n",
      "Epoch 200 Batch  200/215   train_loss = 2.736\n",
      "Epoch 200 Batch  210/215   train_loss = 2.778\n",
      "Epoch 201 Batch    5/215   train_loss = 2.900\n",
      "Epoch 201 Batch   15/215   train_loss = 2.836\n",
      "Epoch 201 Batch   25/215   train_loss = 2.907\n",
      "Epoch 201 Batch   35/215   train_loss = 2.905\n",
      "Epoch 201 Batch   45/215   train_loss = 2.857\n",
      "Epoch 201 Batch   55/215   train_loss = 2.949\n",
      "Epoch 201 Batch   65/215   train_loss = 3.064\n",
      "Epoch 201 Batch   75/215   train_loss = 2.830\n",
      "Epoch 201 Batch   85/215   train_loss = 2.870\n",
      "Epoch 201 Batch   95/215   train_loss = 2.786\n",
      "Epoch 201 Batch  105/215   train_loss = 2.856\n",
      "Epoch 201 Batch  115/215   train_loss = 2.610\n",
      "Epoch 201 Batch  125/215   train_loss = 2.821\n",
      "Epoch 201 Batch  135/215   train_loss = 2.883\n",
      "Epoch 201 Batch  145/215   train_loss = 2.954\n",
      "Epoch 201 Batch  155/215   train_loss = 2.905\n",
      "Epoch 201 Batch  165/215   train_loss = 2.854\n",
      "Epoch 201 Batch  175/215   train_loss = 3.062\n",
      "Epoch 201 Batch  185/215   train_loss = 3.004\n",
      "Epoch 201 Batch  195/215   train_loss = 2.680\n",
      "Epoch 201 Batch  205/215   train_loss = 3.155\n",
      "Epoch 202 Batch    0/215   train_loss = 3.061\n",
      "Epoch 202 Batch   10/215   train_loss = 2.961\n",
      "Epoch 202 Batch   20/215   train_loss = 2.815\n",
      "Epoch 202 Batch   30/215   train_loss = 2.628\n",
      "Epoch 202 Batch   40/215   train_loss = 2.798\n",
      "Epoch 202 Batch   50/215   train_loss = 2.838\n",
      "Epoch 202 Batch   60/215   train_loss = 2.944\n",
      "Epoch 202 Batch   70/215   train_loss = 2.999\n",
      "Epoch 202 Batch   80/215   train_loss = 2.987\n",
      "Epoch 202 Batch   90/215   train_loss = 2.786\n",
      "Epoch 202 Batch  100/215   train_loss = 2.823\n",
      "Epoch 202 Batch  110/215   train_loss = 2.921\n",
      "Epoch 202 Batch  120/215   train_loss = 2.966\n",
      "Epoch 202 Batch  130/215   train_loss = 2.989\n",
      "Epoch 202 Batch  140/215   train_loss = 3.026\n",
      "Epoch 202 Batch  150/215   train_loss = 2.893\n",
      "Epoch 202 Batch  160/215   train_loss = 2.926\n",
      "Epoch 202 Batch  170/215   train_loss = 2.664\n",
      "Epoch 202 Batch  180/215   train_loss = 3.092\n",
      "Epoch 202 Batch  190/215   train_loss = 2.864\n",
      "Epoch 202 Batch  200/215   train_loss = 2.861\n",
      "Epoch 202 Batch  210/215   train_loss = 2.657\n",
      "Epoch 203 Batch    5/215   train_loss = 3.061\n",
      "Epoch 203 Batch   15/215   train_loss = 2.720\n",
      "Epoch 203 Batch   25/215   train_loss = 2.748\n",
      "Epoch 203 Batch   35/215   train_loss = 2.961\n",
      "Epoch 203 Batch   45/215   train_loss = 3.030\n",
      "Epoch 203 Batch   55/215   train_loss = 2.816\n",
      "Epoch 203 Batch   65/215   train_loss = 2.986\n",
      "Epoch 203 Batch   75/215   train_loss = 2.949\n",
      "Epoch 203 Batch   85/215   train_loss = 2.604\n",
      "Epoch 203 Batch   95/215   train_loss = 2.810\n",
      "Epoch 203 Batch  105/215   train_loss = 2.808\n",
      "Epoch 203 Batch  115/215   train_loss = 2.683\n",
      "Epoch 203 Batch  125/215   train_loss = 2.770\n",
      "Epoch 203 Batch  135/215   train_loss = 2.817\n",
      "Epoch 203 Batch  145/215   train_loss = 2.757\n",
      "Epoch 203 Batch  155/215   train_loss = 2.842\n",
      "Epoch 203 Batch  165/215   train_loss = 2.907\n",
      "Epoch 203 Batch  175/215   train_loss = 3.054\n",
      "Epoch 203 Batch  185/215   train_loss = 3.154\n",
      "Epoch 203 Batch  195/215   train_loss = 2.784\n",
      "Epoch 203 Batch  205/215   train_loss = 2.738\n",
      "Epoch 204 Batch    0/215   train_loss = 3.068\n",
      "Epoch 204 Batch   10/215   train_loss = 2.808\n",
      "Epoch 204 Batch   20/215   train_loss = 2.907\n",
      "Epoch 204 Batch   30/215   train_loss = 2.746\n",
      "Epoch 204 Batch   40/215   train_loss = 2.866\n",
      "Epoch 204 Batch   50/215   train_loss = 3.091\n",
      "Epoch 204 Batch   60/215   train_loss = 2.863\n",
      "Epoch 204 Batch   70/215   train_loss = 2.773\n",
      "Epoch 204 Batch   80/215   train_loss = 2.783\n",
      "Epoch 204 Batch   90/215   train_loss = 3.014\n",
      "Epoch 204 Batch  100/215   train_loss = 3.127\n",
      "Epoch 204 Batch  110/215   train_loss = 2.805\n",
      "Epoch 204 Batch  120/215   train_loss = 2.890\n",
      "Epoch 204 Batch  130/215   train_loss = 3.176\n",
      "Epoch 204 Batch  140/215   train_loss = 2.864\n",
      "Epoch 204 Batch  150/215   train_loss = 2.829\n",
      "Epoch 204 Batch  160/215   train_loss = 2.788\n",
      "Epoch 204 Batch  170/215   train_loss = 2.887\n",
      "Epoch 204 Batch  180/215   train_loss = 2.986\n",
      "Epoch 204 Batch  190/215   train_loss = 2.901\n",
      "Epoch 204 Batch  200/215   train_loss = 2.838\n",
      "Epoch 204 Batch  210/215   train_loss = 2.662\n",
      "Epoch 205 Batch    5/215   train_loss = 3.027\n",
      "Epoch 205 Batch   15/215   train_loss = 3.005\n",
      "Epoch 205 Batch   25/215   train_loss = 2.614\n",
      "Epoch 205 Batch   35/215   train_loss = 3.030\n",
      "Epoch 205 Batch   45/215   train_loss = 2.863\n",
      "Epoch 205 Batch   55/215   train_loss = 2.899\n",
      "Epoch 205 Batch   65/215   train_loss = 2.699\n",
      "Epoch 205 Batch   75/215   train_loss = 2.946\n",
      "Epoch 205 Batch   85/215   train_loss = 2.796\n",
      "Epoch 205 Batch   95/215   train_loss = 2.830\n",
      "Epoch 205 Batch  105/215   train_loss = 2.807\n",
      "Epoch 205 Batch  115/215   train_loss = 2.511\n",
      "Epoch 205 Batch  125/215   train_loss = 2.776\n",
      "Epoch 205 Batch  135/215   train_loss = 2.994\n",
      "Epoch 205 Batch  145/215   train_loss = 2.692\n",
      "Epoch 205 Batch  155/215   train_loss = 2.823\n",
      "Epoch 205 Batch  165/215   train_loss = 2.803\n",
      "Epoch 205 Batch  175/215   train_loss = 2.950\n",
      "Epoch 205 Batch  185/215   train_loss = 2.868\n",
      "Epoch 205 Batch  195/215   train_loss = 2.933\n",
      "Epoch 205 Batch  205/215   train_loss = 2.846\n",
      "Epoch 206 Batch    0/215   train_loss = 2.877\n",
      "Epoch 206 Batch   10/215   train_loss = 2.848\n",
      "Epoch 206 Batch   20/215   train_loss = 2.951\n",
      "Epoch 206 Batch   30/215   train_loss = 2.624\n",
      "Epoch 206 Batch   40/215   train_loss = 2.645\n",
      "Epoch 206 Batch   50/215   train_loss = 2.678\n",
      "Epoch 206 Batch   60/215   train_loss = 2.785\n",
      "Epoch 206 Batch   70/215   train_loss = 2.848\n",
      "Epoch 206 Batch   80/215   train_loss = 2.826\n",
      "Epoch 206 Batch   90/215   train_loss = 2.848\n",
      "Epoch 206 Batch  100/215   train_loss = 2.882\n",
      "Epoch 206 Batch  110/215   train_loss = 2.863\n",
      "Epoch 206 Batch  120/215   train_loss = 2.855\n",
      "Epoch 206 Batch  130/215   train_loss = 3.123\n",
      "Epoch 206 Batch  140/215   train_loss = 2.810\n",
      "Epoch 206 Batch  150/215   train_loss = 2.933\n",
      "Epoch 206 Batch  160/215   train_loss = 2.936\n",
      "Epoch 206 Batch  170/215   train_loss = 2.844\n",
      "Epoch 206 Batch  180/215   train_loss = 2.885\n",
      "Epoch 206 Batch  190/215   train_loss = 2.859\n",
      "Epoch 206 Batch  200/215   train_loss = 2.847\n",
      "Epoch 206 Batch  210/215   train_loss = 2.774\n",
      "Epoch 207 Batch    5/215   train_loss = 3.200\n",
      "Epoch 207 Batch   15/215   train_loss = 2.923\n",
      "Epoch 207 Batch   25/215   train_loss = 2.824\n",
      "Epoch 207 Batch   35/215   train_loss = 2.952\n",
      "Epoch 207 Batch   45/215   train_loss = 2.874\n",
      "Epoch 207 Batch   55/215   train_loss = 2.802\n",
      "Epoch 207 Batch   65/215   train_loss = 2.892\n",
      "Epoch 207 Batch   75/215   train_loss = 2.659\n",
      "Epoch 207 Batch   85/215   train_loss = 2.658\n",
      "Epoch 207 Batch   95/215   train_loss = 2.952\n",
      "Epoch 207 Batch  105/215   train_loss = 2.663\n",
      "Epoch 207 Batch  115/215   train_loss = 2.675\n",
      "Epoch 207 Batch  125/215   train_loss = 2.880\n",
      "Epoch 207 Batch  135/215   train_loss = 2.723\n",
      "Epoch 207 Batch  145/215   train_loss = 2.924\n",
      "Epoch 207 Batch  155/215   train_loss = 2.927\n",
      "Epoch 207 Batch  165/215   train_loss = 2.729\n",
      "Epoch 207 Batch  175/215   train_loss = 2.924\n",
      "Epoch 207 Batch  185/215   train_loss = 3.094\n",
      "Epoch 207 Batch  195/215   train_loss = 2.838\n",
      "Epoch 207 Batch  205/215   train_loss = 2.882\n",
      "Epoch 208 Batch    0/215   train_loss = 2.881\n",
      "Epoch 208 Batch   10/215   train_loss = 2.926\n",
      "Epoch 208 Batch   20/215   train_loss = 2.934\n",
      "Epoch 208 Batch   30/215   train_loss = 2.795\n",
      "Epoch 208 Batch   40/215   train_loss = 2.847\n",
      "Epoch 208 Batch   50/215   train_loss = 2.687\n",
      "Epoch 208 Batch   60/215   train_loss = 2.997\n",
      "Epoch 208 Batch   70/215   train_loss = 2.811\n",
      "Epoch 208 Batch   80/215   train_loss = 2.839\n",
      "Epoch 208 Batch   90/215   train_loss = 2.806\n",
      "Epoch 208 Batch  100/215   train_loss = 2.902\n",
      "Epoch 208 Batch  110/215   train_loss = 2.921\n",
      "Epoch 208 Batch  120/215   train_loss = 2.894\n",
      "Epoch 208 Batch  130/215   train_loss = 2.950\n",
      "Epoch 208 Batch  140/215   train_loss = 3.052\n",
      "Epoch 208 Batch  150/215   train_loss = 2.815\n",
      "Epoch 208 Batch  160/215   train_loss = 2.736\n",
      "Epoch 208 Batch  170/215   train_loss = 2.697\n",
      "Epoch 208 Batch  180/215   train_loss = 2.876\n",
      "Epoch 208 Batch  190/215   train_loss = 2.896\n",
      "Epoch 208 Batch  200/215   train_loss = 2.685\n",
      "Epoch 208 Batch  210/215   train_loss = 2.801\n",
      "Epoch 209 Batch    5/215   train_loss = 3.198\n",
      "Epoch 209 Batch   15/215   train_loss = 2.925\n",
      "Epoch 209 Batch   25/215   train_loss = 2.879\n",
      "Epoch 209 Batch   35/215   train_loss = 2.777\n",
      "Epoch 209 Batch   45/215   train_loss = 2.781\n",
      "Epoch 209 Batch   55/215   train_loss = 2.868\n",
      "Epoch 209 Batch   65/215   train_loss = 2.918\n",
      "Epoch 209 Batch   75/215   train_loss = 2.802\n",
      "Epoch 209 Batch   85/215   train_loss = 2.765\n",
      "Epoch 209 Batch   95/215   train_loss = 2.932\n",
      "Epoch 209 Batch  105/215   train_loss = 2.732\n",
      "Epoch 209 Batch  115/215   train_loss = 2.646\n",
      "Epoch 209 Batch  125/215   train_loss = 2.755\n",
      "Epoch 209 Batch  135/215   train_loss = 2.792\n",
      "Epoch 209 Batch  145/215   train_loss = 2.894\n",
      "Epoch 209 Batch  155/215   train_loss = 2.864\n",
      "Epoch 209 Batch  165/215   train_loss = 3.115\n",
      "Epoch 209 Batch  175/215   train_loss = 2.931\n",
      "Epoch 209 Batch  185/215   train_loss = 2.685\n",
      "Epoch 209 Batch  195/215   train_loss = 2.949\n",
      "Epoch 209 Batch  205/215   train_loss = 2.900\n",
      "Epoch 210 Batch    0/215   train_loss = 3.035\n",
      "Epoch 210 Batch   10/215   train_loss = 2.915\n",
      "Epoch 210 Batch   20/215   train_loss = 2.901\n",
      "Epoch 210 Batch   30/215   train_loss = 2.728\n",
      "Epoch 210 Batch   40/215   train_loss = 2.727\n",
      "Epoch 210 Batch   50/215   train_loss = 2.698\n",
      "Epoch 210 Batch   60/215   train_loss = 2.980\n",
      "Epoch 210 Batch   70/215   train_loss = 2.566\n",
      "Epoch 210 Batch   80/215   train_loss = 2.811\n",
      "Epoch 210 Batch   90/215   train_loss = 2.989\n",
      "Epoch 210 Batch  100/215   train_loss = 2.884\n",
      "Epoch 210 Batch  110/215   train_loss = 2.863\n",
      "Epoch 210 Batch  120/215   train_loss = 2.831\n",
      "Epoch 210 Batch  130/215   train_loss = 3.109\n",
      "Epoch 210 Batch  140/215   train_loss = 2.805\n",
      "Epoch 210 Batch  150/215   train_loss = 3.038\n",
      "Epoch 210 Batch  160/215   train_loss = 2.890\n",
      "Epoch 210 Batch  170/215   train_loss = 2.730\n",
      "Epoch 210 Batch  180/215   train_loss = 2.870\n",
      "Epoch 210 Batch  190/215   train_loss = 2.917\n",
      "Epoch 210 Batch  200/215   train_loss = 2.761\n",
      "Epoch 210 Batch  210/215   train_loss = 2.606\n",
      "Epoch 211 Batch    5/215   train_loss = 2.988\n",
      "Epoch 211 Batch   15/215   train_loss = 2.763\n",
      "Epoch 211 Batch   25/215   train_loss = 2.748\n",
      "Epoch 211 Batch   35/215   train_loss = 2.958\n",
      "Epoch 211 Batch   45/215   train_loss = 2.702\n",
      "Epoch 211 Batch   55/215   train_loss = 3.201\n",
      "Epoch 211 Batch   65/215   train_loss = 2.907\n",
      "Epoch 211 Batch   75/215   train_loss = 2.817\n",
      "Epoch 211 Batch   85/215   train_loss = 2.822\n",
      "Epoch 211 Batch   95/215   train_loss = 2.856\n",
      "Epoch 211 Batch  105/215   train_loss = 2.741\n",
      "Epoch 211 Batch  115/215   train_loss = 2.765\n",
      "Epoch 211 Batch  125/215   train_loss = 2.974\n",
      "Epoch 211 Batch  135/215   train_loss = 2.908\n",
      "Epoch 211 Batch  145/215   train_loss = 2.767\n",
      "Epoch 211 Batch  155/215   train_loss = 2.814\n",
      "Epoch 211 Batch  165/215   train_loss = 2.793\n",
      "Epoch 211 Batch  175/215   train_loss = 3.128\n",
      "Epoch 211 Batch  185/215   train_loss = 2.987\n",
      "Epoch 211 Batch  195/215   train_loss = 2.796\n",
      "Epoch 211 Batch  205/215   train_loss = 2.784\n",
      "Epoch 212 Batch    0/215   train_loss = 2.976\n",
      "Epoch 212 Batch   10/215   train_loss = 3.040\n",
      "Epoch 212 Batch   20/215   train_loss = 2.883\n",
      "Epoch 212 Batch   30/215   train_loss = 2.599\n",
      "Epoch 212 Batch   40/215   train_loss = 2.840\n",
      "Epoch 212 Batch   50/215   train_loss = 2.786\n",
      "Epoch 212 Batch   60/215   train_loss = 2.873\n",
      "Epoch 212 Batch   70/215   train_loss = 2.823\n",
      "Epoch 212 Batch   80/215   train_loss = 2.774\n",
      "Epoch 212 Batch   90/215   train_loss = 2.790\n",
      "Epoch 212 Batch  100/215   train_loss = 2.931\n",
      "Epoch 212 Batch  110/215   train_loss = 2.985\n",
      "Epoch 212 Batch  120/215   train_loss = 2.867\n",
      "Epoch 212 Batch  130/215   train_loss = 3.013\n",
      "Epoch 212 Batch  140/215   train_loss = 2.930\n",
      "Epoch 212 Batch  150/215   train_loss = 2.770\n",
      "Epoch 212 Batch  160/215   train_loss = 2.902\n",
      "Epoch 212 Batch  170/215   train_loss = 2.855\n",
      "Epoch 212 Batch  180/215   train_loss = 3.026\n",
      "Epoch 212 Batch  190/215   train_loss = 2.979\n",
      "Epoch 212 Batch  200/215   train_loss = 2.770\n",
      "Epoch 212 Batch  210/215   train_loss = 2.778\n",
      "Epoch 213 Batch    5/215   train_loss = 3.007\n",
      "Epoch 213 Batch   15/215   train_loss = 2.765\n",
      "Epoch 213 Batch   25/215   train_loss = 2.816\n",
      "Epoch 213 Batch   35/215   train_loss = 2.858\n",
      "Epoch 213 Batch   45/215   train_loss = 2.860\n",
      "Epoch 213 Batch   55/215   train_loss = 2.791\n",
      "Epoch 213 Batch   65/215   train_loss = 2.828\n",
      "Epoch 213 Batch   75/215   train_loss = 2.839\n",
      "Epoch 213 Batch   85/215   train_loss = 2.745\n",
      "Epoch 213 Batch   95/215   train_loss = 2.796\n",
      "Epoch 213 Batch  105/215   train_loss = 2.666\n",
      "Epoch 213 Batch  115/215   train_loss = 2.881\n",
      "Epoch 213 Batch  125/215   train_loss = 2.791\n",
      "Epoch 213 Batch  135/215   train_loss = 2.663\n",
      "Epoch 213 Batch  145/215   train_loss = 3.075\n",
      "Epoch 213 Batch  155/215   train_loss = 2.966\n",
      "Epoch 213 Batch  165/215   train_loss = 2.783\n",
      "Epoch 213 Batch  175/215   train_loss = 3.036\n",
      "Epoch 213 Batch  185/215   train_loss = 2.907\n",
      "Epoch 213 Batch  195/215   train_loss = 2.761\n",
      "Epoch 213 Batch  205/215   train_loss = 2.758\n",
      "Epoch 214 Batch    0/215   train_loss = 2.741\n",
      "Epoch 214 Batch   10/215   train_loss = 3.125\n",
      "Epoch 214 Batch   20/215   train_loss = 2.913\n",
      "Epoch 214 Batch   30/215   train_loss = 2.575\n",
      "Epoch 214 Batch   40/215   train_loss = 3.008\n",
      "Epoch 214 Batch   50/215   train_loss = 2.766\n",
      "Epoch 214 Batch   60/215   train_loss = 3.223\n",
      "Epoch 214 Batch   70/215   train_loss = 2.793\n",
      "Epoch 214 Batch   80/215   train_loss = 2.978\n",
      "Epoch 214 Batch   90/215   train_loss = 3.036\n",
      "Epoch 214 Batch  100/215   train_loss = 2.934\n",
      "Epoch 214 Batch  110/215   train_loss = 3.013\n",
      "Epoch 214 Batch  120/215   train_loss = 2.879\n",
      "Epoch 214 Batch  130/215   train_loss = 3.024\n",
      "Epoch 214 Batch  140/215   train_loss = 2.727\n",
      "Epoch 214 Batch  150/215   train_loss = 2.835\n",
      "Epoch 214 Batch  160/215   train_loss = 2.889\n",
      "Epoch 214 Batch  170/215   train_loss = 2.816\n",
      "Epoch 214 Batch  180/215   train_loss = 2.846\n",
      "Epoch 214 Batch  190/215   train_loss = 2.858\n",
      "Epoch 214 Batch  200/215   train_loss = 2.965\n",
      "Epoch 214 Batch  210/215   train_loss = 2.687\n",
      "Epoch 215 Batch    5/215   train_loss = 2.961\n",
      "Epoch 215 Batch   15/215   train_loss = 2.881\n",
      "Epoch 215 Batch   25/215   train_loss = 2.839\n",
      "Epoch 215 Batch   35/215   train_loss = 3.012\n",
      "Epoch 215 Batch   45/215   train_loss = 2.985\n",
      "Epoch 215 Batch   55/215   train_loss = 2.750\n",
      "Epoch 215 Batch   65/215   train_loss = 3.052\n",
      "Epoch 215 Batch   75/215   train_loss = 2.826\n",
      "Epoch 215 Batch   85/215   train_loss = 2.795\n",
      "Epoch 215 Batch   95/215   train_loss = 2.837\n",
      "Epoch 215 Batch  105/215   train_loss = 2.560\n",
      "Epoch 215 Batch  115/215   train_loss = 2.684\n",
      "Epoch 215 Batch  125/215   train_loss = 2.686\n",
      "Epoch 215 Batch  135/215   train_loss = 2.921\n",
      "Epoch 215 Batch  145/215   train_loss = 2.964\n",
      "Epoch 215 Batch  155/215   train_loss = 2.723\n",
      "Epoch 215 Batch  165/215   train_loss = 2.777\n",
      "Epoch 215 Batch  175/215   train_loss = 2.866\n",
      "Epoch 215 Batch  185/215   train_loss = 3.081\n",
      "Epoch 215 Batch  195/215   train_loss = 2.942\n",
      "Epoch 215 Batch  205/215   train_loss = 2.668\n",
      "Epoch 216 Batch    0/215   train_loss = 2.865\n",
      "Epoch 216 Batch   10/215   train_loss = 2.971\n",
      "Epoch 216 Batch   20/215   train_loss = 2.929\n",
      "Epoch 216 Batch   30/215   train_loss = 2.775\n",
      "Epoch 216 Batch   40/215   train_loss = 2.994\n",
      "Epoch 216 Batch   50/215   train_loss = 2.771\n",
      "Epoch 216 Batch   60/215   train_loss = 3.006\n",
      "Epoch 216 Batch   70/215   train_loss = 2.786\n",
      "Epoch 216 Batch   80/215   train_loss = 2.892\n",
      "Epoch 216 Batch   90/215   train_loss = 2.871\n",
      "Epoch 216 Batch  100/215   train_loss = 2.834\n",
      "Epoch 216 Batch  110/215   train_loss = 2.780\n",
      "Epoch 216 Batch  120/215   train_loss = 3.145\n",
      "Epoch 216 Batch  130/215   train_loss = 3.073\n",
      "Epoch 216 Batch  140/215   train_loss = 2.674\n",
      "Epoch 216 Batch  150/215   train_loss = 2.745\n",
      "Epoch 216 Batch  160/215   train_loss = 2.955\n",
      "Epoch 216 Batch  170/215   train_loss = 2.796\n",
      "Epoch 216 Batch  180/215   train_loss = 2.817\n",
      "Epoch 216 Batch  190/215   train_loss = 2.853\n",
      "Epoch 216 Batch  200/215   train_loss = 2.926\n",
      "Epoch 216 Batch  210/215   train_loss = 2.655\n",
      "Epoch 217 Batch    5/215   train_loss = 2.862\n",
      "Epoch 217 Batch   15/215   train_loss = 2.738\n",
      "Epoch 217 Batch   25/215   train_loss = 2.804\n",
      "Epoch 217 Batch   35/215   train_loss = 2.958\n",
      "Epoch 217 Batch   45/215   train_loss = 3.040\n",
      "Epoch 217 Batch   55/215   train_loss = 2.703\n",
      "Epoch 217 Batch   65/215   train_loss = 2.789\n",
      "Epoch 217 Batch   75/215   train_loss = 2.834\n",
      "Epoch 217 Batch   85/215   train_loss = 2.621\n",
      "Epoch 217 Batch   95/215   train_loss = 2.641\n",
      "Epoch 217 Batch  105/215   train_loss = 2.782\n",
      "Epoch 217 Batch  115/215   train_loss = 2.746\n",
      "Epoch 217 Batch  125/215   train_loss = 2.782\n",
      "Epoch 217 Batch  135/215   train_loss = 2.955\n",
      "Epoch 217 Batch  145/215   train_loss = 2.830\n",
      "Epoch 217 Batch  155/215   train_loss = 2.969\n",
      "Epoch 217 Batch  165/215   train_loss = 2.749\n",
      "Epoch 217 Batch  175/215   train_loss = 3.022\n",
      "Epoch 217 Batch  185/215   train_loss = 2.900\n",
      "Epoch 217 Batch  195/215   train_loss = 2.882\n",
      "Epoch 217 Batch  205/215   train_loss = 2.818\n",
      "Epoch 218 Batch    0/215   train_loss = 2.918\n",
      "Epoch 218 Batch   10/215   train_loss = 3.008\n",
      "Epoch 218 Batch   20/215   train_loss = 2.834\n",
      "Epoch 218 Batch   30/215   train_loss = 2.771\n",
      "Epoch 218 Batch   40/215   train_loss = 2.803\n",
      "Epoch 218 Batch   50/215   train_loss = 2.752\n",
      "Epoch 218 Batch   60/215   train_loss = 2.813\n",
      "Epoch 218 Batch   70/215   train_loss = 2.744\n",
      "Epoch 218 Batch   80/215   train_loss = 2.879\n",
      "Epoch 218 Batch   90/215   train_loss = 2.881\n",
      "Epoch 218 Batch  100/215   train_loss = 2.830\n",
      "Epoch 218 Batch  110/215   train_loss = 2.951\n",
      "Epoch 218 Batch  120/215   train_loss = 2.754\n",
      "Epoch 218 Batch  130/215   train_loss = 3.008\n",
      "Epoch 218 Batch  140/215   train_loss = 2.969\n",
      "Epoch 218 Batch  150/215   train_loss = 2.805\n",
      "Epoch 218 Batch  160/215   train_loss = 2.885\n",
      "Epoch 218 Batch  170/215   train_loss = 2.774\n",
      "Epoch 218 Batch  180/215   train_loss = 2.867\n",
      "Epoch 218 Batch  190/215   train_loss = 2.837\n",
      "Epoch 218 Batch  200/215   train_loss = 2.787\n",
      "Epoch 218 Batch  210/215   train_loss = 2.828\n",
      "Epoch 219 Batch    5/215   train_loss = 2.871\n",
      "Epoch 219 Batch   15/215   train_loss = 2.744\n",
      "Epoch 219 Batch   25/215   train_loss = 2.794\n",
      "Epoch 219 Batch   35/215   train_loss = 2.997\n",
      "Epoch 219 Batch   45/215   train_loss = 3.078\n",
      "Epoch 219 Batch   55/215   train_loss = 2.835\n",
      "Epoch 219 Batch   65/215   train_loss = 2.863\n",
      "Epoch 219 Batch   75/215   train_loss = 2.832\n",
      "Epoch 219 Batch   85/215   train_loss = 2.735\n",
      "Epoch 219 Batch   95/215   train_loss = 2.806\n",
      "Epoch 219 Batch  105/215   train_loss = 2.910\n",
      "Epoch 219 Batch  115/215   train_loss = 2.696\n",
      "Epoch 219 Batch  125/215   train_loss = 2.582\n",
      "Epoch 219 Batch  135/215   train_loss = 2.972\n",
      "Epoch 219 Batch  145/215   train_loss = 2.957\n",
      "Epoch 219 Batch  155/215   train_loss = 2.960\n",
      "Epoch 219 Batch  165/215   train_loss = 2.715\n",
      "Epoch 219 Batch  175/215   train_loss = 3.052\n",
      "Epoch 219 Batch  185/215   train_loss = 2.878\n",
      "Epoch 219 Batch  195/215   train_loss = 2.796\n",
      "Epoch 219 Batch  205/215   train_loss = 2.827\n",
      "Epoch 220 Batch    0/215   train_loss = 2.844\n",
      "Epoch 220 Batch   10/215   train_loss = 3.295\n",
      "Epoch 220 Batch   20/215   train_loss = 2.729\n",
      "Epoch 220 Batch   30/215   train_loss = 2.740\n",
      "Epoch 220 Batch   40/215   train_loss = 2.784\n",
      "Epoch 220 Batch   50/215   train_loss = 2.754\n",
      "Epoch 220 Batch   60/215   train_loss = 2.866\n",
      "Epoch 220 Batch   70/215   train_loss = 2.666\n",
      "Epoch 220 Batch   80/215   train_loss = 2.845\n",
      "Epoch 220 Batch   90/215   train_loss = 3.072\n",
      "Epoch 220 Batch  100/215   train_loss = 2.771\n",
      "Epoch 220 Batch  110/215   train_loss = 2.781\n",
      "Epoch 220 Batch  120/215   train_loss = 2.778\n",
      "Epoch 220 Batch  130/215   train_loss = 3.058\n",
      "Epoch 220 Batch  140/215   train_loss = 2.793\n",
      "Epoch 220 Batch  150/215   train_loss = 2.871\n",
      "Epoch 220 Batch  160/215   train_loss = 2.905\n",
      "Epoch 220 Batch  170/215   train_loss = 2.733\n",
      "Epoch 220 Batch  180/215   train_loss = 2.798\n",
      "Epoch 220 Batch  190/215   train_loss = 2.721\n",
      "Epoch 220 Batch  200/215   train_loss = 2.976\n",
      "Epoch 220 Batch  210/215   train_loss = 2.622\n",
      "Epoch 221 Batch    5/215   train_loss = 2.912\n",
      "Epoch 221 Batch   15/215   train_loss = 2.726\n",
      "Epoch 221 Batch   25/215   train_loss = 2.889\n",
      "Epoch 221 Batch   35/215   train_loss = 2.957\n",
      "Epoch 221 Batch   45/215   train_loss = 3.019\n",
      "Epoch 221 Batch   55/215   train_loss = 2.804\n",
      "Epoch 221 Batch   65/215   train_loss = 2.882\n",
      "Epoch 221 Batch   75/215   train_loss = 2.770\n",
      "Epoch 221 Batch   85/215   train_loss = 2.555\n",
      "Epoch 221 Batch   95/215   train_loss = 2.824\n",
      "Epoch 221 Batch  105/215   train_loss = 2.629\n",
      "Epoch 221 Batch  115/215   train_loss = 2.762\n",
      "Epoch 221 Batch  125/215   train_loss = 2.839\n",
      "Epoch 221 Batch  135/215   train_loss = 2.822\n",
      "Epoch 221 Batch  145/215   train_loss = 2.970\n",
      "Epoch 221 Batch  155/215   train_loss = 2.895\n",
      "Epoch 221 Batch  165/215   train_loss = 2.853\n",
      "Epoch 221 Batch  175/215   train_loss = 3.141\n",
      "Epoch 221 Batch  185/215   train_loss = 2.862\n",
      "Epoch 221 Batch  195/215   train_loss = 2.738\n",
      "Epoch 221 Batch  205/215   train_loss = 2.741\n",
      "Epoch 222 Batch    0/215   train_loss = 2.909\n",
      "Epoch 222 Batch   10/215   train_loss = 2.963\n",
      "Epoch 222 Batch   20/215   train_loss = 2.968\n",
      "Epoch 222 Batch   30/215   train_loss = 2.691\n",
      "Epoch 222 Batch   40/215   train_loss = 2.772\n",
      "Epoch 222 Batch   50/215   train_loss = 2.631\n",
      "Epoch 222 Batch   60/215   train_loss = 3.119\n",
      "Epoch 222 Batch   70/215   train_loss = 2.846\n",
      "Epoch 222 Batch   80/215   train_loss = 2.887\n",
      "Epoch 222 Batch   90/215   train_loss = 2.890\n",
      "Epoch 222 Batch  100/215   train_loss = 2.841\n",
      "Epoch 222 Batch  110/215   train_loss = 2.703\n",
      "Epoch 222 Batch  120/215   train_loss = 2.941\n",
      "Epoch 222 Batch  130/215   train_loss = 3.016\n",
      "Epoch 222 Batch  140/215   train_loss = 2.789\n",
      "Epoch 222 Batch  150/215   train_loss = 2.881\n",
      "Epoch 222 Batch  160/215   train_loss = 2.860\n",
      "Epoch 222 Batch  170/215   train_loss = 2.767\n",
      "Epoch 222 Batch  180/215   train_loss = 3.017\n",
      "Epoch 222 Batch  190/215   train_loss = 2.930\n",
      "Epoch 222 Batch  200/215   train_loss = 2.775\n",
      "Epoch 222 Batch  210/215   train_loss = 2.683\n",
      "Epoch 223 Batch    5/215   train_loss = 2.912\n",
      "Epoch 223 Batch   15/215   train_loss = 2.802\n",
      "Epoch 223 Batch   25/215   train_loss = 2.767\n",
      "Epoch 223 Batch   35/215   train_loss = 2.948\n",
      "Epoch 223 Batch   45/215   train_loss = 2.693\n",
      "Epoch 223 Batch   55/215   train_loss = 2.837\n",
      "Epoch 223 Batch   65/215   train_loss = 2.905\n",
      "Epoch 223 Batch   75/215   train_loss = 2.766\n",
      "Epoch 223 Batch   85/215   train_loss = 2.674\n",
      "Epoch 223 Batch   95/215   train_loss = 2.701\n",
      "Epoch 223 Batch  105/215   train_loss = 2.700\n",
      "Epoch 223 Batch  115/215   train_loss = 2.635\n",
      "Epoch 223 Batch  125/215   train_loss = 2.618\n",
      "Epoch 223 Batch  135/215   train_loss = 2.564\n",
      "Epoch 223 Batch  145/215   train_loss = 3.002\n",
      "Epoch 223 Batch  155/215   train_loss = 2.921\n",
      "Epoch 223 Batch  165/215   train_loss = 2.877\n",
      "Epoch 223 Batch  175/215   train_loss = 3.138\n",
      "Epoch 223 Batch  185/215   train_loss = 2.812\n",
      "Epoch 223 Batch  195/215   train_loss = 2.772\n",
      "Epoch 223 Batch  205/215   train_loss = 2.979\n",
      "Epoch 224 Batch    0/215   train_loss = 2.954\n",
      "Epoch 224 Batch   10/215   train_loss = 2.961\n",
      "Epoch 224 Batch   20/215   train_loss = 2.900\n",
      "Epoch 224 Batch   30/215   train_loss = 2.659\n",
      "Epoch 224 Batch   40/215   train_loss = 2.799\n",
      "Epoch 224 Batch   50/215   train_loss = 2.676\n",
      "Epoch 224 Batch   60/215   train_loss = 3.128\n",
      "Epoch 224 Batch   70/215   train_loss = 2.782\n",
      "Epoch 224 Batch   80/215   train_loss = 2.939\n",
      "Epoch 224 Batch   90/215   train_loss = 2.766\n",
      "Epoch 224 Batch  100/215   train_loss = 2.728\n",
      "Epoch 224 Batch  110/215   train_loss = 2.785\n",
      "Epoch 224 Batch  120/215   train_loss = 2.748\n",
      "Epoch 224 Batch  130/215   train_loss = 3.005\n",
      "Epoch 224 Batch  140/215   train_loss = 2.781\n",
      "Epoch 224 Batch  150/215   train_loss = 2.814\n",
      "Epoch 224 Batch  160/215   train_loss = 2.975\n",
      "Epoch 224 Batch  170/215   train_loss = 2.740\n",
      "Epoch 224 Batch  180/215   train_loss = 2.924\n",
      "Epoch 224 Batch  190/215   train_loss = 2.771\n",
      "Epoch 224 Batch  200/215   train_loss = 2.804\n",
      "Epoch 224 Batch  210/215   train_loss = 2.640\n",
      "Epoch 225 Batch    5/215   train_loss = 2.999\n",
      "Epoch 225 Batch   15/215   train_loss = 2.900\n",
      "Epoch 225 Batch   25/215   train_loss = 2.640\n",
      "Epoch 225 Batch   35/215   train_loss = 2.878\n",
      "Epoch 225 Batch   45/215   train_loss = 2.878\n",
      "Epoch 225 Batch   55/215   train_loss = 2.810\n",
      "Epoch 225 Batch   65/215   train_loss = 2.706\n",
      "Epoch 225 Batch   75/215   train_loss = 2.742\n",
      "Epoch 225 Batch   85/215   train_loss = 2.648\n",
      "Epoch 225 Batch   95/215   train_loss = 2.757\n",
      "Epoch 225 Batch  105/215   train_loss = 2.679\n",
      "Epoch 225 Batch  115/215   train_loss = 2.767\n",
      "Epoch 225 Batch  125/215   train_loss = 2.854\n",
      "Epoch 225 Batch  135/215   train_loss = 2.868\n",
      "Epoch 225 Batch  145/215   train_loss = 2.828\n",
      "Epoch 225 Batch  155/215   train_loss = 2.907\n",
      "Epoch 225 Batch  165/215   train_loss = 2.790\n",
      "Epoch 225 Batch  175/215   train_loss = 2.842\n",
      "Epoch 225 Batch  185/215   train_loss = 2.809\n",
      "Epoch 225 Batch  195/215   train_loss = 2.851\n",
      "Epoch 225 Batch  205/215   train_loss = 2.847\n",
      "Epoch 226 Batch    0/215   train_loss = 2.935\n",
      "Epoch 226 Batch   10/215   train_loss = 2.963\n",
      "Epoch 226 Batch   20/215   train_loss = 2.708\n",
      "Epoch 226 Batch   30/215   train_loss = 2.663\n",
      "Epoch 226 Batch   40/215   train_loss = 2.846\n",
      "Epoch 226 Batch   50/215   train_loss = 2.889\n",
      "Epoch 226 Batch   60/215   train_loss = 2.931\n",
      "Epoch 226 Batch   70/215   train_loss = 2.667\n",
      "Epoch 226 Batch   80/215   train_loss = 2.747\n",
      "Epoch 226 Batch   90/215   train_loss = 3.005\n",
      "Epoch 226 Batch  100/215   train_loss = 2.904\n",
      "Epoch 226 Batch  110/215   train_loss = 2.845\n",
      "Epoch 226 Batch  120/215   train_loss = 2.913\n",
      "Epoch 226 Batch  130/215   train_loss = 2.984\n",
      "Epoch 226 Batch  140/215   train_loss = 3.172\n",
      "Epoch 226 Batch  150/215   train_loss = 2.952\n",
      "Epoch 226 Batch  160/215   train_loss = 2.962\n",
      "Epoch 226 Batch  170/215   train_loss = 2.645\n",
      "Epoch 226 Batch  180/215   train_loss = 2.976\n",
      "Epoch 226 Batch  190/215   train_loss = 2.978\n",
      "Epoch 226 Batch  200/215   train_loss = 2.681\n",
      "Epoch 226 Batch  210/215   train_loss = 2.654\n",
      "Epoch 227 Batch    5/215   train_loss = 2.946\n",
      "Epoch 227 Batch   15/215   train_loss = 2.908\n",
      "Epoch 227 Batch   25/215   train_loss = 2.838\n",
      "Epoch 227 Batch   35/215   train_loss = 2.967\n",
      "Epoch 227 Batch   45/215   train_loss = 2.816\n",
      "Epoch 227 Batch   55/215   train_loss = 2.754\n",
      "Epoch 227 Batch   65/215   train_loss = 2.849\n",
      "Epoch 227 Batch   75/215   train_loss = 2.845\n",
      "Epoch 227 Batch   85/215   train_loss = 2.776\n",
      "Epoch 227 Batch   95/215   train_loss = 2.748\n",
      "Epoch 227 Batch  105/215   train_loss = 2.809\n",
      "Epoch 227 Batch  115/215   train_loss = 2.747\n",
      "Epoch 227 Batch  125/215   train_loss = 2.642\n",
      "Epoch 227 Batch  135/215   train_loss = 2.784\n",
      "Epoch 227 Batch  145/215   train_loss = 2.741\n",
      "Epoch 227 Batch  155/215   train_loss = 2.698\n",
      "Epoch 227 Batch  165/215   train_loss = 2.924\n",
      "Epoch 227 Batch  175/215   train_loss = 2.983\n",
      "Epoch 227 Batch  185/215   train_loss = 2.854\n",
      "Epoch 227 Batch  195/215   train_loss = 2.672\n",
      "Epoch 227 Batch  205/215   train_loss = 2.722\n",
      "Epoch 228 Batch    0/215   train_loss = 3.011\n",
      "Epoch 228 Batch   10/215   train_loss = 2.754\n",
      "Epoch 228 Batch   20/215   train_loss = 2.812\n",
      "Epoch 228 Batch   30/215   train_loss = 2.658\n",
      "Epoch 228 Batch   40/215   train_loss = 2.982\n",
      "Epoch 228 Batch   50/215   train_loss = 2.746\n",
      "Epoch 228 Batch   60/215   train_loss = 2.986\n",
      "Epoch 228 Batch   70/215   train_loss = 2.744\n",
      "Epoch 228 Batch   80/215   train_loss = 2.815\n",
      "Epoch 228 Batch   90/215   train_loss = 3.087\n",
      "Epoch 228 Batch  100/215   train_loss = 2.955\n",
      "Epoch 228 Batch  110/215   train_loss = 2.855\n",
      "Epoch 228 Batch  120/215   train_loss = 2.796\n",
      "Epoch 228 Batch  130/215   train_loss = 3.108\n",
      "Epoch 228 Batch  140/215   train_loss = 2.902\n",
      "Epoch 228 Batch  150/215   train_loss = 2.721\n",
      "Epoch 228 Batch  160/215   train_loss = 2.817\n",
      "Epoch 228 Batch  170/215   train_loss = 2.806\n",
      "Epoch 228 Batch  180/215   train_loss = 2.795\n",
      "Epoch 228 Batch  190/215   train_loss = 2.926\n",
      "Epoch 228 Batch  200/215   train_loss = 2.655\n",
      "Epoch 228 Batch  210/215   train_loss = 2.575\n",
      "Epoch 229 Batch    5/215   train_loss = 3.240\n",
      "Epoch 229 Batch   15/215   train_loss = 2.489\n",
      "Epoch 229 Batch   25/215   train_loss = 2.775\n",
      "Epoch 229 Batch   35/215   train_loss = 2.882\n",
      "Epoch 229 Batch   45/215   train_loss = 2.743\n",
      "Epoch 229 Batch   55/215   train_loss = 2.712\n",
      "Epoch 229 Batch   65/215   train_loss = 2.875\n",
      "Epoch 229 Batch   75/215   train_loss = 2.734\n",
      "Epoch 229 Batch   85/215   train_loss = 2.919\n",
      "Epoch 229 Batch   95/215   train_loss = 2.833\n",
      "Epoch 229 Batch  105/215   train_loss = 2.585\n",
      "Epoch 229 Batch  115/215   train_loss = 2.606\n",
      "Epoch 229 Batch  125/215   train_loss = 2.637\n",
      "Epoch 229 Batch  135/215   train_loss = 2.886\n",
      "Epoch 229 Batch  145/215   train_loss = 2.794\n",
      "Epoch 229 Batch  155/215   train_loss = 2.938\n",
      "Epoch 229 Batch  165/215   train_loss = 2.777\n",
      "Epoch 229 Batch  175/215   train_loss = 3.006\n",
      "Epoch 229 Batch  185/215   train_loss = 2.949\n",
      "Epoch 229 Batch  195/215   train_loss = 2.606\n",
      "Epoch 229 Batch  205/215   train_loss = 2.740\n",
      "Epoch 230 Batch    0/215   train_loss = 2.958\n",
      "Epoch 230 Batch   10/215   train_loss = 2.898\n",
      "Epoch 230 Batch   20/215   train_loss = 2.737\n",
      "Epoch 230 Batch   30/215   train_loss = 2.684\n",
      "Epoch 230 Batch   40/215   train_loss = 2.770\n",
      "Epoch 230 Batch   50/215   train_loss = 2.681\n",
      "Epoch 230 Batch   60/215   train_loss = 3.165\n",
      "Epoch 230 Batch   70/215   train_loss = 2.646\n",
      "Epoch 230 Batch   80/215   train_loss = 2.905\n",
      "Epoch 230 Batch   90/215   train_loss = 2.864\n",
      "Epoch 230 Batch  100/215   train_loss = 2.849\n",
      "Epoch 230 Batch  110/215   train_loss = 2.722\n",
      "Epoch 230 Batch  120/215   train_loss = 2.796\n",
      "Epoch 230 Batch  130/215   train_loss = 2.860\n",
      "Epoch 230 Batch  140/215   train_loss = 2.820\n",
      "Epoch 230 Batch  150/215   train_loss = 2.804\n",
      "Epoch 230 Batch  160/215   train_loss = 2.853\n",
      "Epoch 230 Batch  170/215   train_loss = 2.827\n",
      "Epoch 230 Batch  180/215   train_loss = 2.854\n",
      "Epoch 230 Batch  190/215   train_loss = 3.024\n",
      "Epoch 230 Batch  200/215   train_loss = 2.892\n",
      "Epoch 230 Batch  210/215   train_loss = 2.730\n",
      "Epoch 231 Batch    5/215   train_loss = 3.061\n",
      "Epoch 231 Batch   15/215   train_loss = 2.897\n",
      "Epoch 231 Batch   25/215   train_loss = 2.808\n",
      "Epoch 231 Batch   35/215   train_loss = 2.916\n",
      "Epoch 231 Batch   45/215   train_loss = 2.762\n",
      "Epoch 231 Batch   55/215   train_loss = 2.897\n",
      "Epoch 231 Batch   65/215   train_loss = 2.667\n",
      "Epoch 231 Batch   75/215   train_loss = 2.699\n",
      "Epoch 231 Batch   85/215   train_loss = 2.650\n",
      "Epoch 231 Batch   95/215   train_loss = 2.980\n",
      "Epoch 231 Batch  105/215   train_loss = 2.799\n",
      "Epoch 231 Batch  115/215   train_loss = 2.525\n",
      "Epoch 231 Batch  125/215   train_loss = 2.737\n",
      "Epoch 231 Batch  135/215   train_loss = 2.830\n",
      "Epoch 231 Batch  145/215   train_loss = 2.925\n",
      "Epoch 231 Batch  155/215   train_loss = 2.923\n",
      "Epoch 231 Batch  165/215   train_loss = 2.811\n",
      "Epoch 231 Batch  175/215   train_loss = 3.089\n",
      "Epoch 231 Batch  185/215   train_loss = 2.984\n",
      "Epoch 231 Batch  195/215   train_loss = 2.793\n",
      "Epoch 231 Batch  205/215   train_loss = 2.796\n",
      "Epoch 232 Batch    0/215   train_loss = 2.947\n",
      "Epoch 232 Batch   10/215   train_loss = 2.915\n",
      "Epoch 232 Batch   20/215   train_loss = 2.797\n",
      "Epoch 232 Batch   30/215   train_loss = 2.650\n",
      "Epoch 232 Batch   40/215   train_loss = 2.784\n",
      "Epoch 232 Batch   50/215   train_loss = 2.772\n",
      "Epoch 232 Batch   60/215   train_loss = 3.029\n",
      "Epoch 232 Batch   70/215   train_loss = 2.671\n",
      "Epoch 232 Batch   80/215   train_loss = 2.645\n",
      "Epoch 232 Batch   90/215   train_loss = 3.093\n",
      "Epoch 232 Batch  100/215   train_loss = 2.986\n",
      "Epoch 232 Batch  110/215   train_loss = 2.825\n",
      "Epoch 232 Batch  120/215   train_loss = 2.932\n",
      "Epoch 232 Batch  130/215   train_loss = 3.112\n",
      "Epoch 232 Batch  140/215   train_loss = 2.778\n",
      "Epoch 232 Batch  150/215   train_loss = 2.797\n",
      "Epoch 232 Batch  160/215   train_loss = 3.050\n",
      "Epoch 232 Batch  170/215   train_loss = 2.790\n",
      "Epoch 232 Batch  180/215   train_loss = 2.964\n",
      "Epoch 232 Batch  190/215   train_loss = 2.732\n",
      "Epoch 232 Batch  200/215   train_loss = 2.744\n",
      "Epoch 232 Batch  210/215   train_loss = 2.834\n",
      "Epoch 233 Batch    5/215   train_loss = 2.867\n",
      "Epoch 233 Batch   15/215   train_loss = 2.881\n",
      "Epoch 233 Batch   25/215   train_loss = 2.586\n",
      "Epoch 233 Batch   35/215   train_loss = 2.750\n",
      "Epoch 233 Batch   45/215   train_loss = 2.881\n",
      "Epoch 233 Batch   55/215   train_loss = 2.759\n",
      "Epoch 233 Batch   65/215   train_loss = 2.890\n",
      "Epoch 233 Batch   75/215   train_loss = 2.923\n",
      "Epoch 233 Batch   85/215   train_loss = 2.721\n",
      "Epoch 233 Batch   95/215   train_loss = 2.900\n",
      "Epoch 233 Batch  105/215   train_loss = 2.926\n",
      "Epoch 233 Batch  115/215   train_loss = 2.699\n",
      "Epoch 233 Batch  125/215   train_loss = 2.836\n",
      "Epoch 233 Batch  135/215   train_loss = 2.845\n",
      "Epoch 233 Batch  145/215   train_loss = 2.811\n",
      "Epoch 233 Batch  155/215   train_loss = 2.939\n",
      "Epoch 233 Batch  165/215   train_loss = 2.945\n",
      "Epoch 233 Batch  175/215   train_loss = 3.033\n",
      "Epoch 233 Batch  185/215   train_loss = 2.794\n",
      "Epoch 233 Batch  195/215   train_loss = 2.847\n",
      "Epoch 233 Batch  205/215   train_loss = 2.909\n",
      "Epoch 234 Batch    0/215   train_loss = 2.842\n",
      "Epoch 234 Batch   10/215   train_loss = 2.963\n",
      "Epoch 234 Batch   20/215   train_loss = 2.875\n",
      "Epoch 234 Batch   30/215   train_loss = 2.659\n",
      "Epoch 234 Batch   40/215   train_loss = 2.663\n",
      "Epoch 234 Batch   50/215   train_loss = 2.939\n",
      "Epoch 234 Batch   60/215   train_loss = 2.774\n",
      "Epoch 234 Batch   70/215   train_loss = 2.619\n",
      "Epoch 234 Batch   80/215   train_loss = 2.951\n",
      "Epoch 234 Batch   90/215   train_loss = 3.068\n",
      "Epoch 234 Batch  100/215   train_loss = 2.845\n",
      "Epoch 234 Batch  110/215   train_loss = 2.671\n",
      "Epoch 234 Batch  120/215   train_loss = 2.857\n",
      "Epoch 234 Batch  130/215   train_loss = 2.965\n",
      "Epoch 234 Batch  140/215   train_loss = 2.772\n",
      "Epoch 234 Batch  150/215   train_loss = 2.816\n",
      "Epoch 234 Batch  160/215   train_loss = 2.807\n",
      "Epoch 234 Batch  170/215   train_loss = 2.805\n",
      "Epoch 234 Batch  180/215   train_loss = 2.877\n",
      "Epoch 234 Batch  190/215   train_loss = 2.861\n",
      "Epoch 234 Batch  200/215   train_loss = 2.955\n",
      "Epoch 234 Batch  210/215   train_loss = 2.822\n",
      "Epoch 235 Batch    5/215   train_loss = 2.980\n",
      "Epoch 235 Batch   15/215   train_loss = 2.761\n",
      "Epoch 235 Batch   25/215   train_loss = 2.634\n",
      "Epoch 235 Batch   35/215   train_loss = 2.829\n",
      "Epoch 235 Batch   45/215   train_loss = 2.778\n",
      "Epoch 235 Batch   55/215   train_loss = 2.836\n",
      "Epoch 235 Batch   65/215   train_loss = 3.049\n",
      "Epoch 235 Batch   75/215   train_loss = 2.707\n",
      "Epoch 235 Batch   85/215   train_loss = 2.862\n",
      "Epoch 235 Batch   95/215   train_loss = 2.715\n",
      "Epoch 235 Batch  105/215   train_loss = 2.702\n",
      "Epoch 235 Batch  115/215   train_loss = 2.625\n",
      "Epoch 235 Batch  125/215   train_loss = 2.702\n",
      "Epoch 235 Batch  135/215   train_loss = 2.688\n",
      "Epoch 235 Batch  145/215   train_loss = 2.932\n",
      "Epoch 235 Batch  155/215   train_loss = 2.864\n",
      "Epoch 235 Batch  165/215   train_loss = 2.626\n",
      "Epoch 235 Batch  175/215   train_loss = 2.995\n",
      "Epoch 235 Batch  185/215   train_loss = 3.034\n",
      "Epoch 235 Batch  195/215   train_loss = 2.751\n",
      "Epoch 235 Batch  205/215   train_loss = 2.826\n",
      "Epoch 236 Batch    0/215   train_loss = 2.772\n",
      "Epoch 236 Batch   10/215   train_loss = 2.947\n",
      "Epoch 236 Batch   20/215   train_loss = 2.972\n",
      "Epoch 236 Batch   30/215   train_loss = 2.670\n",
      "Epoch 236 Batch   40/215   train_loss = 2.989\n",
      "Epoch 236 Batch   50/215   train_loss = 2.621\n",
      "Epoch 236 Batch   60/215   train_loss = 2.958\n",
      "Epoch 236 Batch   70/215   train_loss = 2.669\n",
      "Epoch 236 Batch   80/215   train_loss = 2.938\n",
      "Epoch 236 Batch   90/215   train_loss = 2.878\n",
      "Epoch 236 Batch  100/215   train_loss = 2.982\n",
      "Epoch 236 Batch  110/215   train_loss = 2.791\n",
      "Epoch 236 Batch  120/215   train_loss = 3.139\n",
      "Epoch 236 Batch  130/215   train_loss = 3.057\n",
      "Epoch 236 Batch  140/215   train_loss = 2.863\n",
      "Epoch 236 Batch  150/215   train_loss = 2.855\n",
      "Epoch 236 Batch  160/215   train_loss = 2.914\n",
      "Epoch 236 Batch  170/215   train_loss = 2.626\n",
      "Epoch 236 Batch  180/215   train_loss = 2.905\n",
      "Epoch 236 Batch  190/215   train_loss = 2.772\n",
      "Epoch 236 Batch  200/215   train_loss = 2.717\n",
      "Epoch 236 Batch  210/215   train_loss = 2.653\n",
      "Epoch 237 Batch    5/215   train_loss = 2.931\n",
      "Epoch 237 Batch   15/215   train_loss = 2.992\n",
      "Epoch 237 Batch   25/215   train_loss = 2.728\n",
      "Epoch 237 Batch   35/215   train_loss = 2.962\n",
      "Epoch 237 Batch   45/215   train_loss = 2.878\n",
      "Epoch 237 Batch   55/215   train_loss = 2.654\n",
      "Epoch 237 Batch   65/215   train_loss = 2.665\n",
      "Epoch 237 Batch   75/215   train_loss = 2.842\n",
      "Epoch 237 Batch   85/215   train_loss = 2.532\n",
      "Epoch 237 Batch   95/215   train_loss = 2.940\n",
      "Epoch 237 Batch  105/215   train_loss = 2.690\n",
      "Epoch 237 Batch  115/215   train_loss = 2.610\n",
      "Epoch 237 Batch  125/215   train_loss = 2.684\n",
      "Epoch 237 Batch  135/215   train_loss = 2.704\n",
      "Epoch 237 Batch  145/215   train_loss = 3.102\n",
      "Epoch 237 Batch  155/215   train_loss = 2.809\n",
      "Epoch 237 Batch  165/215   train_loss = 2.932\n",
      "Epoch 237 Batch  175/215   train_loss = 2.994\n",
      "Epoch 237 Batch  185/215   train_loss = 2.931\n",
      "Epoch 237 Batch  195/215   train_loss = 2.691\n",
      "Epoch 237 Batch  205/215   train_loss = 2.715\n",
      "Epoch 238 Batch    0/215   train_loss = 2.945\n",
      "Epoch 238 Batch   10/215   train_loss = 2.814\n",
      "Epoch 238 Batch   20/215   train_loss = 3.055\n",
      "Epoch 238 Batch   30/215   train_loss = 2.733\n",
      "Epoch 238 Batch   40/215   train_loss = 2.820\n",
      "Epoch 238 Batch   50/215   train_loss = 2.733\n",
      "Epoch 238 Batch   60/215   train_loss = 2.946\n",
      "Epoch 238 Batch   70/215   train_loss = 2.632\n",
      "Epoch 238 Batch   80/215   train_loss = 2.786\n",
      "Epoch 238 Batch   90/215   train_loss = 3.044\n",
      "Epoch 238 Batch  100/215   train_loss = 2.747\n",
      "Epoch 238 Batch  110/215   train_loss = 2.684\n",
      "Epoch 238 Batch  120/215   train_loss = 2.780\n",
      "Epoch 238 Batch  130/215   train_loss = 3.113\n",
      "Epoch 238 Batch  140/215   train_loss = 2.784\n",
      "Epoch 238 Batch  150/215   train_loss = 2.874\n",
      "Epoch 238 Batch  160/215   train_loss = 2.982\n",
      "Epoch 238 Batch  170/215   train_loss = 2.702\n",
      "Epoch 238 Batch  180/215   train_loss = 2.722\n",
      "Epoch 238 Batch  190/215   train_loss = 2.869\n",
      "Epoch 238 Batch  200/215   train_loss = 2.591\n",
      "Epoch 238 Batch  210/215   train_loss = 2.715\n",
      "Epoch 239 Batch    5/215   train_loss = 2.968\n",
      "Epoch 239 Batch   15/215   train_loss = 2.816\n",
      "Epoch 239 Batch   25/215   train_loss = 2.753\n",
      "Epoch 239 Batch   35/215   train_loss = 3.119\n",
      "Epoch 239 Batch   45/215   train_loss = 2.914\n",
      "Epoch 239 Batch   55/215   train_loss = 2.973\n",
      "Epoch 239 Batch   65/215   train_loss = 2.873\n",
      "Epoch 239 Batch   75/215   train_loss = 2.699\n",
      "Epoch 239 Batch   85/215   train_loss = 2.668\n",
      "Epoch 239 Batch   95/215   train_loss = 2.871\n",
      "Epoch 239 Batch  105/215   train_loss = 2.870\n",
      "Epoch 239 Batch  115/215   train_loss = 2.762\n",
      "Epoch 239 Batch  125/215   train_loss = 2.686\n",
      "Epoch 239 Batch  135/215   train_loss = 2.787\n",
      "Epoch 239 Batch  145/215   train_loss = 2.809\n",
      "Epoch 239 Batch  155/215   train_loss = 2.815\n",
      "Epoch 239 Batch  165/215   train_loss = 2.757\n",
      "Epoch 239 Batch  175/215   train_loss = 3.054\n",
      "Epoch 239 Batch  185/215   train_loss = 2.942\n",
      "Epoch 239 Batch  195/215   train_loss = 2.760\n",
      "Epoch 239 Batch  205/215   train_loss = 2.596\n",
      "Epoch 240 Batch    0/215   train_loss = 2.862\n",
      "Epoch 240 Batch   10/215   train_loss = 3.015\n",
      "Epoch 240 Batch   20/215   train_loss = 2.716\n",
      "Epoch 240 Batch   30/215   train_loss = 2.777\n",
      "Epoch 240 Batch   40/215   train_loss = 2.805\n",
      "Epoch 240 Batch   50/215   train_loss = 2.776\n",
      "Epoch 240 Batch   60/215   train_loss = 2.742\n",
      "Epoch 240 Batch   70/215   train_loss = 2.755\n",
      "Epoch 240 Batch   80/215   train_loss = 2.891\n",
      "Epoch 240 Batch   90/215   train_loss = 3.117\n",
      "Epoch 240 Batch  100/215   train_loss = 2.802\n",
      "Epoch 240 Batch  110/215   train_loss = 2.796\n",
      "Epoch 240 Batch  120/215   train_loss = 2.714\n",
      "Epoch 240 Batch  130/215   train_loss = 2.876\n",
      "Epoch 240 Batch  140/215   train_loss = 2.676\n",
      "Epoch 240 Batch  150/215   train_loss = 2.727\n",
      "Epoch 240 Batch  160/215   train_loss = 2.859\n",
      "Epoch 240 Batch  170/215   train_loss = 2.749\n",
      "Epoch 240 Batch  180/215   train_loss = 2.889\n",
      "Epoch 240 Batch  190/215   train_loss = 2.837\n",
      "Epoch 240 Batch  200/215   train_loss = 2.729\n",
      "Epoch 240 Batch  210/215   train_loss = 2.725\n",
      "Epoch 241 Batch    5/215   train_loss = 3.159\n",
      "Epoch 241 Batch   15/215   train_loss = 2.742\n",
      "Epoch 241 Batch   25/215   train_loss = 2.768\n",
      "Epoch 241 Batch   35/215   train_loss = 2.910\n",
      "Epoch 241 Batch   45/215   train_loss = 2.777\n",
      "Epoch 241 Batch   55/215   train_loss = 2.588\n",
      "Epoch 241 Batch   65/215   train_loss = 2.987\n",
      "Epoch 241 Batch   75/215   train_loss = 2.586\n",
      "Epoch 241 Batch   85/215   train_loss = 2.825\n",
      "Epoch 241 Batch   95/215   train_loss = 2.848\n",
      "Epoch 241 Batch  105/215   train_loss = 2.672\n",
      "Epoch 241 Batch  115/215   train_loss = 2.557\n",
      "Epoch 241 Batch  125/215   train_loss = 2.700\n",
      "Epoch 241 Batch  135/215   train_loss = 2.902\n",
      "Epoch 241 Batch  145/215   train_loss = 2.944\n",
      "Epoch 241 Batch  155/215   train_loss = 2.817\n",
      "Epoch 241 Batch  165/215   train_loss = 2.831\n",
      "Epoch 241 Batch  175/215   train_loss = 2.935\n",
      "Epoch 241 Batch  185/215   train_loss = 3.058\n",
      "Epoch 241 Batch  195/215   train_loss = 2.739\n",
      "Epoch 241 Batch  205/215   train_loss = 2.647\n",
      "Epoch 242 Batch    0/215   train_loss = 2.891\n",
      "Epoch 242 Batch   10/215   train_loss = 2.908\n",
      "Epoch 242 Batch   20/215   train_loss = 2.851\n",
      "Epoch 242 Batch   30/215   train_loss = 2.547\n",
      "Epoch 242 Batch   40/215   train_loss = 2.887\n",
      "Epoch 242 Batch   50/215   train_loss = 2.788\n",
      "Epoch 242 Batch   60/215   train_loss = 2.795\n",
      "Epoch 242 Batch   70/215   train_loss = 2.664\n",
      "Epoch 242 Batch   80/215   train_loss = 2.794\n",
      "Epoch 242 Batch   90/215   train_loss = 2.806\n",
      "Epoch 242 Batch  100/215   train_loss = 2.936\n",
      "Epoch 242 Batch  110/215   train_loss = 2.616\n",
      "Epoch 242 Batch  120/215   train_loss = 2.821\n",
      "Epoch 242 Batch  130/215   train_loss = 2.817\n",
      "Epoch 242 Batch  140/215   train_loss = 2.686\n",
      "Epoch 242 Batch  150/215   train_loss = 2.600\n",
      "Epoch 242 Batch  160/215   train_loss = 3.028\n",
      "Epoch 242 Batch  170/215   train_loss = 2.752\n",
      "Epoch 242 Batch  180/215   train_loss = 2.914\n",
      "Epoch 242 Batch  190/215   train_loss = 2.838\n",
      "Epoch 242 Batch  200/215   train_loss = 2.681\n",
      "Epoch 242 Batch  210/215   train_loss = 2.772\n",
      "Epoch 243 Batch    5/215   train_loss = 3.041\n",
      "Epoch 243 Batch   15/215   train_loss = 2.584\n",
      "Epoch 243 Batch   25/215   train_loss = 2.770\n",
      "Epoch 243 Batch   35/215   train_loss = 2.861\n",
      "Epoch 243 Batch   45/215   train_loss = 2.804\n",
      "Epoch 243 Batch   55/215   train_loss = 2.751\n",
      "Epoch 243 Batch   65/215   train_loss = 2.871\n",
      "Epoch 243 Batch   75/215   train_loss = 3.103\n",
      "Epoch 243 Batch   85/215   train_loss = 2.934\n",
      "Epoch 243 Batch   95/215   train_loss = 2.819\n",
      "Epoch 243 Batch  105/215   train_loss = 2.770\n",
      "Epoch 243 Batch  115/215   train_loss = 2.655\n",
      "Epoch 243 Batch  125/215   train_loss = 2.632\n",
      "Epoch 243 Batch  135/215   train_loss = 2.861\n",
      "Epoch 243 Batch  145/215   train_loss = 2.779\n",
      "Epoch 243 Batch  155/215   train_loss = 2.851\n",
      "Epoch 243 Batch  165/215   train_loss = 2.819\n",
      "Epoch 243 Batch  175/215   train_loss = 3.074\n",
      "Epoch 243 Batch  185/215   train_loss = 2.909\n",
      "Epoch 243 Batch  195/215   train_loss = 2.614\n",
      "Epoch 243 Batch  205/215   train_loss = 2.723\n",
      "Epoch 244 Batch    0/215   train_loss = 2.901\n",
      "Epoch 244 Batch   10/215   train_loss = 3.013\n",
      "Epoch 244 Batch   20/215   train_loss = 2.750\n",
      "Epoch 244 Batch   30/215   train_loss = 2.688\n",
      "Epoch 244 Batch   40/215   train_loss = 2.730\n",
      "Epoch 244 Batch   50/215   train_loss = 2.688\n",
      "Epoch 244 Batch   60/215   train_loss = 2.903\n",
      "Epoch 244 Batch   70/215   train_loss = 2.591\n",
      "Epoch 244 Batch   80/215   train_loss = 2.921\n",
      "Epoch 244 Batch   90/215   train_loss = 2.805\n",
      "Epoch 244 Batch  100/215   train_loss = 2.861\n",
      "Epoch 244 Batch  110/215   train_loss = 2.816\n",
      "Epoch 244 Batch  120/215   train_loss = 2.814\n",
      "Epoch 244 Batch  130/215   train_loss = 2.878\n",
      "Epoch 244 Batch  140/215   train_loss = 2.766\n",
      "Epoch 244 Batch  150/215   train_loss = 3.023\n",
      "Epoch 244 Batch  160/215   train_loss = 2.874\n",
      "Epoch 244 Batch  170/215   train_loss = 2.765\n",
      "Epoch 244 Batch  180/215   train_loss = 3.002\n",
      "Epoch 244 Batch  190/215   train_loss = 2.794\n",
      "Epoch 244 Batch  200/215   train_loss = 2.693\n",
      "Epoch 244 Batch  210/215   train_loss = 2.664\n",
      "Epoch 245 Batch    5/215   train_loss = 2.922\n",
      "Epoch 245 Batch   15/215   train_loss = 2.836\n",
      "Epoch 245 Batch   25/215   train_loss = 2.839\n",
      "Epoch 245 Batch   35/215   train_loss = 2.834\n",
      "Epoch 245 Batch   45/215   train_loss = 2.866\n",
      "Epoch 245 Batch   55/215   train_loss = 2.737\n",
      "Epoch 245 Batch   65/215   train_loss = 3.060\n",
      "Epoch 245 Batch   75/215   train_loss = 2.749\n",
      "Epoch 245 Batch   85/215   train_loss = 2.698\n",
      "Epoch 245 Batch   95/215   train_loss = 2.727\n",
      "Epoch 245 Batch  105/215   train_loss = 2.581\n",
      "Epoch 245 Batch  115/215   train_loss = 2.715\n",
      "Epoch 245 Batch  125/215   train_loss = 2.800\n",
      "Epoch 245 Batch  135/215   train_loss = 2.723\n",
      "Epoch 245 Batch  145/215   train_loss = 2.803\n",
      "Epoch 245 Batch  155/215   train_loss = 2.914\n",
      "Epoch 245 Batch  165/215   train_loss = 2.869\n",
      "Epoch 245 Batch  175/215   train_loss = 3.161\n",
      "Epoch 245 Batch  185/215   train_loss = 2.934\n",
      "Epoch 245 Batch  195/215   train_loss = 2.769\n",
      "Epoch 245 Batch  205/215   train_loss = 2.884\n",
      "Epoch 246 Batch    0/215   train_loss = 2.970\n",
      "Epoch 246 Batch   10/215   train_loss = 2.816\n",
      "Epoch 246 Batch   20/215   train_loss = 2.802\n",
      "Epoch 246 Batch   30/215   train_loss = 2.757\n",
      "Epoch 246 Batch   40/215   train_loss = 2.741\n",
      "Epoch 246 Batch   50/215   train_loss = 2.625\n",
      "Epoch 246 Batch   60/215   train_loss = 2.721\n",
      "Epoch 246 Batch   70/215   train_loss = 2.628\n",
      "Epoch 246 Batch   80/215   train_loss = 2.979\n",
      "Epoch 246 Batch   90/215   train_loss = 2.799\n",
      "Epoch 246 Batch  100/215   train_loss = 2.947\n",
      "Epoch 246 Batch  110/215   train_loss = 2.782\n",
      "Epoch 246 Batch  120/215   train_loss = 2.708\n",
      "Epoch 246 Batch  130/215   train_loss = 2.942\n",
      "Epoch 246 Batch  140/215   train_loss = 2.964\n",
      "Epoch 246 Batch  150/215   train_loss = 2.968\n",
      "Epoch 246 Batch  160/215   train_loss = 2.963\n",
      "Epoch 246 Batch  170/215   train_loss = 2.851\n",
      "Epoch 246 Batch  180/215   train_loss = 3.011\n",
      "Epoch 246 Batch  190/215   train_loss = 3.025\n",
      "Epoch 246 Batch  200/215   train_loss = 2.827\n",
      "Epoch 246 Batch  210/215   train_loss = 2.726\n",
      "Epoch 247 Batch    5/215   train_loss = 2.795\n",
      "Epoch 247 Batch   15/215   train_loss = 2.846\n",
      "Epoch 247 Batch   25/215   train_loss = 2.777\n",
      "Epoch 247 Batch   35/215   train_loss = 2.879\n",
      "Epoch 247 Batch   45/215   train_loss = 2.923\n",
      "Epoch 247 Batch   55/215   train_loss = 2.852\n",
      "Epoch 247 Batch   65/215   train_loss = 2.819\n",
      "Epoch 247 Batch   75/215   train_loss = 2.785\n",
      "Epoch 247 Batch   85/215   train_loss = 2.745\n",
      "Epoch 247 Batch   95/215   train_loss = 2.735\n",
      "Epoch 247 Batch  105/215   train_loss = 2.729\n",
      "Epoch 247 Batch  115/215   train_loss = 2.704\n",
      "Epoch 247 Batch  125/215   train_loss = 2.647\n",
      "Epoch 247 Batch  135/215   train_loss = 2.725\n",
      "Epoch 247 Batch  145/215   train_loss = 2.712\n",
      "Epoch 247 Batch  155/215   train_loss = 3.042\n",
      "Epoch 247 Batch  165/215   train_loss = 2.712\n",
      "Epoch 247 Batch  175/215   train_loss = 3.055\n",
      "Epoch 247 Batch  185/215   train_loss = 2.813\n",
      "Epoch 247 Batch  195/215   train_loss = 2.771\n",
      "Epoch 247 Batch  205/215   train_loss = 2.897\n",
      "Epoch 248 Batch    0/215   train_loss = 2.837\n",
      "Epoch 248 Batch   10/215   train_loss = 3.079\n",
      "Epoch 248 Batch   20/215   train_loss = 2.844\n",
      "Epoch 248 Batch   30/215   train_loss = 2.619\n",
      "Epoch 248 Batch   40/215   train_loss = 2.629\n",
      "Epoch 248 Batch   50/215   train_loss = 2.703\n",
      "Epoch 248 Batch   60/215   train_loss = 2.934\n",
      "Epoch 248 Batch   70/215   train_loss = 2.692\n",
      "Epoch 248 Batch   80/215   train_loss = 2.616\n",
      "Epoch 248 Batch   90/215   train_loss = 2.815\n",
      "Epoch 248 Batch  100/215   train_loss = 2.964\n",
      "Epoch 248 Batch  110/215   train_loss = 2.798\n",
      "Epoch 248 Batch  120/215   train_loss = 2.814\n",
      "Epoch 248 Batch  130/215   train_loss = 3.103\n",
      "Epoch 248 Batch  140/215   train_loss = 2.677\n",
      "Epoch 248 Batch  150/215   train_loss = 2.832\n",
      "Epoch 248 Batch  160/215   train_loss = 2.839\n",
      "Epoch 248 Batch  170/215   train_loss = 2.768\n",
      "Epoch 248 Batch  180/215   train_loss = 2.956\n",
      "Epoch 248 Batch  190/215   train_loss = 2.841\n",
      "Epoch 248 Batch  200/215   train_loss = 2.819\n",
      "Epoch 248 Batch  210/215   train_loss = 2.741\n",
      "Epoch 249 Batch    5/215   train_loss = 3.180\n",
      "Epoch 249 Batch   15/215   train_loss = 2.747\n",
      "Epoch 249 Batch   25/215   train_loss = 2.928\n",
      "Epoch 249 Batch   35/215   train_loss = 2.941\n",
      "Epoch 249 Batch   45/215   train_loss = 2.890\n",
      "Epoch 249 Batch   55/215   train_loss = 2.648\n",
      "Epoch 249 Batch   65/215   train_loss = 2.757\n",
      "Epoch 249 Batch   75/215   train_loss = 2.480\n",
      "Epoch 249 Batch   85/215   train_loss = 2.587\n",
      "Epoch 249 Batch   95/215   train_loss = 2.668\n",
      "Epoch 249 Batch  105/215   train_loss = 2.619\n",
      "Epoch 249 Batch  115/215   train_loss = 2.550\n",
      "Epoch 249 Batch  125/215   train_loss = 2.633\n",
      "Epoch 249 Batch  135/215   train_loss = 2.880\n",
      "Epoch 249 Batch  145/215   train_loss = 2.852\n",
      "Epoch 249 Batch  155/215   train_loss = 2.820\n",
      "Epoch 249 Batch  165/215   train_loss = 2.662\n",
      "Epoch 249 Batch  175/215   train_loss = 3.128\n",
      "Epoch 249 Batch  185/215   train_loss = 2.808\n",
      "Epoch 249 Batch  195/215   train_loss = 2.781\n",
      "Epoch 249 Batch  205/215   train_loss = 2.773\n",
      "Epoch 250 Batch    0/215   train_loss = 2.958\n",
      "Epoch 250 Batch   10/215   train_loss = 2.974\n",
      "Epoch 250 Batch   20/215   train_loss = 2.952\n",
      "Epoch 250 Batch   30/215   train_loss = 2.524\n",
      "Epoch 250 Batch   40/215   train_loss = 2.667\n",
      "Epoch 250 Batch   50/215   train_loss = 2.877\n",
      "Epoch 250 Batch   60/215   train_loss = 2.864\n",
      "Epoch 250 Batch   70/215   train_loss = 2.598\n",
      "Epoch 250 Batch   80/215   train_loss = 2.708\n",
      "Epoch 250 Batch   90/215   train_loss = 2.977\n",
      "Epoch 250 Batch  100/215   train_loss = 2.922\n",
      "Epoch 250 Batch  110/215   train_loss = 2.957\n",
      "Epoch 250 Batch  120/215   train_loss = 2.733\n",
      "Epoch 250 Batch  130/215   train_loss = 2.968\n",
      "Epoch 250 Batch  140/215   train_loss = 2.907\n",
      "Epoch 250 Batch  150/215   train_loss = 2.842\n",
      "Epoch 250 Batch  160/215   train_loss = 2.831\n",
      "Epoch 250 Batch  170/215   train_loss = 2.772\n",
      "Epoch 250 Batch  180/215   train_loss = 2.916\n",
      "Epoch 250 Batch  190/215   train_loss = 2.858\n",
      "Epoch 250 Batch  200/215   train_loss = 2.954\n",
      "Epoch 250 Batch  210/215   train_loss = 3.015\n",
      "Epoch 251 Batch    5/215   train_loss = 3.015\n",
      "Epoch 251 Batch   15/215   train_loss = 2.652\n",
      "Epoch 251 Batch   25/215   train_loss = 2.679\n",
      "Epoch 251 Batch   35/215   train_loss = 2.644\n",
      "Epoch 251 Batch   45/215   train_loss = 2.841\n",
      "Epoch 251 Batch   55/215   train_loss = 2.813\n",
      "Epoch 251 Batch   65/215   train_loss = 2.777\n",
      "Epoch 251 Batch   75/215   train_loss = 2.879\n",
      "Epoch 251 Batch   85/215   train_loss = 2.917\n",
      "Epoch 251 Batch   95/215   train_loss = 2.721\n",
      "Epoch 251 Batch  105/215   train_loss = 2.863\n",
      "Epoch 251 Batch  115/215   train_loss = 2.784\n",
      "Epoch 251 Batch  125/215   train_loss = 2.814\n",
      "Epoch 251 Batch  135/215   train_loss = 2.708\n",
      "Epoch 251 Batch  145/215   train_loss = 2.687\n",
      "Epoch 251 Batch  155/215   train_loss = 2.848\n",
      "Epoch 251 Batch  165/215   train_loss = 2.907\n",
      "Epoch 251 Batch  175/215   train_loss = 3.095\n",
      "Epoch 251 Batch  185/215   train_loss = 2.815\n",
      "Epoch 251 Batch  195/215   train_loss = 2.832\n",
      "Epoch 251 Batch  205/215   train_loss = 2.914\n",
      "Epoch 252 Batch    0/215   train_loss = 2.918\n",
      "Epoch 252 Batch   10/215   train_loss = 2.996\n",
      "Epoch 252 Batch   20/215   train_loss = 2.758\n",
      "Epoch 252 Batch   30/215   train_loss = 2.773\n",
      "Epoch 252 Batch   40/215   train_loss = 2.699\n",
      "Epoch 252 Batch   50/215   train_loss = 2.780\n",
      "Epoch 252 Batch   60/215   train_loss = 2.864\n",
      "Epoch 252 Batch   70/215   train_loss = 2.747\n",
      "Epoch 252 Batch   80/215   train_loss = 2.696\n",
      "Epoch 252 Batch   90/215   train_loss = 2.820\n",
      "Epoch 252 Batch  100/215   train_loss = 2.805\n",
      "Epoch 252 Batch  110/215   train_loss = 2.947\n",
      "Epoch 252 Batch  120/215   train_loss = 2.826\n",
      "Epoch 252 Batch  130/215   train_loss = 3.005\n",
      "Epoch 252 Batch  140/215   train_loss = 2.699\n",
      "Epoch 252 Batch  150/215   train_loss = 2.787\n",
      "Epoch 252 Batch  160/215   train_loss = 2.815\n",
      "Epoch 252 Batch  170/215   train_loss = 2.756\n",
      "Epoch 252 Batch  180/215   train_loss = 2.683\n",
      "Epoch 252 Batch  190/215   train_loss = 2.841\n",
      "Epoch 252 Batch  200/215   train_loss = 2.858\n",
      "Epoch 252 Batch  210/215   train_loss = 2.619\n",
      "Epoch 253 Batch    5/215   train_loss = 2.829\n",
      "Epoch 253 Batch   15/215   train_loss = 2.829\n",
      "Epoch 253 Batch   25/215   train_loss = 2.745\n",
      "Epoch 253 Batch   35/215   train_loss = 2.857\n",
      "Epoch 253 Batch   45/215   train_loss = 2.816\n",
      "Epoch 253 Batch   55/215   train_loss = 2.789\n",
      "Epoch 253 Batch   65/215   train_loss = 2.611\n",
      "Epoch 253 Batch   75/215   train_loss = 2.624\n",
      "Epoch 253 Batch   85/215   train_loss = 2.769\n",
      "Epoch 253 Batch   95/215   train_loss = 2.703\n",
      "Epoch 253 Batch  105/215   train_loss = 2.632\n",
      "Epoch 253 Batch  115/215   train_loss = 2.623\n",
      "Epoch 253 Batch  125/215   train_loss = 2.662\n",
      "Epoch 253 Batch  135/215   train_loss = 2.662\n",
      "Epoch 253 Batch  145/215   train_loss = 2.798\n",
      "Epoch 253 Batch  155/215   train_loss = 2.818\n",
      "Epoch 253 Batch  165/215   train_loss = 2.691\n",
      "Epoch 253 Batch  175/215   train_loss = 3.036\n",
      "Epoch 253 Batch  185/215   train_loss = 2.763\n",
      "Epoch 253 Batch  195/215   train_loss = 2.772\n",
      "Epoch 253 Batch  205/215   train_loss = 2.812\n",
      "Epoch 254 Batch    0/215   train_loss = 2.854\n",
      "Epoch 254 Batch   10/215   train_loss = 2.937\n",
      "Epoch 254 Batch   20/215   train_loss = 2.924\n",
      "Epoch 254 Batch   30/215   train_loss = 2.758\n",
      "Epoch 254 Batch   40/215   train_loss = 2.764\n",
      "Epoch 254 Batch   50/215   train_loss = 2.811\n",
      "Epoch 254 Batch   60/215   train_loss = 2.847\n",
      "Epoch 254 Batch   70/215   train_loss = 2.710\n",
      "Epoch 254 Batch   80/215   train_loss = 2.811\n",
      "Epoch 254 Batch   90/215   train_loss = 2.989\n",
      "Epoch 254 Batch  100/215   train_loss = 2.809\n",
      "Epoch 254 Batch  110/215   train_loss = 2.761\n",
      "Epoch 254 Batch  120/215   train_loss = 2.827\n",
      "Epoch 254 Batch  130/215   train_loss = 3.104\n",
      "Epoch 254 Batch  140/215   train_loss = 2.838\n",
      "Epoch 254 Batch  150/215   train_loss = 2.705\n",
      "Epoch 254 Batch  160/215   train_loss = 2.876\n",
      "Epoch 254 Batch  170/215   train_loss = 2.750\n",
      "Epoch 254 Batch  180/215   train_loss = 2.812\n",
      "Epoch 254 Batch  190/215   train_loss = 2.837\n",
      "Epoch 254 Batch  200/215   train_loss = 2.746\n",
      "Epoch 254 Batch  210/215   train_loss = 2.559\n",
      "Epoch 255 Batch    5/215   train_loss = 2.837\n",
      "Epoch 255 Batch   15/215   train_loss = 2.720\n",
      "Epoch 255 Batch   25/215   train_loss = 2.763\n",
      "Epoch 255 Batch   35/215   train_loss = 2.743\n",
      "Epoch 255 Batch   45/215   train_loss = 2.812\n",
      "Epoch 255 Batch   55/215   train_loss = 2.824\n",
      "Epoch 255 Batch   65/215   train_loss = 2.767\n",
      "Epoch 255 Batch   75/215   train_loss = 2.665\n",
      "Epoch 255 Batch   85/215   train_loss = 2.592\n",
      "Epoch 255 Batch   95/215   train_loss = 2.631\n",
      "Epoch 255 Batch  105/215   train_loss = 2.815\n",
      "Epoch 255 Batch  115/215   train_loss = 2.689\n",
      "Epoch 255 Batch  125/215   train_loss = 2.704\n",
      "Epoch 255 Batch  135/215   train_loss = 2.721\n",
      "Epoch 255 Batch  145/215   train_loss = 2.760\n",
      "Epoch 255 Batch  155/215   train_loss = 2.864\n",
      "Epoch 255 Batch  165/215   train_loss = 2.906\n",
      "Epoch 255 Batch  175/215   train_loss = 3.080\n",
      "Epoch 255 Batch  185/215   train_loss = 2.812\n",
      "Epoch 255 Batch  195/215   train_loss = 2.674\n",
      "Epoch 255 Batch  205/215   train_loss = 2.720\n",
      "Epoch 256 Batch    0/215   train_loss = 2.887\n",
      "Epoch 256 Batch   10/215   train_loss = 2.820\n",
      "Epoch 256 Batch   20/215   train_loss = 2.895\n",
      "Epoch 256 Batch   30/215   train_loss = 2.776\n",
      "Epoch 256 Batch   40/215   train_loss = 2.873\n",
      "Epoch 256 Batch   50/215   train_loss = 2.619\n",
      "Epoch 256 Batch   60/215   train_loss = 2.922\n",
      "Epoch 256 Batch   70/215   train_loss = 2.541\n",
      "Epoch 256 Batch   80/215   train_loss = 2.814\n",
      "Epoch 256 Batch   90/215   train_loss = 3.110\n",
      "Epoch 256 Batch  100/215   train_loss = 2.711\n",
      "Epoch 256 Batch  110/215   train_loss = 2.708\n",
      "Epoch 256 Batch  120/215   train_loss = 2.706\n",
      "Epoch 256 Batch  130/215   train_loss = 3.049\n",
      "Epoch 256 Batch  140/215   train_loss = 2.823\n",
      "Epoch 256 Batch  150/215   train_loss = 3.037\n",
      "Epoch 256 Batch  160/215   train_loss = 2.781\n",
      "Epoch 256 Batch  170/215   train_loss = 2.852\n",
      "Epoch 256 Batch  180/215   train_loss = 2.964\n",
      "Epoch 256 Batch  190/215   train_loss = 2.624\n",
      "Epoch 256 Batch  200/215   train_loss = 2.810\n",
      "Epoch 256 Batch  210/215   train_loss = 2.830\n",
      "Epoch 257 Batch    5/215   train_loss = 3.021\n",
      "Epoch 257 Batch   15/215   train_loss = 2.689\n",
      "Epoch 257 Batch   25/215   train_loss = 2.584\n",
      "Epoch 257 Batch   35/215   train_loss = 2.945\n",
      "Epoch 257 Batch   45/215   train_loss = 3.008\n",
      "Epoch 257 Batch   55/215   train_loss = 2.656\n",
      "Epoch 257 Batch   65/215   train_loss = 2.851\n",
      "Epoch 257 Batch   75/215   train_loss = 2.823\n",
      "Epoch 257 Batch   85/215   train_loss = 2.781\n",
      "Epoch 257 Batch   95/215   train_loss = 2.786\n",
      "Epoch 257 Batch  105/215   train_loss = 2.698\n",
      "Epoch 257 Batch  115/215   train_loss = 2.661\n",
      "Epoch 257 Batch  125/215   train_loss = 2.631\n",
      "Epoch 257 Batch  135/215   train_loss = 2.776\n",
      "Epoch 257 Batch  145/215   train_loss = 2.773\n",
      "Epoch 257 Batch  155/215   train_loss = 2.855\n",
      "Epoch 257 Batch  165/215   train_loss = 2.825\n",
      "Epoch 257 Batch  175/215   train_loss = 2.780\n",
      "Epoch 257 Batch  185/215   train_loss = 2.673\n",
      "Epoch 257 Batch  195/215   train_loss = 2.675\n",
      "Epoch 257 Batch  205/215   train_loss = 2.850\n",
      "Epoch 258 Batch    0/215   train_loss = 2.723\n",
      "Epoch 258 Batch   10/215   train_loss = 2.871\n",
      "Epoch 258 Batch   20/215   train_loss = 2.941\n",
      "Epoch 258 Batch   30/215   train_loss = 2.584\n",
      "Epoch 258 Batch   40/215   train_loss = 2.763\n",
      "Epoch 258 Batch   50/215   train_loss = 2.826\n",
      "Epoch 258 Batch   60/215   train_loss = 2.915\n",
      "Epoch 258 Batch   70/215   train_loss = 2.629\n",
      "Epoch 258 Batch   80/215   train_loss = 2.774\n",
      "Epoch 258 Batch   90/215   train_loss = 2.928\n",
      "Epoch 258 Batch  100/215   train_loss = 2.881\n",
      "Epoch 258 Batch  110/215   train_loss = 2.800\n",
      "Epoch 258 Batch  120/215   train_loss = 2.853\n",
      "Epoch 258 Batch  130/215   train_loss = 2.921\n",
      "Epoch 258 Batch  140/215   train_loss = 2.842\n",
      "Epoch 258 Batch  150/215   train_loss = 2.874\n",
      "Epoch 258 Batch  160/215   train_loss = 2.964\n",
      "Epoch 258 Batch  170/215   train_loss = 2.662\n",
      "Epoch 258 Batch  180/215   train_loss = 2.944\n",
      "Epoch 258 Batch  190/215   train_loss = 2.850\n",
      "Epoch 258 Batch  200/215   train_loss = 2.648\n",
      "Epoch 258 Batch  210/215   train_loss = 2.707\n",
      "Epoch 259 Batch    5/215   train_loss = 2.885\n",
      "Epoch 259 Batch   15/215   train_loss = 2.821\n",
      "Epoch 259 Batch   25/215   train_loss = 2.766\n",
      "Epoch 259 Batch   35/215   train_loss = 2.820\n",
      "Epoch 259 Batch   45/215   train_loss = 2.696\n",
      "Epoch 259 Batch   55/215   train_loss = 2.665\n",
      "Epoch 259 Batch   65/215   train_loss = 2.785\n",
      "Epoch 259 Batch   75/215   train_loss = 2.734\n",
      "Epoch 259 Batch   85/215   train_loss = 2.601\n",
      "Epoch 259 Batch   95/215   train_loss = 2.680\n",
      "Epoch 259 Batch  105/215   train_loss = 2.651\n",
      "Epoch 259 Batch  115/215   train_loss = 2.699\n",
      "Epoch 259 Batch  125/215   train_loss = 3.048\n",
      "Epoch 259 Batch  135/215   train_loss = 2.787\n",
      "Epoch 259 Batch  145/215   train_loss = 2.799\n",
      "Epoch 259 Batch  155/215   train_loss = 2.850\n",
      "Epoch 259 Batch  165/215   train_loss = 2.801\n",
      "Epoch 259 Batch  175/215   train_loss = 2.881\n",
      "Epoch 259 Batch  185/215   train_loss = 2.750\n",
      "Epoch 259 Batch  195/215   train_loss = 2.641\n",
      "Epoch 259 Batch  205/215   train_loss = 2.783\n",
      "Epoch 260 Batch    0/215   train_loss = 2.994\n",
      "Epoch 260 Batch   10/215   train_loss = 2.891\n",
      "Epoch 260 Batch   20/215   train_loss = 2.778\n",
      "Epoch 260 Batch   30/215   train_loss = 2.538\n",
      "Epoch 260 Batch   40/215   train_loss = 2.788\n",
      "Epoch 260 Batch   50/215   train_loss = 2.575\n",
      "Epoch 260 Batch   60/215   train_loss = 2.940\n",
      "Epoch 260 Batch   70/215   train_loss = 2.788\n",
      "Epoch 260 Batch   80/215   train_loss = 2.767\n",
      "Epoch 260 Batch   90/215   train_loss = 2.828\n",
      "Epoch 260 Batch  100/215   train_loss = 3.059\n",
      "Epoch 260 Batch  110/215   train_loss = 2.787\n",
      "Epoch 260 Batch  120/215   train_loss = 2.881\n",
      "Epoch 260 Batch  130/215   train_loss = 3.180\n",
      "Epoch 260 Batch  140/215   train_loss = 2.771\n",
      "Epoch 260 Batch  150/215   train_loss = 2.855\n",
      "Epoch 260 Batch  160/215   train_loss = 2.898\n",
      "Epoch 260 Batch  170/215   train_loss = 2.919\n",
      "Epoch 260 Batch  180/215   train_loss = 2.811\n",
      "Epoch 260 Batch  190/215   train_loss = 2.710\n",
      "Epoch 260 Batch  200/215   train_loss = 2.557\n",
      "Epoch 260 Batch  210/215   train_loss = 2.623\n",
      "Epoch 261 Batch    5/215   train_loss = 2.877\n",
      "Epoch 261 Batch   15/215   train_loss = 2.657\n",
      "Epoch 261 Batch   25/215   train_loss = 2.651\n",
      "Epoch 261 Batch   35/215   train_loss = 2.928\n",
      "Epoch 261 Batch   45/215   train_loss = 2.838\n",
      "Epoch 261 Batch   55/215   train_loss = 2.542\n",
      "Epoch 261 Batch   65/215   train_loss = 2.976\n",
      "Epoch 261 Batch   75/215   train_loss = 2.749\n",
      "Epoch 261 Batch   85/215   train_loss = 2.636\n",
      "Epoch 261 Batch   95/215   train_loss = 2.766\n",
      "Epoch 261 Batch  105/215   train_loss = 2.606\n",
      "Epoch 261 Batch  115/215   train_loss = 2.672\n",
      "Epoch 261 Batch  125/215   train_loss = 2.808\n",
      "Epoch 261 Batch  135/215   train_loss = 2.913\n",
      "Epoch 261 Batch  145/215   train_loss = 2.629\n",
      "Epoch 261 Batch  155/215   train_loss = 3.066\n",
      "Epoch 261 Batch  165/215   train_loss = 2.705\n",
      "Epoch 261 Batch  175/215   train_loss = 2.946\n",
      "Epoch 261 Batch  185/215   train_loss = 2.837\n",
      "Epoch 261 Batch  195/215   train_loss = 2.714\n",
      "Epoch 261 Batch  205/215   train_loss = 2.668\n",
      "Epoch 262 Batch    0/215   train_loss = 2.799\n",
      "Epoch 262 Batch   10/215   train_loss = 2.911\n",
      "Epoch 262 Batch   20/215   train_loss = 2.785\n",
      "Epoch 262 Batch   30/215   train_loss = 2.472\n",
      "Epoch 262 Batch   40/215   train_loss = 2.774\n",
      "Epoch 262 Batch   50/215   train_loss = 2.657\n",
      "Epoch 262 Batch   60/215   train_loss = 2.840\n",
      "Epoch 262 Batch   70/215   train_loss = 2.611\n",
      "Epoch 262 Batch   80/215   train_loss = 2.816\n",
      "Epoch 262 Batch   90/215   train_loss = 2.994\n",
      "Epoch 262 Batch  100/215   train_loss = 2.708\n",
      "Epoch 262 Batch  110/215   train_loss = 2.814\n",
      "Epoch 262 Batch  120/215   train_loss = 3.013\n",
      "Epoch 262 Batch  130/215   train_loss = 3.061\n",
      "Epoch 262 Batch  140/215   train_loss = 2.821\n",
      "Epoch 262 Batch  150/215   train_loss = 2.686\n",
      "Epoch 262 Batch  160/215   train_loss = 2.912\n",
      "Epoch 262 Batch  170/215   train_loss = 2.677\n",
      "Epoch 262 Batch  180/215   train_loss = 2.740\n",
      "Epoch 262 Batch  190/215   train_loss = 2.917\n",
      "Epoch 262 Batch  200/215   train_loss = 2.764\n",
      "Epoch 262 Batch  210/215   train_loss = 2.662\n",
      "Epoch 263 Batch    5/215   train_loss = 2.904\n",
      "Epoch 263 Batch   15/215   train_loss = 2.647\n",
      "Epoch 263 Batch   25/215   train_loss = 2.668\n",
      "Epoch 263 Batch   35/215   train_loss = 2.858\n",
      "Epoch 263 Batch   45/215   train_loss = 2.862\n",
      "Epoch 263 Batch   55/215   train_loss = 2.772\n",
      "Epoch 263 Batch   65/215   train_loss = 2.882\n",
      "Epoch 263 Batch   75/215   train_loss = 2.766\n",
      "Epoch 263 Batch   85/215   train_loss = 2.592\n",
      "Epoch 263 Batch   95/215   train_loss = 2.654\n",
      "Epoch 263 Batch  105/215   train_loss = 2.568\n",
      "Epoch 263 Batch  115/215   train_loss = 2.799\n",
      "Epoch 263 Batch  125/215   train_loss = 2.749\n",
      "Epoch 263 Batch  135/215   train_loss = 2.752\n",
      "Epoch 263 Batch  145/215   train_loss = 2.787\n",
      "Epoch 263 Batch  155/215   train_loss = 2.765\n",
      "Epoch 263 Batch  165/215   train_loss = 2.784\n",
      "Epoch 263 Batch  175/215   train_loss = 2.792\n",
      "Epoch 263 Batch  185/215   train_loss = 2.834\n",
      "Epoch 263 Batch  195/215   train_loss = 2.750\n",
      "Epoch 263 Batch  205/215   train_loss = 2.719\n",
      "Epoch 264 Batch    0/215   train_loss = 2.771\n",
      "Epoch 264 Batch   10/215   train_loss = 2.840\n",
      "Epoch 264 Batch   20/215   train_loss = 2.811\n",
      "Epoch 264 Batch   30/215   train_loss = 2.779\n",
      "Epoch 264 Batch   40/215   train_loss = 2.864\n",
      "Epoch 264 Batch   50/215   train_loss = 2.875\n",
      "Epoch 264 Batch   60/215   train_loss = 2.884\n",
      "Epoch 264 Batch   70/215   train_loss = 2.650\n",
      "Epoch 264 Batch   80/215   train_loss = 2.927\n",
      "Epoch 264 Batch   90/215   train_loss = 2.829\n",
      "Epoch 264 Batch  100/215   train_loss = 2.901\n",
      "Epoch 264 Batch  110/215   train_loss = 2.811\n",
      "Epoch 264 Batch  120/215   train_loss = 2.797\n",
      "Epoch 264 Batch  130/215   train_loss = 3.041\n",
      "Epoch 264 Batch  140/215   train_loss = 2.847\n",
      "Epoch 264 Batch  150/215   train_loss = 2.823\n",
      "Epoch 264 Batch  160/215   train_loss = 2.826\n",
      "Epoch 264 Batch  170/215   train_loss = 2.633\n",
      "Epoch 264 Batch  180/215   train_loss = 2.947\n",
      "Epoch 264 Batch  190/215   train_loss = 2.669\n",
      "Epoch 264 Batch  200/215   train_loss = 2.708\n",
      "Epoch 264 Batch  210/215   train_loss = 2.724\n",
      "Epoch 265 Batch    5/215   train_loss = 3.010\n",
      "Epoch 265 Batch   15/215   train_loss = 2.618\n",
      "Epoch 265 Batch   25/215   train_loss = 2.746\n",
      "Epoch 265 Batch   35/215   train_loss = 3.452\n",
      "Epoch 265 Batch   45/215   train_loss = 2.817\n",
      "Epoch 265 Batch   55/215   train_loss = 2.623\n",
      "Epoch 265 Batch   65/215   train_loss = 2.606\n",
      "Epoch 265 Batch   75/215   train_loss = 2.668\n",
      "Epoch 265 Batch   85/215   train_loss = 2.671\n",
      "Epoch 265 Batch   95/215   train_loss = 2.649\n",
      "Epoch 265 Batch  105/215   train_loss = 2.693\n",
      "Epoch 265 Batch  115/215   train_loss = 2.734\n",
      "Epoch 265 Batch  125/215   train_loss = 2.975\n",
      "Epoch 265 Batch  135/215   train_loss = 2.779\n",
      "Epoch 265 Batch  145/215   train_loss = 2.672\n",
      "Epoch 265 Batch  155/215   train_loss = 2.812\n",
      "Epoch 265 Batch  165/215   train_loss = 2.879\n",
      "Epoch 265 Batch  175/215   train_loss = 2.914\n",
      "Epoch 265 Batch  185/215   train_loss = 2.804\n",
      "Epoch 265 Batch  195/215   train_loss = 2.971\n",
      "Epoch 265 Batch  205/215   train_loss = 2.715\n",
      "Epoch 266 Batch    0/215   train_loss = 2.758\n",
      "Epoch 266 Batch   10/215   train_loss = 2.951\n",
      "Epoch 266 Batch   20/215   train_loss = 2.835\n",
      "Epoch 266 Batch   30/215   train_loss = 2.518\n",
      "Epoch 266 Batch   40/215   train_loss = 2.667\n",
      "Epoch 266 Batch   50/215   train_loss = 2.740\n",
      "Epoch 266 Batch   60/215   train_loss = 3.019\n",
      "Epoch 266 Batch   70/215   train_loss = 2.573\n",
      "Epoch 266 Batch   80/215   train_loss = 2.807\n",
      "Epoch 266 Batch   90/215   train_loss = 2.838\n",
      "Epoch 266 Batch  100/215   train_loss = 2.803\n",
      "Epoch 266 Batch  110/215   train_loss = 2.679\n",
      "Epoch 266 Batch  120/215   train_loss = 2.937\n",
      "Epoch 266 Batch  130/215   train_loss = 2.979\n",
      "Epoch 266 Batch  140/215   train_loss = 2.868\n",
      "Epoch 266 Batch  150/215   train_loss = 2.765\n",
      "Epoch 266 Batch  160/215   train_loss = 3.051\n",
      "Epoch 266 Batch  170/215   train_loss = 2.761\n",
      "Epoch 266 Batch  180/215   train_loss = 2.978\n",
      "Epoch 266 Batch  190/215   train_loss = 2.766\n",
      "Epoch 266 Batch  200/215   train_loss = 2.911\n",
      "Epoch 266 Batch  210/215   train_loss = 2.524\n",
      "Epoch 267 Batch    5/215   train_loss = 2.752\n",
      "Epoch 267 Batch   15/215   train_loss = 2.678\n",
      "Epoch 267 Batch   25/215   train_loss = 2.636\n",
      "Epoch 267 Batch   35/215   train_loss = 2.962\n",
      "Epoch 267 Batch   45/215   train_loss = 2.732\n",
      "Epoch 267 Batch   55/215   train_loss = 2.641\n",
      "Epoch 267 Batch   65/215   train_loss = 2.695\n",
      "Epoch 267 Batch   75/215   train_loss = 2.700\n",
      "Epoch 267 Batch   85/215   train_loss = 2.898\n",
      "Epoch 267 Batch   95/215   train_loss = 2.798\n",
      "Epoch 267 Batch  105/215   train_loss = 2.793\n",
      "Epoch 267 Batch  115/215   train_loss = 2.964\n",
      "Epoch 267 Batch  125/215   train_loss = 2.833\n",
      "Epoch 267 Batch  135/215   train_loss = 2.807\n",
      "Epoch 267 Batch  145/215   train_loss = 2.634\n",
      "Epoch 267 Batch  155/215   train_loss = 2.799\n",
      "Epoch 267 Batch  165/215   train_loss = 2.861\n",
      "Epoch 267 Batch  175/215   train_loss = 2.961\n",
      "Epoch 267 Batch  185/215   train_loss = 2.772\n",
      "Epoch 267 Batch  195/215   train_loss = 2.613\n",
      "Epoch 267 Batch  205/215   train_loss = 2.706\n",
      "Epoch 268 Batch    0/215   train_loss = 2.899\n",
      "Epoch 268 Batch   10/215   train_loss = 2.708\n",
      "Epoch 268 Batch   20/215   train_loss = 2.640\n",
      "Epoch 268 Batch   30/215   train_loss = 2.594\n",
      "Epoch 268 Batch   40/215   train_loss = 2.605\n",
      "Epoch 268 Batch   50/215   train_loss = 2.704\n",
      "Epoch 268 Batch   60/215   train_loss = 2.812\n",
      "Epoch 268 Batch   70/215   train_loss = 2.894\n",
      "Epoch 268 Batch   80/215   train_loss = 2.715\n",
      "Epoch 268 Batch   90/215   train_loss = 2.859\n",
      "Epoch 268 Batch  100/215   train_loss = 2.680\n",
      "Epoch 268 Batch  110/215   train_loss = 2.751\n",
      "Epoch 268 Batch  120/215   train_loss = 2.771\n",
      "Epoch 268 Batch  130/215   train_loss = 2.969\n",
      "Epoch 268 Batch  140/215   train_loss = 2.739\n",
      "Epoch 268 Batch  150/215   train_loss = 2.832\n",
      "Epoch 268 Batch  160/215   train_loss = 2.868\n",
      "Epoch 268 Batch  170/215   train_loss = 2.725\n",
      "Epoch 268 Batch  180/215   train_loss = 2.884\n",
      "Epoch 268 Batch  190/215   train_loss = 2.708\n",
      "Epoch 268 Batch  200/215   train_loss = 2.592\n",
      "Epoch 268 Batch  210/215   train_loss = 2.653\n",
      "Epoch 269 Batch    5/215   train_loss = 3.038\n",
      "Epoch 269 Batch   15/215   train_loss = 2.595\n",
      "Epoch 269 Batch   25/215   train_loss = 2.867\n",
      "Epoch 269 Batch   35/215   train_loss = 2.868\n",
      "Epoch 269 Batch   45/215   train_loss = 2.768\n",
      "Epoch 269 Batch   55/215   train_loss = 2.716\n",
      "Epoch 269 Batch   65/215   train_loss = 2.751\n",
      "Epoch 269 Batch   75/215   train_loss = 2.637\n",
      "Epoch 269 Batch   85/215   train_loss = 2.570\n",
      "Epoch 269 Batch   95/215   train_loss = 2.621\n",
      "Epoch 269 Batch  105/215   train_loss = 2.736\n",
      "Epoch 269 Batch  115/215   train_loss = 2.782\n",
      "Epoch 269 Batch  125/215   train_loss = 2.862\n",
      "Epoch 269 Batch  135/215   train_loss = 2.873\n",
      "Epoch 269 Batch  145/215   train_loss = 2.699\n",
      "Epoch 269 Batch  155/215   train_loss = 3.127\n",
      "Epoch 269 Batch  165/215   train_loss = 2.831\n",
      "Epoch 269 Batch  175/215   train_loss = 2.997\n",
      "Epoch 269 Batch  185/215   train_loss = 2.975\n",
      "Epoch 269 Batch  195/215   train_loss = 2.934\n",
      "Epoch 269 Batch  205/215   train_loss = 3.012\n",
      "Epoch 270 Batch    0/215   train_loss = 2.791\n",
      "Epoch 270 Batch   10/215   train_loss = 2.842\n",
      "Epoch 270 Batch   20/215   train_loss = 2.858\n",
      "Epoch 270 Batch   30/215   train_loss = 2.746\n",
      "Epoch 270 Batch   40/215   train_loss = 2.716\n",
      "Epoch 270 Batch   50/215   train_loss = 2.667\n",
      "Epoch 270 Batch   60/215   train_loss = 2.764\n",
      "Epoch 270 Batch   70/215   train_loss = 2.728\n",
      "Epoch 270 Batch   80/215   train_loss = 2.999\n",
      "Epoch 270 Batch   90/215   train_loss = 2.841\n",
      "Epoch 270 Batch  100/215   train_loss = 2.925\n",
      "Epoch 270 Batch  110/215   train_loss = 2.929\n",
      "Epoch 270 Batch  120/215   train_loss = 2.637\n",
      "Epoch 270 Batch  130/215   train_loss = 3.063\n",
      "Epoch 270 Batch  140/215   train_loss = 2.905\n",
      "Epoch 270 Batch  150/215   train_loss = 2.703\n",
      "Epoch 270 Batch  160/215   train_loss = 2.815\n",
      "Epoch 270 Batch  170/215   train_loss = 2.666\n",
      "Epoch 270 Batch  180/215   train_loss = 2.835\n",
      "Epoch 270 Batch  190/215   train_loss = 2.760\n",
      "Epoch 270 Batch  200/215   train_loss = 2.817\n",
      "Epoch 270 Batch  210/215   train_loss = 2.595\n",
      "Epoch 271 Batch    5/215   train_loss = 2.785\n",
      "Epoch 271 Batch   15/215   train_loss = 2.733\n",
      "Epoch 271 Batch   25/215   train_loss = 2.891\n",
      "Epoch 271 Batch   35/215   train_loss = 2.814\n",
      "Epoch 271 Batch   45/215   train_loss = 2.813\n",
      "Epoch 271 Batch   55/215   train_loss = 2.711\n",
      "Epoch 271 Batch   65/215   train_loss = 2.723\n",
      "Epoch 271 Batch   75/215   train_loss = 2.782\n",
      "Epoch 271 Batch   85/215   train_loss = 2.641\n",
      "Epoch 271 Batch   95/215   train_loss = 2.750\n",
      "Epoch 271 Batch  105/215   train_loss = 2.573\n",
      "Epoch 271 Batch  115/215   train_loss = 2.809\n",
      "Epoch 271 Batch  125/215   train_loss = 2.705\n",
      "Epoch 271 Batch  135/215   train_loss = 2.881\n",
      "Epoch 271 Batch  145/215   train_loss = 2.727\n",
      "Epoch 271 Batch  155/215   train_loss = 2.871\n",
      "Epoch 271 Batch  165/215   train_loss = 3.029\n",
      "Epoch 271 Batch  175/215   train_loss = 3.075\n",
      "Epoch 271 Batch  185/215   train_loss = 2.827\n",
      "Epoch 271 Batch  195/215   train_loss = 2.719\n",
      "Epoch 271 Batch  205/215   train_loss = 2.762\n",
      "Epoch 272 Batch    0/215   train_loss = 2.998\n",
      "Epoch 272 Batch   10/215   train_loss = 3.114\n",
      "Epoch 272 Batch   20/215   train_loss = 2.772\n",
      "Epoch 272 Batch   30/215   train_loss = 2.642\n",
      "Epoch 272 Batch   40/215   train_loss = 2.845\n",
      "Epoch 272 Batch   50/215   train_loss = 2.807\n",
      "Epoch 272 Batch   60/215   train_loss = 3.007\n",
      "Epoch 272 Batch   70/215   train_loss = 2.803\n",
      "Epoch 272 Batch   80/215   train_loss = 2.697\n",
      "Epoch 272 Batch   90/215   train_loss = 2.696\n",
      "Epoch 272 Batch  100/215   train_loss = 2.760\n",
      "Epoch 272 Batch  110/215   train_loss = 2.842\n",
      "Epoch 272 Batch  120/215   train_loss = 2.787\n",
      "Epoch 272 Batch  130/215   train_loss = 2.950\n",
      "Epoch 272 Batch  140/215   train_loss = 2.783\n",
      "Epoch 272 Batch  150/215   train_loss = 2.716\n",
      "Epoch 272 Batch  160/215   train_loss = 2.763\n",
      "Epoch 272 Batch  170/215   train_loss = 2.992\n",
      "Epoch 272 Batch  180/215   train_loss = 2.854\n",
      "Epoch 272 Batch  190/215   train_loss = 2.898\n",
      "Epoch 272 Batch  200/215   train_loss = 2.970\n",
      "Epoch 272 Batch  210/215   train_loss = 2.625\n",
      "Epoch 273 Batch    5/215   train_loss = 3.112\n",
      "Epoch 273 Batch   15/215   train_loss = 2.810\n",
      "Epoch 273 Batch   25/215   train_loss = 2.728\n",
      "Epoch 273 Batch   35/215   train_loss = 2.961\n",
      "Epoch 273 Batch   45/215   train_loss = 2.894\n",
      "Epoch 273 Batch   55/215   train_loss = 2.779\n",
      "Epoch 273 Batch   65/215   train_loss = 2.905\n",
      "Epoch 273 Batch   75/215   train_loss = 2.736\n",
      "Epoch 273 Batch   85/215   train_loss = 2.580\n",
      "Epoch 273 Batch   95/215   train_loss = 2.777\n",
      "Epoch 273 Batch  105/215   train_loss = 2.569\n",
      "Epoch 273 Batch  115/215   train_loss = 2.658\n",
      "Epoch 273 Batch  125/215   train_loss = 2.969\n",
      "Epoch 273 Batch  135/215   train_loss = 2.706\n",
      "Epoch 273 Batch  145/215   train_loss = 2.722\n",
      "Epoch 273 Batch  155/215   train_loss = 3.003\n",
      "Epoch 273 Batch  165/215   train_loss = 2.686\n",
      "Epoch 273 Batch  175/215   train_loss = 3.094\n",
      "Epoch 273 Batch  185/215   train_loss = 2.772\n",
      "Epoch 273 Batch  195/215   train_loss = 2.802\n",
      "Epoch 273 Batch  205/215   train_loss = 2.552\n",
      "Epoch 274 Batch    0/215   train_loss = 2.973\n",
      "Epoch 274 Batch   10/215   train_loss = 2.723\n",
      "Epoch 274 Batch   20/215   train_loss = 2.578\n",
      "Epoch 274 Batch   30/215   train_loss = 2.655\n",
      "Epoch 274 Batch   40/215   train_loss = 2.667\n",
      "Epoch 274 Batch   50/215   train_loss = 2.752\n",
      "Epoch 274 Batch   60/215   train_loss = 3.154\n",
      "Epoch 274 Batch   70/215   train_loss = 2.684\n",
      "Epoch 274 Batch   80/215   train_loss = 3.026\n",
      "Epoch 274 Batch   90/215   train_loss = 2.829\n",
      "Epoch 274 Batch  100/215   train_loss = 2.791\n",
      "Epoch 274 Batch  110/215   train_loss = 2.825\n",
      "Epoch 274 Batch  120/215   train_loss = 2.797\n",
      "Epoch 274 Batch  130/215   train_loss = 2.867\n",
      "Epoch 274 Batch  140/215   train_loss = 2.719\n",
      "Epoch 274 Batch  150/215   train_loss = 2.928\n",
      "Epoch 274 Batch  160/215   train_loss = 3.087\n",
      "Epoch 274 Batch  170/215   train_loss = 3.073\n",
      "Epoch 274 Batch  180/215   train_loss = 2.844\n",
      "Epoch 274 Batch  190/215   train_loss = 2.758\n",
      "Epoch 274 Batch  200/215   train_loss = 2.647\n",
      "Epoch 274 Batch  210/215   train_loss = 2.655\n",
      "Epoch 275 Batch    5/215   train_loss = 2.923\n",
      "Epoch 275 Batch   15/215   train_loss = 3.081\n",
      "Epoch 275 Batch   25/215   train_loss = 2.664\n",
      "Epoch 275 Batch   35/215   train_loss = 2.929\n",
      "Epoch 275 Batch   45/215   train_loss = 2.833\n",
      "Epoch 275 Batch   55/215   train_loss = 2.872\n",
      "Epoch 275 Batch   65/215   train_loss = 2.616\n",
      "Epoch 275 Batch   75/215   train_loss = 2.709\n",
      "Epoch 275 Batch   85/215   train_loss = 2.568\n",
      "Epoch 275 Batch   95/215   train_loss = 2.827\n",
      "Epoch 275 Batch  105/215   train_loss = 2.610\n",
      "Epoch 275 Batch  115/215   train_loss = 2.609\n",
      "Epoch 275 Batch  125/215   train_loss = 2.800\n",
      "Epoch 275 Batch  135/215   train_loss = 2.773\n",
      "Epoch 275 Batch  145/215   train_loss = 2.649\n",
      "Epoch 275 Batch  155/215   train_loss = 2.857\n",
      "Epoch 275 Batch  165/215   train_loss = 2.663\n",
      "Epoch 275 Batch  175/215   train_loss = 2.912\n",
      "Epoch 275 Batch  185/215   train_loss = 2.919\n",
      "Epoch 275 Batch  195/215   train_loss = 2.740\n",
      "Epoch 275 Batch  205/215   train_loss = 2.860\n",
      "Epoch 276 Batch    0/215   train_loss = 2.855\n",
      "Epoch 276 Batch   10/215   train_loss = 2.779\n",
      "Epoch 276 Batch   20/215   train_loss = 2.954\n",
      "Epoch 276 Batch   30/215   train_loss = 2.665\n",
      "Epoch 276 Batch   40/215   train_loss = 2.645\n",
      "Epoch 276 Batch   50/215   train_loss = 2.658\n",
      "Epoch 276 Batch   60/215   train_loss = 2.993\n",
      "Epoch 276 Batch   70/215   train_loss = 2.799\n",
      "Epoch 276 Batch   80/215   train_loss = 2.933\n",
      "Epoch 276 Batch   90/215   train_loss = 3.088\n",
      "Epoch 276 Batch  100/215   train_loss = 2.702\n",
      "Epoch 276 Batch  110/215   train_loss = 2.905\n",
      "Epoch 276 Batch  120/215   train_loss = 2.754\n",
      "Epoch 276 Batch  130/215   train_loss = 3.095\n",
      "Epoch 276 Batch  140/215   train_loss = 2.609\n",
      "Epoch 276 Batch  150/215   train_loss = 2.985\n",
      "Epoch 276 Batch  160/215   train_loss = 2.828\n",
      "Epoch 276 Batch  170/215   train_loss = 2.756\n",
      "Epoch 276 Batch  180/215   train_loss = 2.858\n",
      "Epoch 276 Batch  190/215   train_loss = 2.673\n",
      "Epoch 276 Batch  200/215   train_loss = 2.624\n",
      "Epoch 276 Batch  210/215   train_loss = 2.620\n",
      "Epoch 277 Batch    5/215   train_loss = 2.804\n",
      "Epoch 277 Batch   15/215   train_loss = 2.795\n",
      "Epoch 277 Batch   25/215   train_loss = 2.816\n",
      "Epoch 277 Batch   35/215   train_loss = 2.807\n",
      "Epoch 277 Batch   45/215   train_loss = 2.725\n",
      "Epoch 277 Batch   55/215   train_loss = 2.707\n",
      "Epoch 277 Batch   65/215   train_loss = 2.591\n",
      "Epoch 277 Batch   75/215   train_loss = 2.807\n",
      "Epoch 277 Batch   85/215   train_loss = 2.741\n",
      "Epoch 277 Batch   95/215   train_loss = 2.773\n",
      "Epoch 277 Batch  105/215   train_loss = 2.759\n",
      "Epoch 277 Batch  115/215   train_loss = 2.564\n",
      "Epoch 277 Batch  125/215   train_loss = 2.585\n",
      "Epoch 277 Batch  135/215   train_loss = 2.864\n",
      "Epoch 277 Batch  145/215   train_loss = 2.608\n",
      "Epoch 277 Batch  155/215   train_loss = 2.993\n",
      "Epoch 277 Batch  165/215   train_loss = 2.884\n",
      "Epoch 277 Batch  175/215   train_loss = 3.150\n",
      "Epoch 277 Batch  185/215   train_loss = 2.992\n",
      "Epoch 277 Batch  195/215   train_loss = 2.616\n",
      "Epoch 277 Batch  205/215   train_loss = 2.803\n",
      "Epoch 278 Batch    0/215   train_loss = 2.859\n",
      "Epoch 278 Batch   10/215   train_loss = 2.904\n",
      "Epoch 278 Batch   20/215   train_loss = 2.754\n",
      "Epoch 278 Batch   30/215   train_loss = 2.673\n",
      "Epoch 278 Batch   40/215   train_loss = 2.897\n",
      "Epoch 278 Batch   50/215   train_loss = 2.801\n",
      "Epoch 278 Batch   60/215   train_loss = 2.819\n",
      "Epoch 278 Batch   70/215   train_loss = 2.752\n",
      "Epoch 278 Batch   80/215   train_loss = 2.833\n",
      "Epoch 278 Batch   90/215   train_loss = 2.683\n",
      "Epoch 278 Batch  100/215   train_loss = 2.708\n",
      "Epoch 278 Batch  110/215   train_loss = 2.718\n",
      "Epoch 278 Batch  120/215   train_loss = 2.827\n",
      "Epoch 278 Batch  130/215   train_loss = 2.890\n",
      "Epoch 278 Batch  140/215   train_loss = 2.790\n",
      "Epoch 278 Batch  150/215   train_loss = 2.862\n",
      "Epoch 278 Batch  160/215   train_loss = 2.820\n",
      "Epoch 278 Batch  170/215   train_loss = 2.672\n",
      "Epoch 278 Batch  180/215   train_loss = 2.777\n",
      "Epoch 278 Batch  190/215   train_loss = 2.759\n",
      "Epoch 278 Batch  200/215   train_loss = 2.864\n",
      "Epoch 278 Batch  210/215   train_loss = 2.670\n",
      "Epoch 279 Batch    5/215   train_loss = 2.766\n",
      "Epoch 279 Batch   15/215   train_loss = 2.764\n",
      "Epoch 279 Batch   25/215   train_loss = 2.610\n",
      "Epoch 279 Batch   35/215   train_loss = 2.715\n",
      "Epoch 279 Batch   45/215   train_loss = 2.787\n",
      "Epoch 279 Batch   55/215   train_loss = 2.823\n",
      "Epoch 279 Batch   65/215   train_loss = 2.686\n",
      "Epoch 279 Batch   75/215   train_loss = 2.641\n",
      "Epoch 279 Batch   85/215   train_loss = 2.710\n",
      "Epoch 279 Batch   95/215   train_loss = 2.691\n",
      "Epoch 279 Batch  105/215   train_loss = 2.552\n",
      "Epoch 279 Batch  115/215   train_loss = 2.562\n",
      "Epoch 279 Batch  125/215   train_loss = 2.700\n",
      "Epoch 279 Batch  135/215   train_loss = 2.915\n",
      "Epoch 279 Batch  145/215   train_loss = 2.800\n",
      "Epoch 279 Batch  155/215   train_loss = 2.869\n",
      "Epoch 279 Batch  165/215   train_loss = 2.941\n",
      "Epoch 279 Batch  175/215   train_loss = 2.846\n",
      "Epoch 279 Batch  185/215   train_loss = 2.651\n",
      "Epoch 279 Batch  195/215   train_loss = 2.782\n",
      "Epoch 279 Batch  205/215   train_loss = 2.690\n",
      "Epoch 280 Batch    0/215   train_loss = 2.857\n",
      "Epoch 280 Batch   10/215   train_loss = 2.783\n",
      "Epoch 280 Batch   20/215   train_loss = 2.846\n",
      "Epoch 280 Batch   30/215   train_loss = 2.688\n",
      "Epoch 280 Batch   40/215   train_loss = 2.869\n",
      "Epoch 280 Batch   50/215   train_loss = 2.728\n",
      "Epoch 280 Batch   60/215   train_loss = 2.820\n",
      "Epoch 280 Batch   70/215   train_loss = 2.764\n",
      "Epoch 280 Batch   80/215   train_loss = 2.611\n",
      "Epoch 280 Batch   90/215   train_loss = 2.803\n",
      "Epoch 280 Batch  100/215   train_loss = 2.901\n",
      "Epoch 280 Batch  110/215   train_loss = 2.683\n",
      "Epoch 280 Batch  120/215   train_loss = 2.771\n",
      "Epoch 280 Batch  130/215   train_loss = 3.000\n",
      "Epoch 280 Batch  140/215   train_loss = 2.750\n",
      "Epoch 280 Batch  150/215   train_loss = 2.792\n",
      "Epoch 280 Batch  160/215   train_loss = 2.948\n",
      "Epoch 280 Batch  170/215   train_loss = 2.657\n",
      "Epoch 280 Batch  180/215   train_loss = 2.885\n",
      "Epoch 280 Batch  190/215   train_loss = 2.937\n",
      "Epoch 280 Batch  200/215   train_loss = 2.663\n",
      "Epoch 280 Batch  210/215   train_loss = 2.728\n",
      "Epoch 281 Batch    5/215   train_loss = 2.845\n",
      "Epoch 281 Batch   15/215   train_loss = 2.695\n",
      "Epoch 281 Batch   25/215   train_loss = 2.767\n",
      "Epoch 281 Batch   35/215   train_loss = 2.875\n",
      "Epoch 281 Batch   45/215   train_loss = 2.716\n",
      "Epoch 281 Batch   55/215   train_loss = 2.780\n",
      "Epoch 281 Batch   65/215   train_loss = 2.922\n",
      "Epoch 281 Batch   75/215   train_loss = 2.694\n",
      "Epoch 281 Batch   85/215   train_loss = 2.778\n",
      "Epoch 281 Batch   95/215   train_loss = 2.654\n",
      "Epoch 281 Batch  105/215   train_loss = 2.763\n",
      "Epoch 281 Batch  115/215   train_loss = 2.511\n",
      "Epoch 281 Batch  125/215   train_loss = 2.870\n",
      "Epoch 281 Batch  135/215   train_loss = 2.900\n",
      "Epoch 281 Batch  145/215   train_loss = 2.722\n",
      "Epoch 281 Batch  155/215   train_loss = 2.843\n",
      "Epoch 281 Batch  165/215   train_loss = 2.843\n",
      "Epoch 281 Batch  175/215   train_loss = 3.054\n",
      "Epoch 281 Batch  185/215   train_loss = 2.842\n",
      "Epoch 281 Batch  195/215   train_loss = 2.731\n",
      "Epoch 281 Batch  205/215   train_loss = 2.654\n",
      "Epoch 282 Batch    0/215   train_loss = 2.877\n",
      "Epoch 282 Batch   10/215   train_loss = 2.901\n",
      "Epoch 282 Batch   20/215   train_loss = 2.915\n",
      "Epoch 282 Batch   30/215   train_loss = 2.797\n",
      "Epoch 282 Batch   40/215   train_loss = 2.635\n",
      "Epoch 282 Batch   50/215   train_loss = 2.773\n",
      "Epoch 282 Batch   60/215   train_loss = 2.770\n",
      "Epoch 282 Batch   70/215   train_loss = 2.779\n",
      "Epoch 282 Batch   80/215   train_loss = 2.661\n",
      "Epoch 282 Batch   90/215   train_loss = 2.738\n",
      "Epoch 282 Batch  100/215   train_loss = 2.883\n",
      "Epoch 282 Batch  110/215   train_loss = 3.036\n",
      "Epoch 282 Batch  120/215   train_loss = 2.639\n",
      "Epoch 282 Batch  130/215   train_loss = 2.886\n",
      "Epoch 282 Batch  140/215   train_loss = 2.776\n",
      "Epoch 282 Batch  150/215   train_loss = 2.837\n",
      "Epoch 282 Batch  160/215   train_loss = 2.741\n",
      "Epoch 282 Batch  170/215   train_loss = 2.747\n",
      "Epoch 282 Batch  180/215   train_loss = 2.865\n",
      "Epoch 282 Batch  190/215   train_loss = 2.683\n",
      "Epoch 282 Batch  200/215   train_loss = 2.607\n",
      "Epoch 282 Batch  210/215   train_loss = 2.749\n",
      "Epoch 283 Batch    5/215   train_loss = 2.658\n",
      "Epoch 283 Batch   15/215   train_loss = 2.656\n",
      "Epoch 283 Batch   25/215   train_loss = 2.668\n",
      "Epoch 283 Batch   35/215   train_loss = 3.136\n",
      "Epoch 283 Batch   45/215   train_loss = 2.660\n",
      "Epoch 283 Batch   55/215   train_loss = 2.745\n",
      "Epoch 283 Batch   65/215   train_loss = 2.846\n",
      "Epoch 283 Batch   75/215   train_loss = 2.558\n",
      "Epoch 283 Batch   85/215   train_loss = 2.614\n",
      "Epoch 283 Batch   95/215   train_loss = 2.613\n",
      "Epoch 283 Batch  105/215   train_loss = 2.541\n",
      "Epoch 283 Batch  115/215   train_loss = 2.691\n",
      "Epoch 283 Batch  125/215   train_loss = 2.776\n",
      "Epoch 283 Batch  135/215   train_loss = 2.584\n",
      "Epoch 283 Batch  145/215   train_loss = 2.717\n",
      "Epoch 283 Batch  155/215   train_loss = 2.761\n",
      "Epoch 283 Batch  165/215   train_loss = 2.725\n",
      "Epoch 283 Batch  175/215   train_loss = 2.955\n",
      "Epoch 283 Batch  185/215   train_loss = 2.993\n",
      "Epoch 283 Batch  195/215   train_loss = 2.665\n",
      "Epoch 283 Batch  205/215   train_loss = 2.591\n",
      "Epoch 284 Batch    0/215   train_loss = 2.867\n",
      "Epoch 284 Batch   10/215   train_loss = 2.891\n",
      "Epoch 284 Batch   20/215   train_loss = 2.812\n",
      "Epoch 284 Batch   30/215   train_loss = 2.917\n",
      "Epoch 284 Batch   40/215   train_loss = 2.629\n",
      "Epoch 284 Batch   50/215   train_loss = 2.851\n",
      "Epoch 284 Batch   60/215   train_loss = 2.949\n",
      "Epoch 284 Batch   70/215   train_loss = 2.594\n",
      "Epoch 284 Batch   80/215   train_loss = 2.704\n",
      "Epoch 284 Batch   90/215   train_loss = 2.739\n",
      "Epoch 284 Batch  100/215   train_loss = 2.814\n",
      "Epoch 284 Batch  110/215   train_loss = 2.864\n",
      "Epoch 284 Batch  120/215   train_loss = 2.679\n",
      "Epoch 284 Batch  130/215   train_loss = 2.993\n",
      "Epoch 284 Batch  140/215   train_loss = 2.729\n",
      "Epoch 284 Batch  150/215   train_loss = 2.723\n",
      "Epoch 284 Batch  160/215   train_loss = 2.950\n",
      "Epoch 284 Batch  170/215   train_loss = 2.639\n",
      "Epoch 284 Batch  180/215   train_loss = 2.647\n",
      "Epoch 284 Batch  190/215   train_loss = 2.732\n",
      "Epoch 284 Batch  200/215   train_loss = 2.643\n",
      "Epoch 284 Batch  210/215   train_loss = 2.599\n",
      "Epoch 285 Batch    5/215   train_loss = 2.841\n",
      "Epoch 285 Batch   15/215   train_loss = 2.817\n",
      "Epoch 285 Batch   25/215   train_loss = 2.627\n",
      "Epoch 285 Batch   35/215   train_loss = 2.898\n",
      "Epoch 285 Batch   45/215   train_loss = 2.978\n",
      "Epoch 285 Batch   55/215   train_loss = 2.855\n",
      "Epoch 285 Batch   65/215   train_loss = 2.841\n",
      "Epoch 285 Batch   75/215   train_loss = 2.752\n",
      "Epoch 285 Batch   85/215   train_loss = 2.807\n",
      "Epoch 285 Batch   95/215   train_loss = 2.756\n",
      "Epoch 285 Batch  105/215   train_loss = 2.857\n",
      "Epoch 285 Batch  115/215   train_loss = 2.661\n",
      "Epoch 285 Batch  125/215   train_loss = 2.568\n",
      "Epoch 285 Batch  135/215   train_loss = 2.550\n",
      "Epoch 285 Batch  145/215   train_loss = 3.022\n",
      "Epoch 285 Batch  155/215   train_loss = 2.938\n",
      "Epoch 285 Batch  165/215   train_loss = 2.698\n",
      "Epoch 285 Batch  175/215   train_loss = 3.012\n",
      "Epoch 285 Batch  185/215   train_loss = 2.847\n",
      "Epoch 285 Batch  195/215   train_loss = 2.697\n",
      "Epoch 285 Batch  205/215   train_loss = 2.654\n",
      "Epoch 286 Batch    0/215   train_loss = 2.741\n",
      "Epoch 286 Batch   10/215   train_loss = 2.880\n",
      "Epoch 286 Batch   20/215   train_loss = 2.991\n",
      "Epoch 286 Batch   30/215   train_loss = 2.604\n",
      "Epoch 286 Batch   40/215   train_loss = 2.794\n",
      "Epoch 286 Batch   50/215   train_loss = 2.687\n",
      "Epoch 286 Batch   60/215   train_loss = 2.930\n",
      "Epoch 286 Batch   70/215   train_loss = 2.558\n",
      "Epoch 286 Batch   80/215   train_loss = 2.781\n",
      "Epoch 286 Batch   90/215   train_loss = 2.964\n",
      "Epoch 286 Batch  100/215   train_loss = 2.752\n",
      "Epoch 286 Batch  110/215   train_loss = 2.780\n",
      "Epoch 286 Batch  120/215   train_loss = 2.898\n",
      "Epoch 286 Batch  130/215   train_loss = 2.971\n",
      "Epoch 286 Batch  140/215   train_loss = 2.806\n",
      "Epoch 286 Batch  150/215   train_loss = 2.849\n",
      "Epoch 286 Batch  160/215   train_loss = 2.939\n",
      "Epoch 286 Batch  170/215   train_loss = 2.628\n",
      "Epoch 286 Batch  180/215   train_loss = 2.848\n",
      "Epoch 286 Batch  190/215   train_loss = 2.649\n",
      "Epoch 286 Batch  200/215   train_loss = 2.781\n",
      "Epoch 286 Batch  210/215   train_loss = 2.706\n",
      "Epoch 287 Batch    5/215   train_loss = 2.906\n",
      "Epoch 287 Batch   15/215   train_loss = 2.793\n",
      "Epoch 287 Batch   25/215   train_loss = 2.668\n",
      "Epoch 287 Batch   35/215   train_loss = 2.870\n",
      "Epoch 287 Batch   45/215   train_loss = 2.867\n",
      "Epoch 287 Batch   55/215   train_loss = 2.684\n",
      "Epoch 287 Batch   65/215   train_loss = 2.925\n",
      "Epoch 287 Batch   75/215   train_loss = 2.534\n",
      "Epoch 287 Batch   85/215   train_loss = 2.658\n",
      "Epoch 287 Batch   95/215   train_loss = 2.694\n",
      "Epoch 287 Batch  105/215   train_loss = 2.607\n",
      "Epoch 287 Batch  115/215   train_loss = 2.686\n",
      "Epoch 287 Batch  125/215   train_loss = 2.617\n",
      "Epoch 287 Batch  135/215   train_loss = 2.623\n",
      "Epoch 287 Batch  145/215   train_loss = 2.763\n",
      "Epoch 287 Batch  155/215   train_loss = 2.710\n",
      "Epoch 287 Batch  165/215   train_loss = 2.595\n",
      "Epoch 287 Batch  175/215   train_loss = 2.917\n",
      "Epoch 287 Batch  185/215   train_loss = 2.936\n",
      "Epoch 287 Batch  195/215   train_loss = 2.533\n",
      "Epoch 287 Batch  205/215   train_loss = 2.729\n",
      "Epoch 288 Batch    0/215   train_loss = 2.875\n",
      "Epoch 288 Batch   10/215   train_loss = 3.026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-396c263e7e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m                 \u001b[0minitial_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m                 lr: learning_rate}\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;31m# Show every <show_every_n_batches> batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tvscript\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 767\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tvscript\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 965\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tvscript\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1015\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tvscript\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1022\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tvscript\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1004\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# \"\"\"\n",
    "# DON'T MODIFY ANYTHING IN THIS CELL\n",
    "# \"\"\"\n",
    "# batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "# with tf.Session(graph=train_graph) as sess:\n",
    "#     sess.run(tf.global_variables_initializer())\n",
    "\n",
    "#     for epoch_i in range(num_epochs):\n",
    "#         state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "#         for batch_i, (x, y) in enumerate(batches):\n",
    "#             feed = {\n",
    "#                 input_text: x,\n",
    "#                 targets: y,\n",
    "#                 initial_state: state,\n",
    "#                 lr: learning_rate}\n",
    "#             train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "#             # Show every <show_every_n_batches> batches\n",
    "#             if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "#                 print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "#                     epoch_i,\n",
    "#                     batch_i,\n",
    "#                     len(batches),\n",
    "#                     train_loss))\n",
    "\n",
    "#     # Save Model\n",
    "#     saver = tf.train.Saver()\n",
    "#     saver.save(sess, save_dir)\n",
    "#     print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These are the hyperparameter settings that got the result I was looking for, loss of < 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/17   train_loss = 8.917\n",
      "Epoch   1 Batch    3/17   train_loss = 5.055\n",
      "Epoch   2 Batch    6/17   train_loss = 4.128\n",
      "Epoch   3 Batch    9/17   train_loss = 3.341\n",
      "Epoch   4 Batch   12/17   train_loss = 2.658\n",
      "Epoch   5 Batch   15/17   train_loss = 2.198\n",
      "Epoch   7 Batch    1/17   train_loss = 1.812\n",
      "Epoch   8 Batch    4/17   train_loss = 1.556\n",
      "Epoch   9 Batch    7/17   train_loss = 1.342\n",
      "Epoch  10 Batch   10/17   train_loss = 1.168\n",
      "Epoch  11 Batch   13/17   train_loss = 1.069\n",
      "Epoch  12 Batch   16/17   train_loss = 0.989\n",
      "Epoch  14 Batch    2/17   train_loss = 0.935\n",
      "Epoch  15 Batch    5/17   train_loss = 0.852\n",
      "Epoch  16 Batch    8/17   train_loss = 0.862\n",
      "Epoch  17 Batch   11/17   train_loss = 0.768\n",
      "Epoch  18 Batch   14/17   train_loss = 0.772\n",
      "Epoch  20 Batch    0/17   train_loss = 0.740\n",
      "Epoch  21 Batch    3/17   train_loss = 0.726\n",
      "Epoch  22 Batch    6/17   train_loss = 0.688\n",
      "Epoch  23 Batch    9/17   train_loss = 0.628\n",
      "Epoch  24 Batch   12/17   train_loss = 0.667\n",
      "Epoch  25 Batch   15/17   train_loss = 0.643\n",
      "Epoch  27 Batch    1/17   train_loss = 0.617\n",
      "Epoch  28 Batch    4/17   train_loss = 0.626\n",
      "Epoch  29 Batch    7/17   train_loss = 0.591\n",
      "Epoch  30 Batch   10/17   train_loss = 0.576\n",
      "Epoch  31 Batch   13/17   train_loss = 0.561\n",
      "Epoch  32 Batch   16/17   train_loss = 0.571\n",
      "Epoch  34 Batch    2/17   train_loss = 0.550\n",
      "Epoch  35 Batch    5/17   train_loss = 0.542\n",
      "Epoch  36 Batch    8/17   train_loss = 0.548\n",
      "Epoch  37 Batch   11/17   train_loss = 0.568\n",
      "Epoch  38 Batch   14/17   train_loss = 0.544\n",
      "Epoch  40 Batch    0/17   train_loss = 0.560\n",
      "Epoch  41 Batch    3/17   train_loss = 0.557\n",
      "Epoch  42 Batch    6/17   train_loss = 0.557\n",
      "Epoch  43 Batch    9/17   train_loss = 0.556\n",
      "Epoch  44 Batch   12/17   train_loss = 0.578\n",
      "Epoch  45 Batch   15/17   train_loss = 0.550\n",
      "Epoch  47 Batch    1/17   train_loss = 0.554\n",
      "Epoch  48 Batch    4/17   train_loss = 0.541\n",
      "Epoch  49 Batch    7/17   train_loss = 0.566\n",
      "Epoch  50 Batch   10/17   train_loss = 0.550\n",
      "Epoch  51 Batch   13/17   train_loss = 0.595\n",
      "Epoch  52 Batch   16/17   train_loss = 0.574\n",
      "Epoch  54 Batch    2/17   train_loss = 0.559\n",
      "Epoch  55 Batch    5/17   train_loss = 0.566\n",
      "Epoch  56 Batch    8/17   train_loss = 0.587\n",
      "Epoch  57 Batch   11/17   train_loss = 0.605\n",
      "Epoch  58 Batch   14/17   train_loss = 0.610\n",
      "Epoch  60 Batch    0/17   train_loss = 0.618\n",
      "Epoch  61 Batch    3/17   train_loss = 0.655\n",
      "Epoch  62 Batch    6/17   train_loss = 0.597\n",
      "Epoch  63 Batch    9/17   train_loss = 0.616\n",
      "Epoch  64 Batch   12/17   train_loss = 0.632\n",
      "Epoch  65 Batch   15/17   train_loss = 0.624\n",
      "Epoch  67 Batch    1/17   train_loss = 0.596\n",
      "Epoch  68 Batch    4/17   train_loss = 0.624\n",
      "Epoch  69 Batch    7/17   train_loss = 0.621\n",
      "Epoch  70 Batch   10/17   train_loss = 0.640\n",
      "Epoch  71 Batch   13/17   train_loss = 0.656\n",
      "Epoch  72 Batch   16/17   train_loss = 0.663\n",
      "Epoch  74 Batch    2/17   train_loss = 0.631\n",
      "Epoch  75 Batch    5/17   train_loss = 0.638\n",
      "Epoch  76 Batch    8/17   train_loss = 0.651\n",
      "Epoch  77 Batch   11/17   train_loss = 0.633\n",
      "Epoch  78 Batch   14/17   train_loss = 0.692\n",
      "Epoch  80 Batch    0/17   train_loss = 0.676\n",
      "Epoch  81 Batch    3/17   train_loss = 0.649\n",
      "Epoch  82 Batch    6/17   train_loss = 0.640\n",
      "Epoch  83 Batch    9/17   train_loss = 0.620\n",
      "Epoch  84 Batch   12/17   train_loss = 0.662\n",
      "Epoch  85 Batch   15/17   train_loss = 0.635\n",
      "Epoch  87 Batch    1/17   train_loss = 0.627\n",
      "Epoch  88 Batch    4/17   train_loss = 0.615\n",
      "Epoch  89 Batch    7/17   train_loss = 0.649\n",
      "Epoch  90 Batch   10/17   train_loss = 0.642\n",
      "Epoch  91 Batch   13/17   train_loss = 0.623\n",
      "Epoch  92 Batch   16/17   train_loss = 0.623\n",
      "Epoch  94 Batch    2/17   train_loss = 0.627\n",
      "Epoch  95 Batch    5/17   train_loss = 0.619\n",
      "Epoch  96 Batch    8/17   train_loss = 0.634\n",
      "Epoch  97 Batch   11/17   train_loss = 0.641\n",
      "Epoch  98 Batch   14/17   train_loss = 0.638\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: learning_rate}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))\n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Save Parameters\n",
    "Save `seq_length` and `save_dir` for generating a new TV script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Save parameters for checkpoint\n",
    "helper.save_params((seq_length, save_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Generate Functions\n",
    "### Get Tensors\n",
    "Get tensors from `loaded_graph` using the function [`get_tensor_by_name()`](https://www.tensorflow.org/api_docs/python/tf/Graph#get_tensor_by_name).  Get the tensors using the following names:\n",
    "- \"input:0\"\n",
    "- \"initial_state:0\"\n",
    "- \"final_state:0\"\n",
    "- \"probs:0\"\n",
    "\n",
    "Return the tensors in the following tuple `(InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def get_tensors(loaded_graph):\n",
    "    \"\"\"\n",
    "    Get input, initial state, final state, and probabilities tensor from <loaded_graph>\n",
    "    :param loaded_graph: TensorFlow graph loaded from file\n",
    "    :return: Tuple (InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    # Get the tensors by name\n",
    "    #\n",
    "    # NOTE: It seems like tensorflow enumerates the variable names you give it,\n",
    "    #       so I appened :0 to the variable names since there should be only one copy.\n",
    "    InputTensor = loaded_graph.get_tensor_by_name('input:0')\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name('initial_state:0')\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name('final_state:0')\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name('probs:0')\n",
    "    \n",
    "    return InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_get_tensors(get_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Choose Word\n",
    "Implement the `pick_word()` function to select the next word using `probabilities`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \"\"\"\n",
    "    Pick the next word in the generated text\n",
    "    :param probabilities: Probabilites of the next word\n",
    "    :param int_to_vocab: Dictionary of word ids as the keys and words as the values\n",
    "    :return: String of the predicted word\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "#     # DEBUG\n",
    "#     #\n",
    "#     print(\"DEBUG - type(probabilities): {}\".format(type(probabilities)))\n",
    "#     print(\"DEBUG - type(int_to_vocab): {}\".format(type(int_to_vocab)))\n",
    "    \n",
    "    # Generates a random sample from a given 1-D array\n",
    "    # https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.choice.html\n",
    "    pred_word = np.random.choice( list(int_to_vocab.values()),\n",
    "                                  p=probabilities )\n",
    "    \n",
    "    return pred_word\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_pick_word(pick_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Generate TV Script\n",
    "This will generate the TV script for you.  Set `gen_length` to the length of TV script you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "moe_szyslak: oh no!(four hundred nos.\n",
      "carl_carlson: marge, we gotta celebrate! throw a ragin' on, and then there...\n",
      "comic_book_guy: my name is walther hotenhoffer and i'm in an here.\n",
      "marge_simpson:(pained) i guess so one, i'd'a put lenny on the...\n",
      "homer_simpson:(getting idea) no, the pipes not smells so much about lousy.\n",
      "moe_szyslak: wow, that i go with that uh, y'know they mr. the private, show me the secret guy i know named to make my rent.\n",
      "carl_carlson: be for him? every day too?\n",
      "moe_szyslak: 'cause i was inches of you lugs wanna get any specific harm.\n",
      "\n",
      "\n",
      "moe_szyslak: get down here! the you!\n",
      "barney_gumble: of course. but didn't it sounds like one of these\" alone during the bonding phase.\n",
      "moe_szyslak:(desperate) wait, wait, wait, wait, wait... bo's he still got to say one hundred.\n",
      "marguerite: is here that the now of you krusty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gen_length = 200\n",
    "# homer_simpson, moe_szyslak, or Barney_Gumble\n",
    "prime_word = 'moe_szyslak'\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "        \n",
    "        pred_word = pick_word(probabilities[dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# The TV Script is Nonsensical\n",
    "It's ok if the TV script doesn't make any sense.  We trained on less than a megabyte of text.  In order to get good results, you'll have to use a smaller vocabulary or get more data.  Luckly there's more data!  As we mentioned in the begging of this project, this is a subset of [another dataset](https://www.kaggle.com/wcukierski/the-simpsons-by-the-data).  We didn't have you train on all the data, because that would take too long.  However, you are free to train your neural network on all the data.  After you complete the project, of course.\n",
    "# Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook. Save the notebook file as \"dlnd_tv_script_generation.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\". Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tvscript]",
   "language": "python",
   "name": "conda-env-tvscript-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
